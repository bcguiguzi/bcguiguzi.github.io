<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Ubuntu下使用Python开发Spark程序 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Ubuntu下使用Python开发Spark程序" />
<meta property="og:description" content="1、安装配置好Spark环境，确认执行Spark目录下的/bin/pyspark能够成功进入。
Spark 集群搭建从零开始之1 Scala的安装与配置
Spark 集群搭建从零开始之2 Spark单机伪分布式安装与配置
Spark 集群搭建从零开始之3 Spark Standalone集群安装、配置与测试
2、安装anaconda2
https://www.anaconda.com/download/#linux
bash Anaconda2-5.0.1-Linux-x86_64.sh
3、sudo pip install pyspark
4、进入jupyter notebook，编写程序测试
基本上SparkContext那句不报错就说明已经能够启动Spark
附上环境变量:
# /etc/profile: system-wide .profile file for the Bourne shell (sh(1)) # and Bourne compatible shells (bash(1), ksh(1), ash(1), ...). export JAVA_HOME=/usr/lib/jvm/java-8-oracle export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH export HADOOP_HOME=/home/chenjie/hadoop-2.6.5 #export HADOOP_HOME=/home/chenjie/hadoop-2.6.5-net export CLASSPATH=.:$HADOOP_HOME/lib:$CLASSPATH export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME export HADOOP_ROOT_LOGGER=INFO,console export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export HADOOP_OPTS=&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/c838d9611a228c7f86dc3e75ead4a783/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-12-28T14:44:15+08:00" />
<meta property="article:modified_time" content="2017-12-28T14:44:15+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Ubuntu下使用Python开发Spark程序</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>1、安装配置好Spark环境，确认执行Spark目录下的/bin/pyspark能够成功进入。</p> 
<p><a target="_blank" href="http://blog.csdn.net/csj941227/article/details/78023770" rel="noopener noreferrer">Spark 集群搭建从零开始之1 Scala的安装与配置</a><br> </p> 
<p><a target="_blank" href="http://blog.csdn.net/csj941227/article/details/78026423" rel="noopener noreferrer">Spark 集群搭建从零开始之2 Spark单机伪分布式安装与配置</a><br> </p> 
<p><a target="_blank" href="http://blog.csdn.net/csj941227/article/details/78030890" rel="noopener noreferrer">Spark 集群搭建从零开始之3 Spark Standalone集群安装、配置与测试</a><br> </p> 
<p>2、安装anaconda2</p> 
<p>https://www.anaconda.com/download/#linux<br> </p> 
<p>bash  Anaconda2-5.0.1-Linux-x86_64.sh</p> 
<p>3、sudo pip install pyspark</p> 
<p>4、进入jupyter notebook，编写程序测试</p> 
<p>基本上SparkContext那句不报错就说明已经能够启动Spark</p> 
<p><img src="https://images2.imgbox.com/ee/d9/lmta6xKP_o.png" alt=""><br> </p> 
<p><br> </p> 
<p>附上环境变量:</p> 
<p></p> 
<pre><code class="language-plain"># /etc/profile: system-wide .profile file for the Bourne shell (sh(1))
# and Bourne compatible shells (bash(1), ksh(1), ash(1), ...).
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH
export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH

export HADOOP_HOME=/home/chenjie/hadoop-2.6.5
#export HADOOP_HOME=/home/chenjie/hadoop-2.6.5-net
export CLASSPATH=.:$HADOOP_HOME/lib:$CLASSPATH
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_ROOT_LOGGER=INFO,console
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"


#scala
export SCALA_HOME=/home/chenjie/scala-2.10.4
export PATH=${SCALA_HOME}/bin:$PATH
#spark
export SPARK_HOME=/home/chenjie/spark-1.6.0-bin-hadoop2.6
#export SPARK_HOME=/home/chenjie/spark-1.6.0-bin-hadoop2.6-net
export PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:$PATH


#Flume
export FLUME_HOME=/home/chenjie/apache-flume-1.5.0-bin
export FLUME_CONF_DIR=$FLUME_HOME/conf
export PATH=.:$PATH::$FLUME_HOME/bin



#hive
export HIVE_HOME=/home/chenjie/apache-hive-2.3.0-bin
export PATH=$PATH:$HIVE_HOME/bin

#sqoop
export SQOOP_HOME=/home/chenjie/sqoop-1.4.6.bin__hadoop-2.0.4-alpha
export PATH=$PATH:$SQOOP_HOME/bin
export SQOOP_SERVER_EXTRA_LIB=$SQOOP_HOME/extra

#maven
export PATH=$PATH:/home/chenjie/apache-maven-3.5.0/bin

#
export PYTHONPATH=/home/chenjie/spark-1.6.0-bin-hadoop2.6/python
export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH
export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.9.0-src.zip:$PYTHONPATH
PYSPARK_DRIVER_PYTHON=ipython  
PYSPARK_DRIVER_PYTHON_OPTS='notebook' 




if [ "$PS1" ]; then
  if [ "$BASH" ] &amp;&amp; [ "$BASH" != "/bin/sh" ]; then
    # The file bash.bashrc already sets the default PS1.
    # PS1='\h:\w\$ '
    if [ -f /etc/bash.bashrc ]; then
      . /etc/bash.bashrc
    fi
  else
    if [ "`id -u`" -eq 0 ]; then
      PS1='# '
    else
      PS1='$ '
    fi
  fi
fi

# The default umask is now handled by pam_umask.
# See pam_umask(8) and /etc/login.defs.

if [ -d /etc/profile.d ]; then
  for i in /etc/profile.d/*.sh; do
    if [ -r $i ]; then
      . $i
    fi
  done
  unset i
fi</code></pre> 
<br> 
<br> 
<p></p> 
<p><br> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8cf88476c8e99c72fb0424a4f761c295/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">double类型的最大值表示</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/18a44f50ccaad18126e7245e4bcfb303/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">安卓使用友盟推送问题总结</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>