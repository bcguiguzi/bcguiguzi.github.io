<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【Pytorch】详解RNN网络中文本的pack和pad操作 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【Pytorch】详解RNN网络中文本的pack和pad操作" />
<meta property="og:description" content="1. 引言 RNN模型一般设定固定的文本长度（text sequence length，可理解为文本序列在时间维度上的步数 time step），以保证网络输出层数据维度的一致性。但在训练和测试时，难以保证输入文本长度的一致性，因此常常需要截断操作（即将超过预设长度的文本截断）和pad操作（即对不足预设长度的文本进行补0填充）。pad操作需满足：
（1）pad后，不足预设长度的文本用相同特征维度的0填充；
（2）pad的部分不参与forward和backward计算。
Pytorch中，在文本数据的transfrom以及RNN网络的输入阶段，均充分考虑了pad操作。其主要体现在：
（1）RNN、LSTM和GRU等网络的输入数据均可为PackedSequence类型数据；
（2）可通过pad_sequence、pack_sequence、pack_padded_sequence和pad_packed_sequence等操作，实现pad和pack操作。
2. pack和pad操作 那么，究竟pad和pack操作对原始数据会有何影响？下面通过一个简单的示例来体现。
from torch.nn.utils.rnn import pack_sequence, pad_sequence,pad_packed_sequence, pack_padded_sequence, text1 = torch.tensor([1,2,3,4]) # 可视为有4个文字的样本 text2 = torch.tensor([5,6,7]) # 可视为有3个文字的样本 text3 = torch.tensor([8,9]) # 可视为有2个文字的样本 sequences = [text1, text2, text3] # 三个文本序列 2.1 pack操作 [Input] pack_sequence(sequences) [Output] PackedSequence(data=tensor([1, 5, 8, 2, 6, 9, 3, 7, 4]), batch_sizes=tensor([3, 3, 2, 1])) pack操作将原来的二维数据（batch*sequence）进行了压缩，但其排列是按照列（即sequence的顺序）进行排列，每个时间步一次性输出batch上的所有样本。
pack后的返回值包括两数据。一类为data，即压缩后的数据；而另一类batch_sizes表示每个时间步，batch中包含的样本量。
拿本例来说，因为总共有4个时间步（即文本序列长度为4），所以batch_sizes的长度为4。在第1、2个时间内步，所有样本均有编码，所有对应值为3，而在第三个时间步，只有前两个样本有编码，所以对应值为2。
值的注意的是，sequences列表内的各元素长度必须按照降序排列，也就是越长的文本应放在前面，整个Batch*Sequence矩阵为上三角阵。注意：在最新版本的pytorch中已经升级，可文本长度可乱序组织，但需要指定各sequence的length。
前文提到的RNN网络中可以接收的Input数据可以为PackedSequence类型数据，即是类似于这里的返回值。
2.2 pad操作 [Input] pad_sequence(sequences) [Output] tensor([[1, 5, 8], [2, 6, 9], [3, 7, 0], [4, 0, 0]]) [Input] pad_sequence(sequences, batch_first=True) [Output] tensor([[1, 2, 3, 4], [5, 6, 7, 0], [8, 9, 0, 0]]) pad操作即是将不同长度的文本序列进行对齐的填充过程。默认情况下，参数batch_first=False，其返回值的第一个维度将变成sequence，而第二个维度才为batch。当然，可以通过设置batch_first=True，使得返回值的第一个维度为batch，从而保持与输入值的一致性。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/ff13bd22dcc80bf5a911d79a3399942d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-01-21T13:41:09+08:00" />
<meta property="article:modified_time" content="2020-01-21T13:41:09+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【Pytorch】详解RNN网络中文本的pack和pad操作</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h5><a id="1__0"></a>1. 引言</h5> 
<p>RNN模型一般设定固定的文本长度（text sequence length，可理解为文本序列在时间维度上的步数 time step），以保证网络输出层数据维度的一致性。但在训练和测试时，难以保证输入文本长度的一致性，因此常常需要截断操作（即将超过预设长度的文本截断）和pad操作（即对不足预设长度的文本进行补0填充）。pad操作需满足：<br> （1）pad后，不足预设长度的文本用相同特征维度的0填充；<br> （2）pad的部分不参与forward和backward计算。</p> 
<p>Pytorch中，在文本数据的transfrom以及RNN网络的输入阶段，均充分考虑了pad操作。其主要体现在：<br> （1）RNN、LSTM和GRU等网络的输入数据均可为<code>PackedSequence</code>类型数据；<br> （2）可通过<code>pad_sequence</code>、<code>pack_sequence</code>、<code>pack_padded_sequence</code>和<code>pad_packed_sequence</code>等操作，实现pad和pack操作。</p> 
<h5><a id="2_packpad_9"></a>2. pack和pad操作</h5> 
<p>那么，究竟pad和pack操作对原始数据会有何影响？下面通过一个简单的示例来体现。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>rnn <span class="token keyword">import</span> pack_sequence<span class="token punctuation">,</span> pad_sequence<span class="token punctuation">,</span>pad_packed_sequence<span class="token punctuation">,</span> pack_padded_sequence<span class="token punctuation">,</span> 

text1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># 可视为有4个文字的样本</span>
text2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 可视为有3个文字的样本</span>
text3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># 可视为有2个文字的样本</span>
sequences <span class="token operator">=</span> <span class="token punctuation">[</span>text1<span class="token punctuation">,</span> text2<span class="token punctuation">,</span> text3<span class="token punctuation">]</span>  <span class="token comment"># 三个文本序列</span>
</code></pre> 
<h6><a id="21_pack_19"></a>2.1 pack操作</h6> 
<pre><code class="prism language-python"><span class="token punctuation">[</span>Input<span class="token punctuation">]</span>  pack_sequence<span class="token punctuation">(</span>sequences<span class="token punctuation">)</span>
<span class="token punctuation">[</span>Output<span class="token punctuation">]</span> PackedSequence<span class="token punctuation">(</span>data<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> batch_sizes<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>pack操作将原来的二维数据（batch*sequence）进行了压缩，但其排列是按照列（即sequence的顺序）进行排列，每个时间步一次性输出batch上的所有样本。<br> <img src="https://images2.imgbox.com/5a/ee/lDuy8dlt_o.png" alt="在这里插入图片描述">pack后的返回值包括两数据。一类为<code>data</code>，即压缩后的数据；而另一类<code>batch_sizes</code>表示每个时间步，batch中包含的样本量。<br> 拿本例来说，因为总共有4个时间步（即文本序列长度为4），所以<code>batch_sizes</code>的长度为4。在第1、2个时间内步，所有样本均有编码，所有对应值为3，而在第三个时间步，只有前两个样本有编码，所以对应值为2。</p> 
<p>值的注意的是，<code>sequences</code>列表内的各元素长度必须按照<strong>降序</strong>排列，也就是越长的文本应放在前面，整个Batch*Sequence矩阵为上三角阵。<strong>注意：在最新版本的pytorch中已经升级，可文本长度可乱序组织，但需要指定各sequence的length。</strong></p> 
<p>前文提到的RNN网络中可以接收的<code>Input</code>数据可以为<code>PackedSequence</code>类型数据，即是类似于这里的返回值。</p> 
<h6><a id="22_pad_32"></a>2.2 pad操作</h6> 
<pre><code class="prism language-python"><span class="token punctuation">[</span>Input<span class="token punctuation">]</span>  pad_sequence<span class="token punctuation">(</span>sequences<span class="token punctuation">)</span>
<span class="token punctuation">[</span>Output<span class="token punctuation">]</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        		<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        		<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        		<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token punctuation">[</span>Input<span class="token punctuation">]</span>  pad_sequence<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span>Output<span class="token punctuation">]</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        		<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        		<span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>pad操作即是将不同长度的文本序列进行对齐的填充过程。默认情况下，参数<code>batch_first=False</code>，其返回值的第一个维度将变成<code>sequence</code>，而第二个维度才为<code>batch</code>。当然，可以通过设置<code>batch_first=True</code>，使得返回值的第一个维度为<code>batch</code>，从而保持与输入值的一致性。<br> <img src="https://images2.imgbox.com/83/ac/6veB1qQw_o.png" alt="在这里插入图片描述"><br> 与<code>pack</code>操作不同，<code>pad</code>操作对于<code>sequences</code>列表内的各元素长度顺序并无要求。</p> 
<h6><a id="23_batch_48"></a>2.3 关于batch顺序的思考</h6> 
<p>观察上述pack和pad操作，返回结果均倾向于<strong>按照序列sequece的顺序进行输出，而将batch的输出顺序后置</strong>，其实这是pytorch中整个<strong>RNN</strong>网络的统一推荐用法，观察<code>RNN</code>、<code>LSTM</code>和<code>GRU</code>等网络架构，参数<code>batch_first</code>的默认值均为<code>False</code>！</p> 
<p>这里简单对比下文本数据在<code>batch_first=True</code>下的数据维度 <strong>[Batch, Sequence, Features]</strong> 和在<code>batch_first=False</code>下的**[Sequence, Batch, Features]**两种数据排列方式的异同。</p> 
<p>我们可以将第一个维度视为<strong>循环维度</strong>。对于前者，其循环项为Batch内的每一个样本，因此无法同时喂入RNN网络处理；而对于后者，其循环项为时间步，可以一次性将每个时间步内的全量样本喂入RNN网络处理。所以后者这种cross-batch的方式更为推荐。不过pytorch内做了处理，无论将<code>batch_first</code>设为True或者False，其内部计算时均采用每个时间步cross-batch的并行方式，但需要注意网络模型的<code>batch_first</code>设置应与输入数据的<code>batch</code>维度保持一致性。</p> 
<h5><a id="3_pack_padded_sequencepad_packed_sequence_55"></a>3. pack_padded_sequence和pad_packed_sequence</h5> 
<p>因为RNN网络可以接受的是<code>PackedSequence</code>类型数据（通过pack操作实现），而pad操作又可以实现不等长文本的填充对齐，所以自然会想到将两个操作联合起来，这就是pytorch提供的<code>pack_padded_sequence</code>和<code>pad_packed_sequence</code>功能。</p> 
<h6><a id="31_pack_padded_sequence_57"></a>3.1· pack_padded_sequence</h6> 
<p>顾名思义，将经pad后的文本序列在做pack，从而实现对文本缺失位置的填0和维度压缩。</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span>Input<span class="token punctuation">]</span>  pack_padded_sequence<span class="token punctuation">(</span>pad_sequence<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lengths<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span>Output<span class="token punctuation">]</span> PackedSequence<span class="token punctuation">(</span>data<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> batch_sizes<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/1e/a7/dgx0GH0T_o.png" alt="在这里插入图片描述"><br> <code>pack_padded_sequence</code>函数的作用过程可分解为如下步骤：<br> （1）接收一个<code>padded_sequence</code>数据；<br> （2）根据<code>batch_first</code>参数明确该数据的布局（默认为<code>batch_first=False</code>）；<br> （3）根据<code>lengths</code>参数明确<code>batch</code>内各样本的时间步长，选择数据；<br> （4）将上述数据按照时间维度进行压缩，得到目标的<code>PackedSequence</code>类型数据<br> 右上可见，在<code>pack_padded_sequence</code>函数有两个重要参数：</p> 
<ul><li><code>batch_first</code>：用于明确输入的<code>padded_sequence</code>数据的布局型式</li><li><code>lengths</code>：用于明确batch内各样本截取的时间长度列表，注意列表内的元素必须为降序</li></ul> 
<p>再看一个例子：</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span>Input<span class="token punctuation">]</span>  pack_padded_sequence<span class="token punctuation">(</span>pad_sequence<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lengths<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span>Output<span class="token punctuation">]</span> PackedSequence<span class="token punctuation">(</span>data<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> batch_sizes<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>是否和想象的不一样？下面来图解下其具体的变换过程：<br> <img src="https://images2.imgbox.com/41/7a/OzYIFXYq_o.png" alt="在这里插入图片描述"><br> 其与之前例子的最大区别在于<code>batch_first</code>取默认值<code>False</code>，因此即使传进来的数据本身是Batch*Sequence的布局，但对于<code>pack_padded_sequence</code>函数会将其认为是Sequence*Batch的，因此其<code>lenghts</code>参数的长度必须为4（错误的认为是4个样本），而返回的<code>batch_sizes</code>的长度为3（错误的认为只有3个时间步长）！</p> 
<p>总体来看，在使用<code>pack_padded_sequence</code>需要牢记各参数的意义（见下图），其中最重要的就是要保证<code>pad_sequence</code>中<code>batch_first</code>参数与<code>pack_padded_sequence</code>中<code>batch_first</code>参数的一致性！<br> <img src="https://images2.imgbox.com/61/02/dlxSnPKH_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="32_pack_85"></a>3.2· 再看pack操作</h6> 
<p>观察<code>pack_sequence</code>源码，不难发现，其本质就是调用了<code>pack_padded_sequence</code>函数：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">pack_sequence</span><span class="token punctuation">(</span>sequences<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">return</span> pack_padded_sequence<span class="token punctuation">(</span>pad_sequence<span class="token punctuation">(</span>sequences<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>v<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">for</span> v <span class="token keyword">in</span> sequences<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>注意到其在<code>pad</code>和<code>pack</code>阶段都未设置<code>batch_first</code>参数（即<code>batch_first=False</code>），而<code>lenghts</code>设为各文本的真实长度，所以保证了<code>pack_sequence</code>的有效性。</p> 
<h6><a id="33_pad_packed_sequence_92"></a>3.3· pad_packed_sequence</h6> 
<p><code>pad_packed_sequence</code>函数即为<code>pack_padded_sequence</code>的逆操作（<strong>注意</strong>pad后的最大长度等于batch内的最大非padding长度，所以并不是严格意义上的逆操作），其在参数设定时也许注意通过<code>batch_first</code>控制返回值的维度顺序，同时可通过设置<code>total_lengths</code>来控制<code>pad</code>后的总步长（该值必须不小于输入<code>PackedSequence</code>数据的步长数）。</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span>Input<span class="token punctuation">]</span> pad_packed_sequence<span class="token punctuation">(</span>pack_sequence<span class="token punctuation">(</span>sequences<span class="token punctuation">)</span><span class="token punctuation">,</span>total_length<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span>Output<span class="token punctuation">]</span> <span class="token punctuation">(</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
         <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="4__101"></a>4. 小结</h5> 
<p>在RNN网络中，文本的pad操作用于各文本长度的对其；而pack操作用于实现文本序列数据的压缩。在使用时，需要注意如下事项：<br> （1）参数<code>batch_first</code>用于设定文本数据的布局，在进行pad、pack以及RNN网络训练时，需结合输入端和输出端的要求，进行明确和统一；<br> （2）参数<code>bacth_sizes</code>指沿着时间维度，每个时间步的所包含的样本数量；<br> （3）参数<code>lengths</code>指沿着样本维度，每个样本需要截断的时间步长;<br> （4）在实际操作中，最常用的是通过<code>pad_packed_sequence</code>h和<code>pack_padded_sequence</code>实现对文本的填充和相互转化。</p> 
<p><strong>[References]</strong><br> 1.<a href="https://blog.csdn.net/shunaoxi2313/article/details/99843368">pytorch中LSTM的细节分析理解</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/64469d02bf9da5bf560cf2650760bbc9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">“undefined variable from import……”问题解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6745c3ea07f84c064ebc9a64fc7e46c2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">剑指offer-第一个只出现一次的字符</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>