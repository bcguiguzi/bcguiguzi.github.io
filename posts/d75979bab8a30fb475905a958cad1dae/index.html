<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>RNN、LSTM反向传播推导详解 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="RNN、LSTM反向传播推导详解" />
<meta property="og:description" content="在做吴恩达深度学习课程相关作业时，顺便进行了RNN和LSTM的反向传播推导。顺便记录如下，希望能对你有所帮助~
RNN前向与反向: 模型的整体结构如下图所示，输入的是序列x、输出y，长度为Tx。
BasicRNN 前向传播 现在我们单独对每个cell进行公式推导，最终整个模型的公式其实就是单个cell的循环调用。
下图是单个cell的具体结构图，以及前向传播的公式，非常的简洁明了 Basic RNN的反向传播很简单，直接上图:
LSTM前向传播 LSTM单个cell的反向传播比Basic RNN看起来要复杂很多，主要变化就是添加了三个门：遗忘门、更新门和输出门。但是我们理清楚单个cell接收到的所有梯度，就很容易理解了。 （1）当前cell中a&lt; t &gt;通过反向传播得到的梯度同样有两个部分 - 当前输出y^&lt; t &gt;代入损失函数，对a&lt; t &gt;求导得到的da&lt; t &gt;1 - 输入到下一个cell的a&lt; t &gt;传回的梯度da&lt; t &gt;2 （2）当前cell还要接受输入到下一个cell的c&lt; t &gt;传回的梯度dc&lt; t &gt;1
为了便于理解，现在图上标记一些符号：
注意其中的 da&lt;t&gt; 是同 da&lt;t&gt;1与da&lt;t&gt;2 反向来的，而 dc&lt;t&gt; 是由 dc&lt;t&gt;1与da&lt;t&gt; 反向来的
LSTM反向传播详细推导(第一种推导法) PS：这里是本人根据吴恩达作业结合自己理解推导出来的。之所以自己宁愿手撕推导一遍，再（二）中讲明原因。
第一步计算反向传播:
第二步计算反向传播:
第三步计算反向传播:
第四步计算反向传播:
第五步计算反向传播:
注意第五步这里的 W前 W 后要分别对应 a 和 x 的维数。
至此，整个LSTM反向传播完成~~~
LSTM反向传播详细推导(第二种推导法) PS： 之所以手推了上面的(一）。是因为一开始看吴恩达作业列的式子时，有些不明白式子怎么会这样，例如dot = da_next * np.tanh(c_next) * ot * (1 - ot)，但是后来结合了代码去看，发现ot * (1 - ot)是把应该在后面才求导的式子中部分计算直接在前面给计算了，例如dwf的求导，这里就不用ot * (1 - ot)。。。当然这样结果是对的，只是一开始真把我搞糊涂了。（这里的dc并没有像上面那样直接单独算出整体结果）" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/d75979bab8a30fb475905a958cad1dae/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-04-14T12:18:11+08:00" />
<meta property="article:modified_time" content="2019-04-14T12:18:11+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">RNN、LSTM反向传播推导详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>在做吴恩达深度学习课程相关作业时，顺便进行了RNN和LSTM的反向传播推导。顺便记录如下，希望能对你有所帮助~</p> 
<h2>RNN前向与反向:</h2> 
<p>模型的整体结构如下图所示，输入的是序列x、输出y，长度为Tx。</p> 
<p><img alt="" class="has" height="323" src="https://images2.imgbox.com/6a/66/xc9usI9u_o.png" width="1000"></p> 
<p> </p> 
<h4 id="basicrnn-前向传播">BasicRNN 前向传播</h4> 
<p> 现在我们单独对每个cell进行公式推导，最终整个模型的公式其实就是单个cell的循环调用。</p> 
<p> 下图是单个cell的具体结构图，以及前向传播的公式，非常的简洁明了 </p> 
<p><img alt="" class="has" height="315" src="https://images2.imgbox.com/50/f9/ITUWGA4L_o.png" width="700"></p> 
<p><br><strong>Basic RNN的反向传播很简单，直接上图:</strong></p> 
<p><img alt="" class="has" height="571" src="https://images2.imgbox.com/05/ef/blaxJ8lC_o.png" width="1200"></p> 
<h2>LSTM前向传播</h2> 
<p><img alt="" class="has" height="551" src="https://images2.imgbox.com/03/33/ZXQruy6L_o.png" width="1200"></p> 
<p> </p> 
<p>LSTM单个cell的反向传播比Basic RNN看起来要复杂很多，主要变化就是添加了三个门：遗忘门、更新门和输出门。但是我们理清楚单个cell接收到的所有梯度，就很容易理解了。 <br><span style="color:#f33b45;">（1）当前cell中a&lt; t &gt;通过反向传播得到的梯度同样有两个部分 <br>     - 当前输出y^&lt; t &gt;代入损失函数，对a&lt; t &gt;求导得到的da&lt; t &gt;1 <br>     - 输入到下一个cell的a&lt; t &gt;传回的梯度da&lt; t &gt;2 <br> （2）当前cell还要接受输入到下一个cell的c&lt; t &gt;传回的梯度dc&lt; t &gt;1</span></p> 
<p> </p> 
<p><strong>为了便于理解，现在图上标记一些符号：</strong></p> 
<p><img alt="" class="has" height="551" src="https://images2.imgbox.com/5e/2d/h3rGUn6A_o.png" width="1200"></p> 
<p><span style="color:#f33b45;">注意其中的 da&lt;t&gt; 是同 da&lt;t&gt;1与da&lt;t&gt;2 反向来的，而 dc&lt;t&gt; 是由 dc&lt;t&gt;1与da&lt;t&gt; 反向来的</span></p> 
<h2>LSTM反向传播详细推导(第一种推导法)</h2> 
<p><span style="color:#f33b45;">PS：这里是本人根据吴恩达作业结合自己理解推导出来的。之所以自己宁愿手撕推导一遍，再（二）中讲明原因。</span></p> 
<p>第一步计算反向传播:<br><img alt="" class="has" height="400" src="https://images2.imgbox.com/31/89/OKPXVa8O_o.jpg" width="600"></p> 
<p>第二步计算反向传播:</p> 
<p><img alt="" class="has" height="345" src="https://images2.imgbox.com/a1/6f/pSIaWNZT_o.jpg" width="500"></p> 
<p>第三步计算反向传播:</p> 
<p><img alt="" class="has" height="450" src="https://images2.imgbox.com/f2/38/AgzM0JNI_o.jpg" width="600"></p> 
<p>第四步计算反向传播:</p> 
<p> <img alt="" class="has" height="450" src="https://images2.imgbox.com/e1/46/kCvNqUaE_o.jpg" width="600"></p> 
<p>第五步计算反向传播:</p> 
<p><img alt="" class="has" height="450" src="https://images2.imgbox.com/c0/28/0ZFcPEMr_o.jpg" width="600"></p> 
<p>注意第五步这里的 W前 W 后要分别对应 a 和 x 的维数。</p> 
<p>至此，整个LSTM反向传播完成~~~</p> 
<h2>LSTM反向传播详细推导(第二种推导法)</h2> 
<p><img alt="" class="has" height="551" src="https://images2.imgbox.com/98/51/17Dm5jxn_o.png" width="1200"></p> 
<p>PS： 之所以手推了上面的(一）。是因为一开始看吴恩达作业列的式子时，有些不明白式子怎么会这样，例如dot = da_next * np.tanh(c_next) *<span style="color:#f33b45;"> ot * (1 - ot)</span>，但是后来结合了代码去看，<span style="color:#f33b45;">发现ot * (1 - ot)是把应该在后面才求导的式子中部分计算直接在前面给计算了，例如dwf的求导，这里就不用ot * (1 - ot)</span>。。。当然这样结果是对的，只是一开始真把我搞糊涂了。（<span style="color:#f33b45;">这里的dc并没有像上面那样直接单独算出整体结果</span>）</p> 
<p><img alt="" class="has" height="246" src="https://images2.imgbox.com/da/3e/jHd0OEuH_o.png" width="1036"></p> 
<p><span style="color:#f33b45;">注意：  图中标记的1部分是计算dWo时才用到的，但是下面的dWo没列出，这里提前列出来了</span>。。。<span style="color:#3399ea;">(8)(9)(10)后面的下划线部分都是一样提前计算了，真正的反向求导只是中扩号里面的</span>。一开始觉得很奇怪，后来结合整体结合才看明白了。至于为什么提前计算，就是为了后面的公式写起来简单吧~</p> 
<p>          （8）式中画叉的应该是图错了，把那个去掉。</p> 
<p><img alt="" class="has" height="297" src="https://images2.imgbox.com/2d/d5/VGODS0oO_o.png" width="1023"></p> 
<p><img alt="" class="has" height="293" src="https://images2.imgbox.com/01/6f/XPyHV1q2_o.png" width="1016"></p> 
<p>实现代码如下:</p> 
<pre class="has"><code>def lstm_cell_backward(da_next, dc_next, cache):
    """
    Implement the backward pass for the LSTM-cell (single time-step).

    Arguments:
    da_next -- Gradients of next hidden state, of shape (n_a, m)
    dc_next -- Gradients of next cell state, of shape (n_a, m)
    cache -- cache storing information from the forward pass

    Returns:
    gradients -- python dictionary containing:
                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)
                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)
                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)
                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)
                        dWi -- Gradient w.r.t. the weight matrix of the input gate, numpy array of shape (n_a, n_a + n_x)
                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)
                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)
                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)
                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)
                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)
                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)
    """

    # Retrieve information from "cache"
    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache
    
    ### START CODE HERE ###
    # Retrieve dimensions from xt's and a_next's shape (≈2 lines)
    n_x, m = xt.shape
    n_a, m = a_next.shape
    
    # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)
    dot = da_next * np.tanh(c_next) * ot * (1 - ot)
    dcct = (dc_next * it + ot * (1 - np.square(np.tanh(c_next))) * it * da_next) * (1 - np.square(cct))
    dit = (dc_next * cct + ot * (1 - np.square(np.tanh(c_next))) * cct * da_next) * it * (1 - it)
    dft = (dc_next * c_prev + ot *(1 - np.square(np.tanh(c_next))) * c_prev * da_next) * ft * (1 - ft)
 

    # Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines)
    dWf = np.dot(dft,np.concatenate((a_prev, xt), axis=0).T)
    dWi = np.dot(dit,np.concatenate((a_prev, xt), axis=0).T)
    dWc = np.dot(dcct,np.concatenate((a_prev, xt), axis=0).T)
    dWo = np.dot(dot,np.concatenate((a_prev, xt), axis=0).T)
    dbf = np.sum(dft, axis=1 ,keepdims = True)
    dbi = np.sum(dit, axis=1, keepdims = True)
    dbc = np.sum(dcct, axis=1,  keepdims = True)
    dbo = np.sum(dot, axis=1, keepdims = True)

    # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)
    da_prev = np.dot(parameters['Wf'][:,:n_a].T,dft)+np.dot(parameters['Wi'][:,:n_a].T,dit)+np.dot(parameters['Wc'][:,:n_a].T,dcct)+np.dot(parameters['Wo'][:,:n_a].T,dot)
    dc_prev = dc_next*ft+ot*(1-np.square(np.tanh(c_next)))*ft*da_next
    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft)+np.dot(parameters['Wi'][:,n_a:].T,dit)+np.dot(parameters['Wc'][:,n_a:].T,dcct)+np.dot(parameters['Wo'][:,n_a:].T,dot)
    # parameters['Wf'][:, :n_a].T 每一行的 第 0 到 n_a-1 列的数据取出来
    # parameters['Wf'][:, n_a:].T 每一行的 第 n_a 到最后列的数据取出来
    ### END CODE HERE ###
    
    # Save gradients in dictionary
    gradients = {"dxt": dxt, "da_prev": da_prev, "dc_prev": dc_prev, "dWf": dWf,"dbf": dbf, "dWi": dWi,"dbi": dbi,
                "dWc": dWc,"dbc": dbc, "dWo": dWo,"dbo": dbo}

    return gradients</code></pre> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d1d856d0bf53f435b1e68e235891070f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Mac OS安装brew出现错误的解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d038d9636422532e866fcff96d3edc6a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">修改远程桌面的端口最简单最详细过程（亲测可用）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>