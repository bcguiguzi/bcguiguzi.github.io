<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习自学记录（6）——标准化、归一化和BatchNormal的理解 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习自学记录（6）——标准化、归一化和BatchNormal的理解" />
<meta property="og:description" content="深度学习自学记录（6）——归一化和BatchNormal的理解 1、标准化与归一化1.1归一化的目的和优势1.2常用的数据标准化方法 2、BatchNormal2.1BN算法强大之处2.2BN算法诞生的背景与目的2.3BN算法思路2.4BN算法实战2.5Keras中的Batch Normalization 3、总结4、参考 1、标准化与归一化 数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间，或有特定的数据分布。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。标准化不会改变数据分布。
其中最典型的数据标准化处理就是数据的归一化，即将数据统一映射到[0,1]区间上。
1.1归一化的目的和优势 目的：
1、把数变为（0，1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速
2、把有量纲表达式变为无量纲表达式。 归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量，便于不同单位或量级的指标能够进行比较和加权
总得来说：归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响
优势：
1、提升模型的收敛速度
2、提升模型的精度；在涉及到一些距离计算的算法时效果显著，可以让各个特征对结果做出的贡献相同。从经验上说，归一化是让各指标值都处于同一个数量级别上，在数值上有一定比较性，可以大大提高分类器的准确性。
3.、深度学习中数据归一化可以防止模型梯度爆炸。
1.2常用的数据标准化方法 最常用的是 min-max标准化 和 z-score 标准化：
min-max标准化：
数据的归一化，即将数据统一映射到[0,1]区间上。
z-score 标准化：
最常见的标准化方法就是Z标准化，也叫标准差标准化，这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1。也可以说把数据归一化至：均值0、方差为1的数据分布。
转换函数为：
x* = (x - μ ) / σ
其中μ为所有样本数据的均值，σ为所有样本数据的标准差。
这两种最常用方法使用场景：
1、在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，第二种方法(Z-score standardization)表现更好。
2、在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。
2、BatchNormal 2.1BN算法强大之处 (1)你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；
(2)你再也不用去理会过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；
(3)再也不需要使用使用局部响应归一化层了（局部响应归一化是Alexnet网络用到的方法，搞视觉的估计比较熟悉），因为BN本身就是一个归一化网络层；
(4)可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度，这句话我也是百思不得其解啊）。
2.2BN算法诞生的背景与目的 神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。
对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化，具有同样的数据分布)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了Batch Normalization
2.3BN算法思路 为了防止因低层网络在训练的时候更新了参数，而引起后面层输入数据分布的变化；我们想到如果在每一层输入的时候，再加个预处理操作那该有多好啊，所以BN层的本质原理：在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。
用这个公式，对某一个层网络的输入数据做一个归一化处理。需要注意的是，我们训练过程中采用batch
随机梯度下降，上面的E(xk)指的是每一批训练数据神经元xk的平均值；然后分母就是每一批数据神经元xk激活度的一个标准差了。
但是有一个问题需要解决：如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的，使得特征表达变弱
于是文献使出了一招惊天地泣鬼神的招式：变换重构，引入了可学习参数γ、β，对归一化后的数据x进行（线性变换）变换重构。根据训练数据的每一个batch数据求得均值u、标准差σ 以及γ、β
进一步深入理解参数γ、β的作用：
γ、β在训练过程中学习到对应的两个调节因子，对规范到均值为 0 ，方差为1 的值进行微调。因为经过第一步操作后，Normalization有可能降低神经网络的非线性表达能力，所以会以此方式来补偿 Normalization 操作后神经网络的表达能力。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/ddbe02c6893ccfcb4b4a16cc42b28983/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-11T18:44:05+08:00" />
<meta property="article:modified_time" content="2020-05-11T18:44:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习自学记录（6）——标准化、归一化和BatchNormal的理解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>深度学习自学记录（6）——归一化和BatchNormal的理解</h4> 
 <ul><li><a href="#1_1" rel="nofollow">1、标准化与归一化</a></li><li><ul><li><a href="#11_4" rel="nofollow">1.1归一化的目的和优势</a></li><li><a href="#12_13" rel="nofollow">1.2常用的数据标准化方法</a></li></ul> 
  </li><li><a href="#2BatchNormal_27" rel="nofollow">2、BatchNormal</a></li><li><ul><li><a href="#21BN_28" rel="nofollow">2.1BN算法强大之处</a></li><li><a href="#22BN_36" rel="nofollow">2.2BN算法诞生的背景与目的</a></li><li><a href="#23BN_39" rel="nofollow">2.3BN算法思路</a></li><li><a href="#24BN_60" rel="nofollow">2.4BN算法实战</a></li><li><a href="#25KerasBatch_Normalization_72" rel="nofollow">2.5Keras中的Batch Normalization</a></li></ul> 
  </li><li><a href="#3_93" rel="nofollow">3、总结</a></li><li><a href="#4_105" rel="nofollow">4、参考</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1_1"></a>1、标准化与归一化</h2> 
<p><strong>数据的标准化</strong>（normalization）<strong>是将数据按比例缩放，使之落入一个小的特定区间，或有特定的数据分布</strong>。在某些比较和评价的指标处理中经常会用到，<strong>去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权</strong>。标准化不会改变数据分布。<br> 其中最典型的数据标准化处理就是数据的归一化，即将数据统一映射到[0,1]区间上。</p> 
<h3><a id="11_4"></a>1.1归一化的目的和优势</h3> 
<p><strong>目的</strong>：<br> <strong>1</strong>、把数变为（0，1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速<br> <strong>2</strong>、把有量纲表达式变为无量纲表达式。 归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量，便于不同单位或量级的指标能够进行比较和加权<br> 总得来说：<strong>归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响</strong><br> <strong>优势</strong>：<br> <strong>1</strong>、提升模型的收敛速度<br> <strong>2</strong>、提升模型的精度；在涉及到一些距离计算的算法时效果显著，可以让各个特征对结果做出的贡献相同。从经验上说，归一化是让各指标值都处于同一个数量级别上，在数值上有一定比较性，可以大大提高分类器的准确性。<br> <strong>3</strong>.、深度学习中数据归一化可以防止模型梯度爆炸。</p> 
<h3><a id="12_13"></a>1.2常用的数据标准化方法</h3> 
<p>最常用的是 min-max标准化 和 z-score 标准化：<br> <strong>min-max标准化</strong>：<br> 数据的归一化，<strong>即将数据统一映射到[0,1]区间上</strong>。<br> <img src="https://images2.imgbox.com/90/9b/zf0eL8sD_o.png" alt="在这里插入图片描述"><br> <strong>z-score 标准化</strong>：<br> 最常见的标准化方法就是Z标准化，也叫标准差标准化，这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。<strong>经过处理的数据符合标准正态分布，即均值为0，标准差为1</strong>。<strong>也可以说把数据归一化至：均值0、方差为1的数据分布</strong>。<br> 转换函数为：<br> x* = (x - μ ) / σ<br> 其中μ为所有样本数据的均值，σ为所有样本数据的标准差。</p> 
<p><strong>这两种最常用方法使用场景</strong>：<br> 1、在分类、聚类算法中，<strong>需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候</strong>，第二种方法(Z-score standardization)表现更好。<br> 2、<strong>在不涉及距离度量、协方差计算、数据不符合正太分布的时候</strong>，可以使用第一种方法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。</p> 
<h2><a id="2BatchNormal_27"></a>2、BatchNormal</h2> 
<h3><a id="21BN_28"></a>2.1BN算法强大之处</h3> 
<p>(1)你<strong>可以选择比较大的初始学习率，让你的训练速度飙涨</strong>。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；</p> 
<p>(2)你再也不用去理会过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，<strong>因为BN具有提高网络泛化能力的特性</strong>；</p> 
<p>(3)再也不需要使用使用局部响应归一化层了（局部响应归一化是Alexnet网络用到的方法，搞视觉的估计比较熟悉），因为<strong>BN本身就是一个归一化网络层</strong>；</p> 
<p>(4)<strong>可以把训练数据彻底打乱</strong>（<strong>防止每批训练的时候，某一个样本都经常被挑选到</strong>，文献说这个可以提高1%的精度，这句话我也是百思不得其解啊）。</p> 
<h3><a id="22BN_36"></a>2.2BN算法诞生的背景与目的</h3> 
<p><strong>神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因</strong>。<br> 对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(<strong>因为输入层数据，我们已经人为的为每个样本归一化，具有同样的数据分布</strong>)，后面网络每一层的<strong>输入数据分布</strong>是一直在发生变化的，<strong>因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化</strong>。就是<strong>要解决在训练过程中，中间层数据分布发生改变的情况</strong>，于是就有了Batch Normalization</p> 
<h3><a id="23BN_39"></a>2.3BN算法思路</h3> 
<p>为了防止因低层网络在训练的时候更新了参数，而引起后面层输入数据分布的变化；我们想到如果在每一层输入的时候，再加个预处理操作那该有多好啊，<strong>所以BN层的本质原理：在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层</strong>。</p> 
<blockquote> 
 <p><img src="https://images2.imgbox.com/3b/79/kJ2p6flz_o.png" alt="在这里插入图片描述"><br> 用这个公式，对某一个层网络的输入数据做一个归一化处理。需要注意的是，我们训练过程中采用batch<br> 随机梯度下降，上面的E(xk)指的是每一批训练数据神经元xk的平均值；然后分母就是每一批数据神经元xk激活度的一个标准差了。</p> 
</blockquote> 
<p>但是有一个问题需要解决：<strong>如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的，使得特征表达变弱</strong><br> 于是文献使出了一招惊天地泣鬼神的招式：变换重构，引入了可学习参数γ、β，对归一化后的数据x进行（线性变换）变换重构。<strong>根据训练数据的每一个batch数据求得均值u、标准差σ 以及γ、β</strong><br> <img src="https://images2.imgbox.com/a7/1d/f4Z7uTKT_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p><strong>进一步深入理解参数γ、β的作用</strong>：<br> γ、β在训练过程中学习到对应的两个调节因子，对规范到均值为 0 ，方差为1 的值进行微调。因为经过第一步操作后，Normalization有可能降低神经网络的非线性表达能力，所以会以此方式来补偿 Normalization 操作后神经网络的表达能力。<br> γ和β为输出的线性调整参数，可以让分布曲线压缩或延长一点，左移或右移一点。由于γ和β是可训练的，那么意味着神经网络会随着训练过程自己挑选一个最适合的分布。</p> 
</blockquote> 
<p><strong>每一个神经元xk都会有一对这样的参数γ、β</strong>。<strong>这样其实当</strong>：<br> <img src="https://images2.imgbox.com/da/c7/SHOkUrC7_o.png" alt="在这里插入图片描述"><br> 是可以恢复出原始的某一层所学到的特征的。<strong>因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布</strong>。</p> 
<p>BN算法的全过程：<br> 我们获得了一个mini-batch的输入，公式中m指的是mini-batch size<br> <img src="https://images2.imgbox.com/c8/ac/p2PyXOs6_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="24BN_60"></a>2.4BN算法实战</h3> 
<p>（1）<strong>预测阶段中的BN</strong>：<br> 个网络一旦训练完了，就没有了min-batch这个概念了。测试阶段中，测试样本，前向传导的时候，上面的均值u、标准差σ 要哪里来？其实网络一旦训练完毕，参数都是固定的，<strong>训练过程中的计算得到的方差和均值会被记录下来</strong>，那么BN层计算的均值u、和标准差都是固定不变的，<strong>我们可以采用这些数值来作为测试样本所需要的均值、标准差</strong>。<strong>对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch σB的无偏估计。</strong><br> <img src="https://images2.imgbox.com/68/3d/6NmQZ3bY_o.png" alt="在这里插入图片描述"></p> 
<p>（2）<strong>CNN中的BN</strong>：<br> <strong>在CNN中操作时，BN算法把每个特征图看作一个神经元，计算该特征图对应数据的均值和方差进行归一化，并且每个特征图对应两个学习变量γ和β</strong>。</p> 
<p>在卷积神经网络中，经过卷积后得到的是一系列的特征图，如果min-batch sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,p,q)，m为min-batch sizes，f为特征图个数，p、q分别为特征图的宽高。在cnn中我们可以把<strong>每个特征图看成是一个特征处理（一个神经元）</strong>，因此在使用Batch Normalization，mini-batch size 的大小就是：m x p x q，<strong>于是对于每个特征图（神经元）都只有一对可学习参数：γ、β</strong>，<strong>一共会得到 f 对γ、β的值</strong>。<strong>特征图里的值，作为BN的输入，也就是这一个m x p x q个数值通过BN计算并保存均值与方差，并通过当前均值与方差计算归一化的值，最后根据γ,β以及归一化得值计算BN层输出</strong></p> 
<p>说白了吧，<strong>这就是相当于求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对该特征图神经元做归一化，然后利用训练得到的γ、β增强特征表达</strong>。</p> 
<h3><a id="25KerasBatch_Normalization_72"></a>2.5Keras中的Batch Normalization</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>normalization<span class="token punctuation">.</span>BatchNormalization<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> center<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> scale<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> beta_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> gamma_initializer<span class="token operator">=</span><span class="token string">'ones'</span><span class="token punctuation">,</span> moving_mean_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> moving_variance_initializer<span class="token operator">=</span><span class="token string">'ones'</span><span class="token punctuation">,</span> beta_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> gamma_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> beta_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> gamma_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>功能为</strong>：该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1</p> 
<blockquote> 
 <p>axis:整数，指定要规范化的轴，通常为特征轴。例如在进行data_format="channels_first的2D卷积后，一般会设axis=1。<br> momentum: 动态均值的动量 epsilon：大于0的小浮点数，用于防止除0错误<br> center:若设为True，将会将beta作为偏置加上去，否则忽略参数beta<br> scale:若设为True，则会乘以gamma，否则不使用gamma。当下一层是线性的时，可以设False，因为scaling的操作将被下一层执行。<br> beta_initializer：beta权重的初始方法 gamma_initializer: gamma的初始化方法<br> moving_mean_initializer: 动态均值的初始化方法<br> moving_variance_initializer:动态方差的初始化方法<br> beta_regularizer: 可选的beta正则<br> gamma_regularizer: 可选的gamma正则<br> beta_constraint: 可选的beta约束<br> gamma_constraint: 可选的gamma约束</p> 
</blockquote> 
<p><strong>在Keras的BatchNormalization层中，如输入的神经元数目（或者CNN中输入的特征图数目）为N，则BatchNormalization的参数数目为N x 2 x 2，其中2N个参数是可训练的，对应于λ与β；而剩下的2N个参数是不可训练的，个人理解是 momentum, 和epsilon</strong><br> keras在训练过程中会自动地更新BN层的均值和方差，在测试过程中停止更新参数。</p> 
<h2><a id="3_93"></a>3、总结</h2> 
<p><strong>训练阶段和测试阶段中BN层的数据是不一样的，训练阶段为每一个batch数据计算出结果，而测试阶段为基于所有batch的结果（均值、方差）计算出来的结果。</strong><br> 对于BN层：<br> <img src="https://images2.imgbox.com/81/6e/EXyxDbS1_o.png" alt="在这里插入图片描述"><br> 输入：待进入激活函数的变量<br> 输出：<br> 1.对于K输入，所以需要K个循环。每个循环中按照上面所介绍的方法计算均值与方差。通过γ,β与输入x的变换求出BN层输出。<br> 2.在反向传播时利用γ与β求得梯度从而改变训练权值（变量）。<br> 3.<strong>通过不断迭代直到训练结束，得到γ与β，以及记录的均值方差</strong>。<br> 4.<strong>在预测的正向传播时，使用训练时最后得到的γ与β，以及所有batch u值的平均值、每个batch 方差的无偏估计</strong>，利用上图中的第10步可以求得预测用的均值和方差，用第11步计算预测过程中BN层输出。</p> 
<h2><a id="4_105"></a>4、参考</h2> 
<p>https://blog.csdn.net/zenghaitao0128/article/details/78361038<br> https://blog.csdn.net/hjimce/article/details/50866313</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/66e7fbb041cfc4fe9ec2124aad2d776a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">以太坊智能合约开发基础概念</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4a06bca625f8e7bb89f8dfe0cf81b2ec/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vs 外部依赖项、附加依赖项以及如何添加依赖项目</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>