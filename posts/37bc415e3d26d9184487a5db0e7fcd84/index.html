<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【Pytorch】BCELoss和BCEWithLogitsLoss损失函数详解 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【Pytorch】BCELoss和BCEWithLogitsLoss损失函数详解" />
<meta property="og:description" content="在Pytorch中，BCELoss和BCEWithLogitsLoss是一组常用的二元交叉熵损失函数，常用于二分类问题，其区别在于前者的输入为已进行sigmoid处理过的值，而后者为sigmoid函数 1 1 &#43; exp ⁡ ( − x ) \frac{1}{1&#43;\exp(-x)} 1&#43;exp(−x)1​中的 x x x。
下面为一个简单的示例：
import torch import torch.nn as nn predicts = torch.tensor([[0.4,0.7,1.2,0.3], [1.1,0.6,0.9,1.6]]) labels = torch.tensor([[1,0,1,0],[0,1,1,0]], dtype=torch.float) # 通过BCELoss计算sigmoid处理后的值 criterion1 = nn.BCELoss() loss1= criterion1(torch.sigmoid(predicts), labels) # 通过BCEWithLogitsLoss直接计算输入值 criterion2 = nn.BCEWithLogitsLoss() loss2 = criterion2(predicts, labels) # 会发现loss1=loss2 BCELoss和BCEWithLogitsLoss还提供了两个重要参数：
weight：可用于控制各样本的权重，常用作对对齐后的数据进行mask操作（设为0）reduction：控制损失输出模式。设为&#34;sum&#34;表示对样本进行求损失和；设为&#34;mean&#34;表示对样本进行求损失的平均值；而设为&#34;none&#34;表示对样本逐个求损失，输出与输入的shape一样。 此外BCEWithLogitsLoss还提供了参数pos_weight用于设置损失的class权重，用于缓解样本的不均衡问题。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/37bc415e3d26d9184487a5db0e7fcd84/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-18T20:34:36+08:00" />
<meta property="article:modified_time" content="2020-02-18T20:34:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【Pytorch】BCELoss和BCEWithLogitsLoss损失函数详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>在Pytorch中，<code>BCELoss</code>和<code>BCEWithLogitsLoss</code>是一组常用的<strong>二元交叉熵损失函数</strong>，常用于二分类问题，其区别在于前者的输入为已进行<code>sigmoid</code>处理过的值，而后者为<code>sigmoid</code>函数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          1 
         
         
         
           1 
          
         
           + 
          
         
           exp 
          
         
           ⁡ 
          
         
           ( 
          
         
           − 
          
         
           x 
          
         
           ) 
          
         
        
       
      
        \frac{1}{1+\exp(-x)} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.36511em; vertical-align: -0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mop mtight">exp</span><span class="mopen mtight">(</span><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mclose mtight">)</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.52em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>中的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>。</p> 
<p>下面为一个简单的示例：</p> 
<pre><code class="prism language-pythoin">import torch
import torch.nn as nn

predicts = torch.tensor([[0.4,0.7,1.2,0.3], [1.1,0.6,0.9,1.6]])
labels = torch.tensor([[1,0,1,0],[0,1,1,0]], dtype=torch.float)

# 通过BCELoss计算sigmoid处理后的值
criterion1 = nn.BCELoss()
loss1= criterion1(torch.sigmoid(predicts), labels)

# 通过BCEWithLogitsLoss直接计算输入值
criterion2 = nn.BCEWithLogitsLoss()
loss2 = criterion2(predicts, labels)

# 会发现loss1=loss2
</code></pre> 
<p><code>BCELoss</code>和<code>BCEWithLogitsLoss</code>还提供了两个重要参数：</p> 
<ul><li><code>weight</code>：可用于控制各样本的权重，常用作对对齐后的数据进行<code>mask</code>操作（设为0）</li><li><code>reduction</code>：控制损失输出模式。设为"sum"表示对样本进行求损失和；设为"mean"表示对样本进行求损失的平均值；而设为"none"表示对样本逐个求损失，输出与输入的shape一样。</li></ul> 
<p>此外<code>BCEWithLogitsLoss</code>还提供了参数<code>pos_weight</code>用于设置损失的class权重，用于缓解样本的不均衡问题。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/296ff96a8841a7c2cbe4a9eace493600/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">visio画两条直线交叉但不弯曲不跨线</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/16d91db8462515ad086f3d7e41635ba1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">动态规划经典例题汇总 （附最全题目链接）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>