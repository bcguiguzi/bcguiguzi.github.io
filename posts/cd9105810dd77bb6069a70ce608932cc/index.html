<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch-textclassification是一个专注于中文文本分类（多类分类、多标签分类）的轻量级自然语言处理工具包，基于pytorch和transformers，包含各种实验 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch-textclassification是一个专注于中文文本分类（多类分类、多标签分类）的轻量级自然语言处理工具包，基于pytorch和transformers，包含各种实验" />
<meta property="og:description" content="pytorch-textclassification pytorch-textclassification是一个以pytorch和transformers为基础，专注于文本分类的轻量级自然语言处理工具包。支持中文长文本、短文本的多类分类和多标签分类。
目录 数据使用方式paper参考 项目地址 pytorch-textclassification: https://github.com/yongzhuo/Pytorch-NLU 数据 数据来源 所有数据集均来源于网络，只做整理供大家提取方便，如果有侵权等问题，请及时联系删除。
baidu_event_extract_2020, 项目以 2020语言与智能技术竞赛：事件抽取任务中的数据作为多分类标签的样例数据，借助多标签分类模型来解决, 共13456个样本, 65个类别;AAPD-dataset, 数据集出现在论文-SGM: Sequence Generation Model for Multi-label Classification, 英文多标签分类语料, 共55840样本, 54个类别;toutiao-news, 今日头条新闻标题, 多标签分类语料, 约300w-语料, 1000&#43;类别;unknow-data, 来源未知, 多标签分类语料, 约22339语料, 7个类别;SMP2018中文人机对话技术评测（ECDT）, SMP2018 中文人机对话技术评测（SMP2018-ECDT）比赛语料, 短文本意图识别语料, 多类分类, 共3069样本, 31个类别;文本分类语料库（复旦）语料, 复旦大学计算机信息与技术系国际数据库中心自然语言处理小组提供的新闻语料, 多类分类语料, 共9804篇文档，分为20个类别。MiningZhiDaoQACorpus, 中国科学院软件研究所刘焕勇整理的问答语料, 百度知道问答语料, 可以把领域当作类别, 多类分类语料, 100w&#43;样本, 共17个类别;THUCNEWS, 清华大学自然语言处理实验室整理的语料, 新浪新闻RSS订阅频道2005-2011年间的历史数据筛选, 多类分类语料, 74w新闻文档, 14个类别;IFLYTEK, 科大讯飞开源的长文本分类语料, APP应用描述的标注数据，包含和日常生活相关的各类应用主题, 链接为CLUE, 共17333样例, 119个类别;TNEWS, 今日头条提供的中文新闻标题分类语料, 数据集来自今日头条的新闻版块, 链接为CLUE, 共73360样例, 15个类别; 数据格式 1. 文本分类 (txt格式, 每行为一个json): 1.1 多类分类格式: {&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/cd9105810dd77bb6069a70ce608932cc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-05T16:36:17+08:00" />
<meta property="article:modified_time" content="2022-12-05T16:36:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch-textclassification是一个专注于中文文本分类（多类分类、多标签分类）的轻量级自然语言处理工具包，基于pytorch和transformers，包含各种实验</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="pytorchtextclassificationhttpsgithubcomyongzhuoPytorchNLUpytorch_textclassification_1"></a><a href="https://github.com/yongzhuo/Pytorch-NLU/pytorch_textclassification"><em><strong>pytorch-textclassification</strong></em></a></h2> 
<blockquote> 
 <blockquote> 
  <blockquote> 
   <p>pytorch-textclassification是一个以pytorch和transformers为基础，专注于文本分类的轻量级自然语言处理工具包。支持中文长文本、短文本的多类分类和多标签分类。</p> 
  </blockquote> 
 </blockquote> 
</blockquote> 
<h3><a id="_5"></a>目录</h3> 
<ul><li><a href="#%E6%95%B0%E6%8D%AE" rel="nofollow">数据</a></li><li><a href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F" rel="nofollow">使用方式</a></li><li><a href="#paper" rel="nofollow">paper</a></li><li><a href="#%E5%8F%82%E8%80%83" rel="nofollow">参考</a></li></ul> 
<h2><a id="_12"></a>项目地址</h2> 
<ul><li>pytorch-textclassification: <a href="https://github.com/yongzhuo/Pytorch-NLU">https://github.com/yongzhuo/Pytorch-NLU</a></li></ul> 
<h2><a id="_16"></a>数据</h2> 
<h3><a id="_17"></a>数据来源</h3> 
<p>所有数据集均来源于网络，只做整理供大家提取方便，如果有侵权等问题，请及时联系删除。</p> 
<ul><li><a href="https://aistudio.baidu.com/aistudio/competition/detail/32?isFromCcf=true" rel="nofollow">baidu_event_extract_2020</a>, 项目以 2020语言与智能技术竞赛：事件抽取任务中的数据作为<a href="https://github.com/percent4/keras_bert_multi_label_cls">多分类标签的样例数据</a>，借助多标签分类模型来解决, 共13456个样本, 65个类别;</li><li><a href="https://git.uwaterloo.ca/jimmylin/Castor-data/tree/master/datasets/AAPD" rel="nofollow">AAPD-dataset</a>, 数据集出现在论文-SGM: Sequence Generation Model for Multi-label Classification, 英文多标签分类语料, 共55840样本, 54个类别;</li><li><a href="https://github.com/fate233/toutiao-multilevel-text-classfication-dataset">toutiao-news</a>, 今日头条新闻标题, 多标签分类语料, 约300w-语料, 1000+类别;</li><li><a href="https://github.com/FBI1314/textClassification/tree/master/multilabel_text_classfication/data">unknow-data</a>, 来源未知, 多标签分类语料, 约22339语料, 7个类别;</li><li><a href="https://worksheets.codalab.org/worksheets/0x27203f932f8341b79841d50ce0fd684f/" rel="nofollow">SMP2018中文人机对话技术评测（ECDT）</a>, SMP2018 中文人机对话技术评测（SMP2018-ECDT）比赛语料, 短文本意图识别语料, 多类分类, 共3069样本, 31个类别;</li><li><a href="http://www.nlpir.org/wordpress/2017/10/02/%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%e8%af%ad%e6%96%99%e5%ba%93%ef%bc%88%e5%a4%8d%e6%97%a6%ef%bc%89%e6%b5%8b%e8%af%95%e8%af%ad%e6%96%99/" rel="nofollow">文本分类语料库（复旦）语料</a>, 复旦大学计算机信息与技术系国际数据库中心自然语言处理小组提供的新闻语料, 多类分类语料, 共9804篇文档，分为20个类别。</li><li><a href="https://github.com/liuhuanyong/MiningZhiDaoQACorpus">MiningZhiDaoQACorpus</a>, 中国科学院软件研究所刘焕勇整理的问答语料, 百度知道问答语料, 可以把领域当作类别, 多类分类语料, 100w+样本, 共17个类别;</li><li><a href="http://thuctc.thunlp.org/" rel="nofollow">THUCNEWS</a>, 清华大学自然语言处理实验室整理的语料, 新浪新闻RSS订阅频道2005-2011年间的历史数据筛选, 多类分类语料, 74w新闻文档, 14个类别;</li><li><a href="https://storage.googleapis.com/cluebenchmark/tasks/iflytek_public.zip" rel="nofollow">IFLYTEK</a>, 科大讯飞开源的长文本分类语料, APP应用描述的标注数据，包含和日常生活相关的各类应用主题, 链接为CLUE, 共17333样例, 119个类别;</li><li><a href="https://storage.googleapis.com/cluebenchmark/tasks/tnews_public.zip" rel="nofollow">TNEWS</a>, 今日头条提供的中文新闻标题分类语料, 数据集来自今日头条的新闻版块, 链接为CLUE, 共73360样例, 15个类别;</li></ul> 
<h3><a id="_31"></a>数据格式</h3> 
<pre><code>1. 文本分类  (txt格式, 每行为一个json):

1.1 多类分类格式:
{"text": "人站在地球上为什么没有头朝下的感觉", "label": "教育"}
{"text": "我的小baby", "label": "娱乐"}
{"text": "请问这起交通事故是谁的责任居多小车和摩托车发生事故在无红绿灯", "label": "娱乐"}

1.2 多标签分类格式:
{"label": "3|myz|5", "text": "课堂搞东西，没认真听"}
{"label": "3|myz|2", "text": "测验90-94.A-"}
{"label": "3|myz|2", "text": "长江作业未交"}

</code></pre> 
<h2><a id="_48"></a>使用方式</h2> 
<p>更多样例sample详情见test/tc目录</p> 
<h3><a id="TC_TextClassification_50"></a>文本分类(TC), Text-Classification</h3> 
<pre><code class="prism language-bash"><span class="token comment"># !/usr/bin/python</span>
<span class="token comment"># -*- coding: utf-8 -*-</span>
<span class="token comment"># !/usr/bin/python</span>
<span class="token comment"># -*- coding: utf-8 -*-</span>
<span class="token comment"># @time    : 2021/2/23 21:34</span>
<span class="token comment"># @author  : Mo</span>
<span class="token comment"># @function: 多标签分类, 根据label是否有|myz|分隔符判断是多类分类, 还是多标签分类</span>


<span class="token comment"># 适配linux</span>
<span class="token function">import</span> platform
<span class="token function">import</span> json
<span class="token function">import</span> sys
<span class="token function">import</span> os
path_root <span class="token operator">=</span> os.path.abspath<span class="token punctuation">(</span>os.path.join<span class="token punctuation">(</span>os.path.dirname<span class="token punctuation">(</span>__file__<span class="token punctuation">)</span>, <span class="token string">"../.."</span><span class="token punctuation">))</span>
sys.path.append<span class="token punctuation">(</span>os.path.join<span class="token punctuation">(</span>path_root, <span class="token string">"pytorch_textclassification"</span><span class="token punctuation">))</span>
print<span class="token punctuation">(</span>path_root<span class="token punctuation">)</span>
<span class="token comment"># 分类下的引入, pytorch_textclassification</span>
from tcTools <span class="token function">import</span> get_current_time
from tcRun <span class="token function">import</span> TextClassification
from tcConfig <span class="token function">import</span> model_config

evaluate_steps <span class="token operator">=</span> <span class="token number">320</span>  <span class="token comment"># 评估步数</span>
save_steps <span class="token operator">=</span> <span class="token number">320</span>  <span class="token comment"># 存储步数</span>
<span class="token comment"># pytorch预训练模型目录, 必填</span>
pretrained_model_name_or_path <span class="token operator">=</span> <span class="token string">"bert-base-chinese"</span>
<span class="token comment"># 训练-验证语料地址, 可以只输入训练地址</span>
path_corpus <span class="token operator">=</span> os.path.join<span class="token punctuation">(</span>path_root, <span class="token string">"corpus"</span>, <span class="token string">"text_classification"</span>, <span class="token string">"school"</span><span class="token punctuation">)</span>
path_train <span class="token operator">=</span> os.path.join<span class="token punctuation">(</span>path_corpus, <span class="token string">"train.json"</span><span class="token punctuation">)</span>
path_dev <span class="token operator">=</span> os.path.join<span class="token punctuation">(</span>path_corpus, <span class="token string">"dev.json"</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token builtin class-name">:</span>
 
    model_config<span class="token punctuation">[</span><span class="token string">"evaluate_steps"</span><span class="token punctuation">]</span> <span class="token operator">=</span> evaluate_steps  <span class="token comment"># 评估步数</span>
    model_config<span class="token punctuation">[</span><span class="token string">"save_steps"</span><span class="token punctuation">]</span> <span class="token operator">=</span> save_steps  <span class="token comment"># 存储步数</span>
    model_config<span class="token punctuation">[</span><span class="token string">"path_train"</span><span class="token punctuation">]</span> <span class="token operator">=</span> path_train  <span class="token comment"># 训练模语料, 必须</span>
    model_config<span class="token punctuation">[</span><span class="token string">"path_dev"</span><span class="token punctuation">]</span> <span class="token operator">=</span> path_dev      <span class="token comment"># 验证语料, 可为None</span>
    model_config<span class="token punctuation">[</span><span class="token string">"path_tet"</span><span class="token punctuation">]</span> <span class="token operator">=</span> None          <span class="token comment"># 测试语料, 可为None</span>
    <span class="token comment"># 损失函数类型,</span>
    <span class="token comment"># multi-class:  可选 None(BCE), BCE, BCE_LOGITS, MSE, FOCAL_LOSS, DICE_LOSS, LABEL_SMOOTH</span>
    <span class="token comment"># multi-label:  SOFT_MARGIN_LOSS, PRIOR_MARGIN_LOSS, FOCAL_LOSS, CIRCLE_LOSS, DICE_LOSS等</span>
    model_config<span class="token punctuation">[</span><span class="token string">"path_tet"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"FOCAL_LOSS"</span>
    os.environ<span class="token punctuation">[</span><span class="token string">"CUDA_VISIBLE_DEVICES"</span><span class="token punctuation">]</span> <span class="token operator">=</span> str<span class="token punctuation">(</span>model_config<span class="token punctuation">[</span><span class="token string">"CUDA_VISIBLE_DEVICES"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    model_config<span class="token punctuation">[</span><span class="token string">"pretrained_model_name_or_path"</span><span class="token punctuation">]</span> <span class="token operator">=</span> pretrained_model_name_or_path
    model_config<span class="token punctuation">[</span><span class="token string">"model_save_path"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"../output/text_classification/model_{}"</span>.format<span class="token punctuation">(</span>model_type<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">)</span>
    model_config<span class="token punctuation">[</span><span class="token string">"model_type"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"BERT"</span>
    <span class="token comment"># main</span>
    lc <span class="token operator">=</span> TextClassification<span class="token punctuation">(</span>model_config<span class="token punctuation">)</span>
    lc.process<span class="token punctuation">(</span><span class="token punctuation">)</span>
    lc.train<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<h2><a id="paper_108"></a>paper</h2> 
<h3><a id="TC_TextClassification_109"></a>文本分类(TC), Text-Classification</h3> 
<ul><li>FastText: <a href="https://arxiv.org/abs/1607.01759" rel="nofollow">Bag of Tricks for Efﬁcient Text Classiﬁcation</a></li><li>TextCNN： <a href="https://arxiv.org/abs/1408.5882" rel="nofollow">Convolutional Neural Networks for Sentence Classiﬁcation</a></li><li>charCNN-kim： <a href="https://arxiv.org/abs/1508.06615" rel="nofollow">Character-Aware Neural Language Models</a></li><li>charCNN-zhang: <a href="https://arxiv.org/pdf/1509.01626.pdf" rel="nofollow">Character-level Convolutional Networks for Text Classiﬁcation</a></li><li>TextRNN： <a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf" rel="nofollow">Recurrent Neural Network for Text Classification with Multi-Task Learning</a></li><li>RCNN： <a href="http://www.nlpr.ia.ac.cn/cip/~liukang/liukangPageFile/Recurrent%20Convolutional%20Neural%20Networks%20for%20Text%20Classification.pdf" rel="nofollow">Recurrent Convolutional Neural Networks for Text Classification</a></li><li>DCNN: <a href="https://arxiv.org/abs/1404.2188" rel="nofollow">A Convolutional Neural Network for Modelling Sentences</a></li><li>DPCNN: <a href="https://www.aclweb.org/anthology/P17-1052" rel="nofollow">Deep Pyramid Convolutional Neural Networks for Text Categorization</a></li><li>VDCNN: <a href="https://www.aclweb.org/anthology/E17-1104" rel="nofollow">Very Deep Convolutional Networks</a></li><li>CRNN: <a href="https://arxiv.org/abs/1511.08630" rel="nofollow">A C-LSTM Neural Network for Text Classification</a></li><li>DeepMoji: <a href="https://arxiv.org/abs/1708.00524" rel="nofollow">Using millions of emojio ccurrences to learn any-domain represent ations for detecting sentiment, emotion and sarcasm</a></li><li>SelfAttention: <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">Attention Is All You Need</a></li><li>HAN: <a href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf" rel="nofollow">Hierarchical Attention Networks for Document Classification</a></li><li>CapsuleNet: <a href="https://arxiv.org/pdf/1710.09829.pdf" rel="nofollow">Dynamic Routing Between Capsules</a></li><li>Transformer(encode or decode): <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">Attention Is All You Need</a></li><li>Bert: <a href="" rel="nofollow">BERT: Pre-trainingofDeepBidirectionalTransformersfor LanguageUnderstanding</a></li><li>Xlnet: <a href="https://arxiv.org/abs/1906.08237" rel="nofollow">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></li><li>Albert: <a href="https://arxiv.org/pdf/1909.11942.pdf" rel="nofollow">ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</a></li><li>RoBERTa: <a href="https://arxiv.org/abs/1907.11692" rel="nofollow">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li>ELECTRA: <a href="https://openreview.net/pdf?id=r1xMH1BtvB" rel="nofollow">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></li><li>TextGCN: <a href="https://arxiv.org/abs/1809.05679" rel="nofollow">Graph Convolutional Networks for Text Classification</a></li></ul> 
<h2><a id="_133"></a>参考</h2> 
<p>This library is inspired by and references following frameworks and papers.</p> 
<ul><li>keras与tensorflow版本对应: <a href="https://docs.floydhub.com/guides/environments/" rel="nofollow">https://docs.floydhub.com/guides/environments/</a></li><li>bert4keras: <a href="https://github.com/bojone/bert4keras">https://github.com/bojone/bert4keras</a></li><li>Kashgari: <a href="https://github.com/BrikerMan/Kashgari">https://github.com/BrikerMan/Kashgari</a></li><li>fastNLP: <a href="https://github.com/fastnlp/fastNLP">https://github.com/fastnlp/fastNLP</a></li><li>HanLP: <a href="https://github.com/hankcs/HanLP">https://github.com/hankcs/HanLP</a></li><li>FGM: <a href="https://zhuanlan.zhihu.com/p/91269728" rel="nofollow">【炼丹技巧】功守道：NLP中的对抗训练 + PyTorch实现</a></li><li>pytorch-loss: <a href="https://github.com/CoinCheung/pytorch-loss">pytorch-loss</a></li><li>PriorLoss: <a href="https://spaces.ac.cn/archives/7615" rel="nofollow">通过互信息思想来缓解类别不平衡问题</a></li><li>CircleLoss: <a href="https://spaces.ac.cn/archives/7359" rel="nofollow">将“softmax+交叉熵”推广到多标签分类问题</a></li><li>FocalLoss: <a href="https://arxiv.org/abs/1708.02002" rel="nofollow">Focal Loss for Dense Object Detection</a></li><li>scikit-learn: <a href="https://github.com/scikit-learn/scikit-learn">https://github.com/scikit-learn/scikit-learn</a></li><li>tqdm: <a href="https://github.com/tqdm/tqdm">https://github.com/tqdm/tqdm</a></li></ul> 
<h2><a id="Reference_150"></a>Reference</h2> 
<p>For citing this work, you can refer to the present GitHub project. For example, with BibTeX:</p> 
<pre><code>@software{Pytorch-NLU,
    url = {https://github.com/yongzhuo/Pytorch-NLU},
    author = {Yongzhuo Mo},
    title = {Pytorch-NLU},
    year = {2021}
    
</code></pre> 
<p>*希望对你有所帮助!</p> 
<h2><a id="_164"></a>实验</h2> 
<h3><a id="corpusunknowdatahttpsgithubcomFBI1314textClassificationtreemastermultilabel_text_classficationdata_pretrainmodelernietiny_batch32_lr1e5_epoch21_165"></a>corpus==<a href="https://github.com/FBI1314/textClassification/tree/master/multilabel_text_classfication/data">unknow-data</a>, pretrain-model==ernie-tiny, batch=32, lr=1e-5, epoch=21</h3> 
<h4><a id="_167"></a>总结</h4> 
<h5><a id="micro_168"></a>micro-微平均</h5> 
<pre><code>              precision    recall  f1-score   support

   micro_avg     0.7920    0.7189    0.7537       466    MARGIN_LOSS
   micro_avg     0.6706    0.8519    0.7505       466    PRIOR-MARGIN_LOSS
   micro_avg     0.8258    0.6309    0.7153       466    FOCAL_LOSS【0.5, 2】
   micro_avg     0.7890    0.7382    0.7627       466    CIRCLE_LOSS
   micro_avg     0.7612    0.7661    0.7636       466    DICE_LOSS【直接学习F1?】
   micro_avg     0.8062    0.7232    0.7624       466    BCE
   micro_avg     0.7825    0.7103    0.7447       466    BCE-Logits
   micro_avg     0.7899    0.7017    0.7432       466    BCE-smooth
   micro_avg     0.7235    0.8197    0.7686       466    FOCAL_LOSS【0.5, 2】 + PRIOR-MARGIN_LOSS / 2
</code></pre> 
<h5><a id="macro_183"></a>macro-宏平均</h5> 
<pre><code>              precision    recall  f1-score   support

   macro_avg     0.6198    0.5338    0.5641       466    MARGIN_LOSS
   macro_avg     0.5103    0.7200    0.5793       466    PRIOR-MARGIN_LOSS
   macro_avg     0.7655    0.4973    0.5721       466    FOCAL_LOSS【0.5, 2】
   macro_avg     0.6275    0.5235    0.5627       466    CIRCLE_LOSS
   macro_avg     0.4287    0.3918    0.4025       466    DICE_LOSS【直接学习F1?】
   macro_avg     0.6978    0.5158    0.5828       466    BCE
   macro_avg     0.6046    0.5123    0.5433       466    BCE-Logits
   macro_avg     0.6963    0.5012    0.5721       466    BCE-smooth
   macro_avg     0.6033    0.6809    0.6369       466    FOCAL_LOSS【0.5, 2】 + PRIOR-MARGIN_LOSS / 2

</code></pre> 
<h4><a id="1_batch32_lossMARGIN_LOSS_lr1e5_epoch21_________199"></a>1. batch=32, loss=MARGIN_LOSS, lr=1e-5, epoch=21, 【精确率高些】</h4> 
<pre><code>              precision    recall  f1-score   support

           3     0.8102    0.7919    0.8009       221
           2     0.8030    0.8030    0.8030       132
           1     0.7333    0.4925    0.5893        67
           6     0.7143    0.5000    0.5882        10
           5     0.7778    0.4828    0.5957        29
           0     0.0000    0.0000    0.0000         4
           4     0.5000    0.6667    0.5714         3

   micro_avg     0.7920    0.7189    0.7537       466
   macro_avg     0.6198    0.5338    0.5641       466
weighted_avg     0.7841    0.7189    0.7454       466
</code></pre> 
<h4><a id="2_batch32_lossPRIORMARGIN_LOSS_lr1e5_epoch21___216"></a>2. batch=32, loss=PRIOR-MARGIN_LOSS, lr=1e-5, epoch=21, 【召回率高些】</h4> 
<pre><code>              precision    recall  f1-score   support

           3     0.7279    0.8959    0.8032       221
           2     0.7039    0.9545    0.8103       132
           1     0.5897    0.6866    0.6345        67
           6     0.3333    0.5000    0.4000        10
           5     0.6296    0.5862    0.6071        29
           0     0.1875    0.7500    0.3000         4
           4     0.4000    0.6667    0.5000         3

   micro_avg     0.6706    0.8519    0.7505       466
   macro_avg     0.5103    0.7200    0.5793       466
weighted_avg     0.6799    0.8519    0.7538       466
</code></pre> 
<h4><a id="3_batch32_lossFOCAL_LOSS05_2_lr1e5_epoch21__025_233"></a>3. batch=32, loss=FOCAL_LOSS【(0.5, 2)】, lr=1e-5, epoch=21, 【精确率超级高, 0.25效果会变差】</h4> 
<pre><code>              precision    recall  f1-score   support

           3     0.8482    0.7330    0.7864       221
           2     0.8349    0.6894    0.7552       132
           1     0.7586    0.3284    0.4583        67
           6     0.6667    0.4000    0.5000        10
           5     0.7500    0.4138    0.5333        29
           0     1.0000    0.2500    0.4000         4
           4     0.5000    0.6667    0.5714         3

   micro_avg     0.8258    0.6309    0.7153       466
   macro_avg     0.7655    0.4973    0.5721       466
weighted_avg     0.8206    0.6309    0.7038       466
</code></pre> 
<h4><a id="4_batch32_lossCIRCLE_LOSS_lr1e5_epoch21___250"></a>4. batch=32, loss=CIRCLE_LOSS【, lr=1e-5, epoch=21, 【效果很好, 精确率召回率相对比较均衡】</h4> 
<pre><code>              precision    recall  f1-score   support

           3     0.8125    0.8235    0.8180       221
           2     0.7914    0.8333    0.8118       132
           1     0.7333    0.4925    0.5893        67
           6     0.6667    0.4000    0.5000        10
           5     0.7222    0.4483    0.5532        29
           0     0.0000    0.0000    0.0000         4
           4     0.6667    0.6667    0.6667         3

   micro_avg     0.7890    0.7382    0.7627       466
   macro_avg     0.6275    0.5235    0.5627       466
weighted_avg     0.7785    0.7382    0.7521       466
</code></pre> 
<h4><a id="5_batch32_lossDICE_LOSS_lr1e5_epoch21_F1___267"></a>5. batch=32, loss=DICE_LOSS, lr=1e-5, epoch=21, 【F1指标比较高, 少样本数据学不到, 不稳定】</h4> 
<pre><code>              precision    recall  f1-score   support

           3     0.7714    0.8552    0.8112       221
           2     0.7727    0.9015    0.8322       132
           1     0.7347    0.5373    0.6207        67
           6     0.0000    0.0000    0.0000        10
           5     0.7222    0.4483    0.5532        29
           0     0.0000    0.0000    0.0000         4
           4     0.0000    0.0000    0.0000         3

   micro_avg     0.7612    0.7661    0.7636       466
   macro_avg     0.4287    0.3918    0.4025       466
weighted_avg     0.7353    0.7661    0.7441       466
</code></pre> 
<h4><a id="6_batch32_lossBCE_lr1e5_epoch21_________284"></a>6. batch=32, loss=BCE, lr=1e-5, epoch=21, 【普通的居然意外的好呢】</h4> 
<pre><code>              precision    recall  f1-score   support

           3     0.8136    0.8100    0.8118       221
           2     0.8029    0.8333    0.8178       132
           1     0.8235    0.4179    0.5545        67
           6     0.6667    0.4000    0.5000        10
           5     0.7778    0.4828    0.5957        29
           0     0.0000    0.0000    0.0000         4
           4     1.0000    0.6667    0.8000         3

   micro_avg     0.8062    0.7232    0.7624       466
   macro_avg     0.6978    0.5158    0.5828       466
weighted_avg     0.8009    0.7232    0.7493       466
</code></pre> 
<h4><a id="7_batch32_lossBCE_LOGITS_lr1e5_epoch21_torchnnBCEWithLogitsLoss_301"></a>7. batch=32, loss=BCE_LOGITS, lr=1e-5, epoch=21, 【torch.nn.BCEWithLogitsLoss】</h4> 
<pre><code>
              precision    recall  f1-score   support

           3     0.7973    0.8009    0.7991       221
           2     0.8000    0.7879    0.7939       132
           1     0.7317    0.4478    0.5556        67
           6     0.6667    0.4000    0.5000        10
           5     0.7368    0.4828    0.5833        29
           0     0.0000    0.0000    0.0000         4
           4     0.5000    0.6667    0.5714         3

   micro_avg     0.7825    0.7103    0.7447       466
   macro_avg     0.6046    0.5123    0.5433       466
weighted_avg     0.7733    0.7103    0.7344       466
</code></pre> 
<h4><a id="8_batch32_lossLABEL_SMOOTH_lr1e5_epoch21_BCELabelsmooth_319"></a>8. batch=32, loss=LABEL_SMOOTH, lr=1e-5, epoch=21, 【BCE-Label-smooth】</h4> 
<pre><code>              precision    recall  f1-score   support

           3     0.7945    0.7873    0.7909       221
           2     0.8120    0.8182    0.8151       132
           1     0.7027    0.3881    0.5000        67
           6     0.8000    0.4000    0.5333        10
           5     0.7647    0.4483    0.5652        29
           0     0.0000    0.0000    0.0000         4
           4     1.0000    0.6667    0.8000         3

   micro_avg     0.7899    0.7017    0.7432       466
   macro_avg     0.6963    0.5012    0.5721       466
weighted_avg     0.7790    0.7017    0.7296       466
</code></pre> 
<h4><a id="9_batch32_lossFOCAL_LOSS__PRIORMARGIN_LOSS_lr1e5_epoch21_Lossmacroavg_336"></a>9. batch=32, loss=FOCAL_LOSS + PRIOR-MARGIN_LOSS, lr=1e-5, epoch=21, 【这两个Loss混合，宏平均(macro-avg)效果居然意外的好呢！】</h4> 
<pre><code>           【1/2】
              precision    recall  f1-score   support

           3     0.7640    0.8643    0.8110       221
           2     0.7205    0.8788    0.7918       132
           1     0.6620    0.7015    0.6812        67
           6     0.4167    0.5000    0.4545        10
           5     0.7600    0.6552    0.7037        29
           0     0.4000    0.5000    0.4444         4
           4     0.5000    0.6667    0.5714         3

   micro_avg     0.7235    0.8197    0.7686       466
   macro_avg     0.6033    0.6809    0.6369       466
weighted_avg     0.7245    0.8197    0.7679       466
           
           【调和平均数】
              precision    recall  f1-score   support

           3     0.8474    0.7285    0.7835       221
           2     0.8304    0.7045    0.7623       132
           1     0.8182    0.4030    0.5400        67
           6     0.8000    0.4000    0.5333        10
           5     0.7143    0.3448    0.4651        29
           0     1.0000    0.2500    0.4000         4
           4     0.6667    0.6667    0.6667         3

   micro_avg     0.8324    0.6395    0.7233       466
   macro_avg     0.8110    0.4996    0.5930       466
weighted_avg     0.8292    0.6395    0.7132       466

           【1/3 + 2/3-focal】
              precision    recall  f1-score   support

           3     0.7890    0.8462    0.8166       221
           2     0.7516    0.8939    0.8166       132
           1     0.6935    0.6418    0.6667        67
           6     0.3636    0.4000    0.3810        10
           5     0.6538    0.5862    0.6182        29
           0     0.4000    0.5000    0.4444         4
           4     0.5000    0.6667    0.5714         3

   micro_avg     0.7430    0.8004    0.7707       466
   macro_avg     0.5931    0.6478    0.6164       466
weighted_avg     0.7420    0.8004    0.7686       466

           【1/4-prior + 3/4-focal】
              precision    recall  f1-score   support

           3     0.7956    0.8100    0.8027       221
           2     0.7712    0.8939    0.8281       132
           1     0.6981    0.5522    0.6167        67
           6     0.6667    0.4000    0.5000        10
           5     0.7143    0.5172    0.6000        29
           0     0.3333    0.2500    0.2857         4
           4     0.5000    0.6667    0.5714         3

   micro_avg     0.7656    0.7639    0.7648       466
   macro_avg     0.6399    0.5843    0.6007       466
weighted_avg     0.7610    0.7639    0.7581       466

           【4/9-prior + 5/9-focal】
              precision    recall  f1-score   support

           3     0.7819    0.8597    0.8190       221
           2     0.7578    0.9242    0.8328       132
           1     0.6567    0.6567    0.6567        67
           6     0.5000    0.5000    0.5000        10
           5     0.6250    0.5172    0.5660        29
           0     0.2857    0.5000    0.3636         4
           4     0.5000    0.6667    0.5714         3

   micro_avg     0.7364    0.8155    0.7739       466
   macro_avg     0.5867    0.6607    0.6156       466
weighted_avg     0.7352    0.8155    0.7715       466
</code></pre> 
<p>希望对你有所帮助!</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/aa6faf3cc1686866bb94bff5034b08d3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Python】要过滤掉两个list，两个list相减：Error: TypeError: unsupported operand type(s) for -: ‘list‘ and ‘list‘</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8fa628542cb80df3cc213d1c02349073/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MyBatis-Plus解决主键自增</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>