<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>联邦学习FedAvg-基于去中心化数据的深度网络高效通信学习 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="联邦学习FedAvg-基于去中心化数据的深度网络高效通信学习" />
<meta property="og:description" content="随着计算机算力的提升，机器学习作为海量数据的分析处理技术，已经广泛服务于人类社会。 然而，机器学习技术的发展过程中面临两大挑战：一是数据安全难以得到保障，隐私泄露问题亟待解决；二是网络安全隔离和行业隐私，不同行业部门之间存在数据壁垒，导致数据形成“孤岛”无法安全共享，而仅凭各部门独立数据训练的机器学习模型性能无法达到全局最优化。为解决上述问题，谷歌提出了联邦学习（FL，federated learning）技术。
本文主要对联邦学习的开山之作《Communication-Efficient Learning of Deep Networks from Decentralized Data》 进行重点内容的解读与整理总结。
论文链接：Communication-Efficient Learning of Deep Networks from Decentralized Data
源码实现：https://gitcode.net/mirrors/WHDY/fedavg?utm_source=csdn_github_accelerator 目录
摘要
1. 介绍
1.1 问题来源
1.2 本文贡献
1.3 联邦学习特性
1.4 联邦优化
1.5 相关工作
1.6 联邦学习框架图
2. 算法介绍
2.1 联邦随机梯度下降（FedSGD）
2.2 联邦平均算法（FedAvg）
3. 实验设计与实现
3.1 模型初始化
3.2 数据集的设置
3.2.1 MNIST数据集
3.2.2 莎士比亚作品集
3.3 实验优化
3.3.1 增加并行性
3.3.2 增加客户端计算量
3.4 探究客户端数据集的过度优化
3.5 CIFAR实验
3.6 大规模LSTM实验
4. 总结展望
摘要 现代移动设备拥有大量的适合模型学习的数据，基于这些数据训练得到的模型可以极大地提升用户体验。例如，语言模型能提升语音设别的准确率和文本输入的效率，图像模型能自动筛选好的照片。然而，移动设备拥有的丰富的数据经常具有关于用户的敏感的隐私信息且多个移动设备所存储的数据总量很大，这样一来，不适合将各个移动设备的数据上传到数据中心，然后使用传统的方法进行模型训练。作者提出了一个替代方法，这种方法可以基于分布在各个设备上的数据（无需上传到数据中心），然后通过局部计算的更新值进行聚合来学习到一个共享模型。作者定义这种非中心化方法为“联邦学习”。作者针对深度网络的联邦学习任务提出了一种实用方法，这种方法在学习过程中多次对模型进行平均。同时，作者使用了五种不同的模型和四个数据集对这种方法进行了实验验证。实验结果表明，这种方法面对不平衡以及非独立同分布的数据，具有较好的鲁棒性。在这种方法中，通信所产生的资源开销是主要的瓶颈，实验结果表明，与同步随机梯度下降相比，该方法的通信轮次减少了10-100倍。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/fb347319793c2bae280a6a203301e60f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-30T16:54:23+08:00" />
<meta property="article:modified_time" content="2023-08-30T16:54:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">联邦学习FedAvg-基于去中心化数据的深度网络高效通信学习</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>        <span style="color:#0d0016;">随着计算机算力的提升，机器学习作为海量数据的分析处理技术，已经广泛服务于人类社会。 然而，机器学习技术的发展过程中面临两大挑战：一是数据安全难以得到保障，隐私泄露问题亟待解决；二是网络安全隔离和行业隐私，不同行业部门之间存在数据壁垒，导致数据形成“孤岛”无法安全共享，而仅凭各部门独立数据训练的机器学习模型性能无法达到全局最优化。为解决上述问题，谷歌提出了联邦学习（FL，federated learning）技术。</span></p> 
<p><span style="color:#0d0016;">        本文主要对联邦学习的开山之作《Communication-Efficient Learning of Deep Networks from Decentralized Data》 进行重点内容的解读与整理总结。</span></p> 
<p>论文链接：<a class="link-info" href="http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf" rel="nofollow" title="Communication-Efficient Learning of Deep Networks from Decentralized Data">Communication-Efficient Learning of Deep Networks from Decentralized Data</a></p> 
<p>源码实现：<a class="link-info" href="https://gitcode.net/mirrors/WHDY/fedavg?utm_source=csdn_github_accelerator" rel="nofollow" title="https://gitcode.net/mirrors/WHDY/fedavg?utm_source=csdn_github_accelerator">https://gitcode.net/mirrors/WHDY/fedavg?utm_source=csdn_github_accelerator</a> </p> 
<p></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E6%91%98%E8%A6%81-toc" style="margin-left:0px;"><a href="#%E6%91%98%E8%A6%81" rel="nofollow">摘要</a></p> 
<p id="1.%20%E4%BB%8B%E7%BB%8D-toc" style="margin-left:0px;"><a href="#1.%20%E4%BB%8B%E7%BB%8D" rel="nofollow">1. 介绍</a></p> 
<p id="1.1%20%E9%97%AE%E9%A2%98%E6%9D%A5%E6%BA%90-toc" style="margin-left:40px;"><a href="#1.1%20%E9%97%AE%E9%A2%98%E6%9D%A5%E6%BA%90" rel="nofollow">1.1 问题来源</a></p> 
<p id="1.2%20%E6%9C%AC%E6%96%87%E8%B4%A1%E7%8C%AE-toc" style="margin-left:40px;"><a href="#1.2%20%E6%9C%AC%E6%96%87%E8%B4%A1%E7%8C%AE" rel="nofollow">1.2 本文贡献</a></p> 
<p id="1.3%20%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%89%B9%E6%80%A7-toc" style="margin-left:40px;"><a href="#1.3%20%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%89%B9%E6%80%A7" rel="nofollow">1.3 联邦学习特性</a></p> 
<p id="1.4%20%E8%81%94%E9%82%A6%E4%BC%98%E5%8C%96-toc" style="margin-left:40px;"><a href="#1.4%20%E8%81%94%E9%82%A6%E4%BC%98%E5%8C%96" rel="nofollow">1.4 联邦优化</a></p> 
<p id="1.5%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C-toc" style="margin-left:40px;"><a href="#1.5%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C" rel="nofollow">1.5 相关工作</a></p> 
<p id="1.6%C2%A0%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E5%9B%BE-toc" style="margin-left:40px;"><a href="#1.6%C2%A0%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E5%9B%BE" rel="nofollow">1.6 联邦学习框架图</a></p> 
<p id="2.%20%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D-toc" style="margin-left:0px;"><a href="#2.%20%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D" rel="nofollow">2. 算法介绍</a></p> 
<p id="2.1%C2%A0%E8%81%94%E9%82%A6%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88FedSGD%EF%BC%89-toc" style="margin-left:40px;"><a href="#2.1%C2%A0%E8%81%94%E9%82%A6%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88FedSGD%EF%BC%89" rel="nofollow">2.1 联邦随机梯度下降（FedSGD）</a></p> 
<p id="2.2%C2%A0%E8%81%94%E9%82%A6%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95%EF%BC%88FedAvg%EF%BC%89-toc" style="margin-left:40px;"><a href="#2.2%C2%A0%E8%81%94%E9%82%A6%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95%EF%BC%88FedAvg%EF%BC%89" rel="nofollow">2.2 联邦平均算法（FedAvg）</a></p> 
<p id="3.%20%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0-toc" style="margin-left:0px;"><a href="#3.%20%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0" rel="nofollow">3. 实验设计与实现</a></p> 
<p id="3.1%20%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96-toc" style="margin-left:40px;"><a href="#3.1%20%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96" rel="nofollow">3.1 模型初始化</a></p> 
<p id="3.2%20%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%AE%BE%E7%BD%AE-toc" style="margin-left:40px;"><a href="#3.2%20%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%AE%BE%E7%BD%AE" rel="nofollow">3.2 数据集的设置</a></p> 
<p id="3.2.1%C2%A0MNIST%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:80px;"><a href="#3.2.1%C2%A0MNIST%E6%95%B0%E6%8D%AE%E9%9B%86" rel="nofollow">3.2.1 MNIST数据集</a></p> 
<p id="3.2.2%C2%A0%E8%8E%8E%E5%A3%AB%E6%AF%94%E4%BA%9A%E4%BD%9C%E5%93%81%E9%9B%86-toc" style="margin-left:80px;"><a href="#3.2.2%C2%A0%E8%8E%8E%E5%A3%AB%E6%AF%94%E4%BA%9A%E4%BD%9C%E5%93%81%E9%9B%86" rel="nofollow">3.2.2 莎士比亚作品集</a></p> 
<p id="3.3%20%E5%AE%9E%E9%AA%8C%E4%BC%98%E5%8C%96-toc" style="margin-left:40px;"><a href="#3.3%20%E5%AE%9E%E9%AA%8C%E4%BC%98%E5%8C%96" rel="nofollow">3.3 实验优化</a></p> 
<p id="3.3.1%20%E5%A2%9E%E5%8A%A0%E5%B9%B6%E8%A1%8C%E6%80%A7-toc" style="margin-left:80px;"><a href="#3.3.1%20%E5%A2%9E%E5%8A%A0%E5%B9%B6%E8%A1%8C%E6%80%A7" rel="nofollow">3.3.1 增加并行性</a></p> 
<p id="3.3.2%C2%A0%E5%A2%9E%E5%8A%A0%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%AE%A1%E7%AE%97%E9%87%8F-toc" style="margin-left:80px;"><a href="#3.3.2%C2%A0%E5%A2%9E%E5%8A%A0%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%AE%A1%E7%AE%97%E9%87%8F" rel="nofollow">3.3.2 增加客户端计算量</a></p> 
<p id="%C2%A03.4%C2%A0%E6%8E%A2%E7%A9%B6%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%BF%87%E5%BA%A6%E4%BC%98%E5%8C%96-toc" style="margin-left:40px;"><a href="#%C2%A03.4%C2%A0%E6%8E%A2%E7%A9%B6%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%BF%87%E5%BA%A6%E4%BC%98%E5%8C%96" rel="nofollow"> 3.4 探究客户端数据集的过度优化</a></p> 
<p id="3.5%C2%A0CIFAR%E5%AE%9E%E9%AA%8C-toc" style="margin-left:40px;"><a href="#3.5%C2%A0CIFAR%E5%AE%9E%E9%AA%8C" rel="nofollow">3.5 CIFAR实验</a></p> 
<p id="3.6%C2%A0%E5%A4%A7%E8%A7%84%E6%A8%A1LSTM%E5%AE%9E%E9%AA%8C-toc" style="margin-left:40px;"><a href="#3.6%C2%A0%E5%A4%A7%E8%A7%84%E6%A8%A1LSTM%E5%AE%9E%E9%AA%8C" rel="nofollow">3.6 大规模LSTM实验</a></p> 
<p id="4.%20%E6%80%BB%E7%BB%93%E5%B1%95%E6%9C%9B-toc" style="margin-left:0px;"><a href="#4.%20%E6%80%BB%E7%BB%93%E5%B1%95%E6%9C%9B" rel="nofollow">4. 总结展望</a></p> 
<p style="margin-left:0px;"></p> 
<h2 id="%E6%91%98%E8%A6%81"> <span style="color:#0d0016;">摘要</span></h2> 
<p><span style="color:#0d0016;">现代移动设备拥有大量的适合模型学习的数据，基于这些数据训练得到的模型可以极大地提升用户体验。例如，语言模型能提升语音设别的准确率和文本输入的效率，图像模型能自动筛选好的照片。然而，移动设备拥有的丰富的数据经常具有关于用户的敏感的隐私信息且多个移动设备所存储的数据总量很大，这样一来，不适合将各个移动设备的数据上传到数据中心，然后使用传统的方法进行模型训练。作者提出了一个替代方法，这种方法可以基于分布在各个设备上的数据（无需上传到数据中心），然后通过局部计算的更新值进行聚合来学习到一个共享模型。作者定义这种非中心化方法为“联邦学习”。作者针对深度网络的联邦学习任务提出了一种实用方法，这种方法在学习过程中多次对模型进行平均。同时，作者使用了五种不同的模型和四个数据集对这种方法进行了实验验证。实验结果表明，这种方法面对不平衡以及非独立同分布的数据，具有较好的鲁棒性。在这种方法中，通信所产生的资源开销是主要的瓶颈，实验结果表明，与同步随机梯度下降相比，该方法的通信轮次减少了10-100倍。</span></p> 
<h2 id="1.%20%E4%BB%8B%E7%BB%8D"><span style="color:#0d0016;">1 介绍</span></h2> 
<h3 id="1.1%20%E9%97%AE%E9%A2%98%E6%9D%A5%E6%BA%90"><span style="color:#0d0016;">1.1 问题来源</span></h3> 
<p><span style="color:#0d0016;">        移动设备中有大量数据适合机器学习任务，利用这些数据反过来可以改善用户体验。例如图像识别模型可以帮助用户挑选好的照片。但是这些数据具有高度私密性，并且数据量大，所以我们不可能把这些数据拿到云端服务器进行集中训练。论文提出了一种分布式机器学习方法称为联邦学习（Federal Learning），在该框架中，服务器将全局模型下发给客户，客户端利用本地数据集进行训练，并将训练后的权重上传到服务器，从而实现全局模型的更新。</span></p> 
<h3 id="1.2%20%E6%9C%AC%E6%96%87%E8%B4%A1%E7%8C%AE"><span style="color:#0d0016;">1.2 本文贡献</span></h3> 
<ul><li><span style="color:#0d0016;">提出了从分散的存储于各个移动设备的数据中训练模型是一个重要的研究方向</span></li><li><span style="color:#0d0016;">提出了一个简单实用的算法来解决这种在非中心化设置下的学习问题</span></li><li><span style="color:#0d0016;">做了大量实验来评估所提算法</span></li></ul> 
<p><span style="color:#0d0016;">        具体来说，本文介绍了“联邦平均”算法，这种算法融合了客户端上的局部随机梯度下降计算与服务器上的模型平均。作者使用该算法进行了大量实验，结果表明了这种算法对于不平衡且非独立同分布的数据具有很好的鲁棒性，并且使得在非中心存储的数据上进行深度网络训练所需的通信轮次减少了几个数量级。</span></p> 
<h3 id="1.3%20%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%89%B9%E6%80%A7"><span style="color:#0d0016;">1.3 联邦学习特性</span></h3> 
<ul><li><span style="color:#0d0016;">从多个移动设备中存储的真实数据中进行模型训练比从存储在数据中心的数据中进行模型训练更具优势</span></li><li><span style="color:#0d0016;">由于数据具有隐私，且多个移动设备所存储的数据总量很大，因此不适合将其上传至数据中心再进行模型训练</span></li><li><span style="color:#0d0016;">对于监督学习任务，数据中的标签信息可以从用户与应用程序的交互中推断出来</span></li></ul> 
<h3 id="1.4%20%E8%81%94%E9%82%A6%E4%BC%98%E5%8C%96" style="background-color:transparent;"><span style="color:#0d0016;">1.4 联邦优化</span></h3> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#ffffff;">        传统分布式学习关注点在于如何将一个大型神经网络训练分布式进行，数据仍然可能是在几个大的训练中心存储。而联邦学习更关注数据本身，利用联邦学习保证了数据不出本地，并根据数据的特点，对学习模型进行改进。相比于典型的分布式优化问题，联邦优化具有几个关键特性：</span></span></p> 
<ul><li><span style="color:#0d0016;">Non-IID：数据的特征和分布在不同参与方间存在差异</span></li><li><span style="color:#0d0016;">Unbalanced：一些用户会更多地使用服务或应用程序，导致本地训练数据量存在差</span></li><li><span style="color:#0d0016;">Massively distributed：参与优化的用户数&gt;&gt;平均每个用户的数据量</span></li><li><span style="color:#0d0016;">Limited communication：无法保证客户端和服务器端的高效通信</span></li></ul> 
<p><span style="color:#0d0016;"> 本文重点关注优化任务中非独立同分布和不平衡问题，以及通信受限的临界属性。</span></p> 
<p><span style="color:#0d0016;">注：独立同分布假设（IID）</span></p> 
<p><span style="color:#0d0016;">        非凸神经网络的目标函数：</span></p> 
<p class="img-center"><img alt="" height="47" src="https://images2.imgbox.com/f2/23/LhzchcNi_o.png" width="286"></p> 
<p><span style="color:#0d0016;">对于一个机器学习的问题来说，有<img alt="" height="20" src="https://images2.imgbox.com/48/52/i7MHXOtE_o.png" width="105">，即用模型参数w预测实例<img alt="" height="28" src="https://images2.imgbox.com/eb/c9/bswgFC07_o.png" width="46">的损失。</span></p> 
<p><span style="color:#0d0016;">        设有K个client，第k个client的数据点为<img alt="P_{k}" class="mathcode" src="https://images2.imgbox.com/2e/ab/2JFye0hw_o.png">，对应的数据集数量为<img alt="n_{k}=\left | P_{k} \right |" class="mathcode" src="https://images2.imgbox.com/6d/b9/1ccXAWJj_o.png">上式可写为：</span></p> 
<p class="img-center"><img alt="" height="49" src="https://images2.imgbox.com/cb/4b/CPUckoor_o.png" width="343"></p> 
<p><span style="color:#0d0016;">若<img alt="P_{k}" class="mathcode" src="https://images2.imgbox.com/9c/0f/KKw5z7tJ_o.png">上的数据集是随机均匀采样的，称IID设置，此时有：</span></p> 
<p class="img-center"><img alt="" height="25" src="https://images2.imgbox.com/12/04/U8FtNWz5_o.png" width="156"></p> 
<p><span style="color:#0d0016;">不成立则称Non-IID。 </span></p> 
<h3 id="1.5%20%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span style="color:#0d0016;">1.5 相关工作</span></h3> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#ffffff;">        相关工作中，2010年通过迭代平均本地训练的模型来对感知机进行分布式训练，2015年研究了语音识别深度神经网络的分布式训练，在2015论文里研究了使用“软”平均的异步训练方法。这些工作都考虑的是数据中心化背景下的分布式训练，没有考虑具有数据不平衡且非独立同分布特点的联邦学习任务。但是它们提供了一种思路，即通过迭代平均本地训练模型的算法来解决联邦学习的问题。与本文的研究动机相似</span><span style="background-color:#ffffff;">，</span><span style="background-color:#ffffff;">在这篇论文中</span><span style="background-color:#ffffff;">讨论了保护设备中的用户数据的隐私的优点。</span><span style="background-color:#ffffff;">而在这篇论文中，作者</span><span style="background-color:#ffffff;">关注于训练深度网络，强调隐私的重要性以及通过在每一轮通信中仅共享一部分参数，进而降低通信开销；</span><span style="background-color:#ffffff;">但是</span><span style="background-color:#ffffff;">，他们也没有考虑数据的不平衡以及非独立同分布性，并且他们的研究工作缺乏实验评估。</span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="222" src="https://images2.imgbox.com/7f/78/OIuhD14E_o.png" width="702"></p> 
<h3 id="1.6%C2%A0%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E5%9B%BE"><span style="color:#0d0016;">1.6 联邦学习框架图</span></h3> 
<p class="img-center"><img alt="" height="301" src="https://images2.imgbox.com/51/8f/9kVxbPjO_o.png" width="487"></p> 
<h2 id="2.%20%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D" style="background-color:transparent;"><span style="color:#0d0016;">2 算法介绍</span></h2> 
<h3 id="2.1%C2%A0%E8%81%94%E9%82%A6%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88FedSGD%EF%BC%89"><span style="color:#0d0016;">2.1 联邦随机梯度下降（FedSGD）</span></h3> 
<div> 
 <span style="color:#0d0016;">设置固定的学习率η，对K个客户端的数据计算其损失梯度：</span> 
</div> 
<div> 
 <p class="img-center"><img alt="" height="25" src="https://images2.imgbox.com/93/81/w85reQs2_o.png" width="112"></p> 
 <p><span style="color:#0d0016;">中心服务器聚合每个客户端计算的梯度，以此来更新模型参数：</span></p> 
 <p class="img-center"><img alt="" height="50" src="https://images2.imgbox.com/61/5d/OnDu64uy_o.png" width="323"></p> 
</div> 
<div> 
 <span style="color:#0d0016;">其中，</span> 
</div> 
<div> 
 <p class="img-center"><img alt="" height="44" src="https://images2.imgbox.com/33/e6/nSrKYPrz_o.png" width="135"></p> 
 <h3 id="2.2%C2%A0%E8%81%94%E9%82%A6%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95%EF%BC%88FedAvg%EF%BC%89"><span style="color:#0d0016;">2.2 联邦平均算法（FedAvg）</span></h3> 
 <p><span style="color:#0d0016;">在客户端进行局部模型的更新：</span></p> 
 <p class="img-center"><img alt="" height="33" src="https://images2.imgbox.com/db/6f/GdhXUw9H_o.png" width="141"></p> 
</div> 
<div> 
 <span style="color:#0d0016;">中心服务器对每个客户端更新后的参数进行加权平均：</span> 
</div> 
<div> 
 <p class="img-center"><img alt="" height="53" src="https://images2.imgbox.com/ab/7b/pcBWDCLQ_o.png" width="158"></p> 
 <p><span style="color:#0d0016;">每个客户端可以独立地更新模型参数多次，然后再将更新好的参数发送给中心服务器进行加权平均：</span></p> 
 <p class="img-center"><img alt="" height="34" src="https://images2.imgbox.com/23/15/CcaCGXFa_o.png" width="192"></p> 
 <p><span style="color:#0d0016;">FedAvg的计算量与三个参数有关：</span></p> 
 <ul><li><span style="color:#0d0016;">C：每轮训练选择客户端的比例</span></li><li><span style="color:#0d0016;">E：每个客户端更新参数的循环次数所设计的一个因子</span></li><li><span style="color:#0d0016;">B：客户端更新参数时，每次梯度下降所使用的数据量</span></li></ul> 
 <p><span style="color:#0d0016;">对于一个拥有<img alt="n_{k}" class="mathcode" src="https://images2.imgbox.com/19/39/6btnFjg6_o.png">个数据样本的客户端，每轮本地参数更新的次数为：</span></p> 
 <p class="img-center"><img alt="" height="41" src="https://images2.imgbox.com/f2/71/JYs0VzXG_o.png" width="77"></p> 
</div> 
<div> 
 <span style="color:#0d0016;">注：FedSGD只是FedAvg的一个特例，即当参数E=1，B=∞时，FedAvg等价于FedSGD。</span> 
 <br>   
</div> 
<div> 
 <span style="color:#0d0016;">FedSGD和FedAvg的关系示意图：</span> 
</div> 
<div> 
 <span style="color:#0d0016;"><img alt="" height="1084" src="https://images2.imgbox.com/8b/c1/ZxJf3VwW_o.png" width="1200"></span> 
</div> 
<div> 
 <span style="color:#0d0016;">地址：<a class="link-info" href="https://blog.csdn.net/biongbiongdou/article/details/104358321" title="https://blog.csdn.net/biongbiongdou/article/details/104358321">https://blog.csdn.net/biongbiongdou/article/details/104358321</a></span> 
</div> 
<h2 id="3.%20%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0" style="background-color:transparent;"><span style="color:#0d0016;">3 实验设计与实现</span></h2> 
<h3 id="3.1%20%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span style="color:#0d0016;">3.1 模型初始化</span></h3> 
<div> 
 <span style="color:#0d0016;">实验设置</span> 
</div> 
<ul><li><span style="color:#0d0016;">数据集：MNIST中600个无重复的独立同分布样本</span></li><li><span style="color:#0d0016;">E=20; C=1; B=50; 中心服务器聚合一次</span></li><li><span style="color:#0d0016;">不同模型使用不同/相同的初始化模型，并通过θ对两模型参数进行加权求和<img alt="" height="19" src="https://images2.imgbox.com/7c/59/wyCuiDKr_o.png" width="75"></span></li></ul> 
<div> 
 <span style="color:#0d0016;"><img alt="" height="223" src="https://images2.imgbox.com/06/3d/Saf715vh_o.png" width="239">       <img alt="" height="227" src="https://images2.imgbox.com/a5/42/ZOCuYoUw_o.png" width="230"></span> 
</div> 
<div> 
 <p style="margin-left:.0001pt;text-align:left;"></p> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;"><span style="background-color:#ffffff;">研究模型平均对模型效果的影响：</span></span></p> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;"><span style="background-color:#ffffff;">        这里有两种情况，一种是不同模型使用不同的初始化模型；一种是不同模型使用相同的初始化模型。并且可以通过参数</span><em><em>控制权重比进行模型的加权求和。</em></em></span></p> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;"><em><em>        </em></em><span style="background-color:#ffffff;">可看到，采用不同的初始化参数进行模型平均后，平均模型的效果变差，模型性能比两个父模型都差；</span><span style="background-color:#ffffff;">采用相同的初始化参数进行模型平均后，对模型的平均可以显著的减少整个训练集的损失，模型性能优于两个父模型。</span></span></p> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;"><span style="background-color:#ffffff;">        该结论是用于实现联邦学习的重要支撑，在每一轮训练时，server发布全局模型，使各个client采用相同的参数模型进行训练，可以有效的减少训练集的损失。</span></span></p> 
 <h3 id="3.2%20%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%AE%BE%E7%BD%AE" style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">3.2 数据集的设置</span></h3> 
 <p><span style="color:#0d0016;">        初步研究包括两个数据集三个模型族，前两个模型用于识别MNIST数据集,后一个用于实现莎士比亚作品集单词预测。</span></p> 
 <h4 id="3.2.1%C2%A0MNIST%E6%95%B0%E6%8D%AE%E9%9B%86" style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">3.2.1 MNIST数据集</span></h4> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">2NN：拥有两个隐藏层，每层200个神经元的多层感知机模型，ReLu激活；</span></p> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">CNN：两个卷积核大小为5X5的卷积层（分别是32通道和64通道，每层后都有一个2X2的最大池化层）；</span></p> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">IDD：数据随机打乱分给100个客户端，每个客户端600个样例； </span></p> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">Non-IDD：按数字标签将数据集划分为200个大小为300的碎片，每个客户端两个碎片；</span></p> 
 <ul><li style="margin-left:.0001pt;text-align:left;"> <h4 id="3.2.2%C2%A0%E8%8E%8E%E5%A3%AB%E6%AF%94%E4%BA%9A%E4%BD%9C%E5%93%81%E9%9B%86"><span style="color:#0d0016;">3.2.2 莎士比亚作品集</span></h4> </li></ul> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">LSTM：将输入字符嵌入到一个已学习的8维空间中，然后通过两个LSTM层处理嵌入的字符，每层256个节点，最后，第二个LSTM层的输出被发送到每一个字符有一个节点的softmax输出层，使用unroll的80个字符长度进行训练； </span></p> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">Unbalanced-Non-IID：每个角色形成一个客户端，共1146个客户端； </span></p> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">Balanced-IID：直接将数据集划分给1146个客户端；</span></p> 
 <h3 id="3.3%20%E5%AE%9E%E9%AA%8C%E4%BC%98%E5%8C%96" style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;">3.3 实验优化</span></h3> 
 <p style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;"><span style="background-color:#ffffff;">        在数据中心存储的优化中，通信开销相对较小，计算开销占主导地位。</span><span style="background-color:#ffffff;">而在联邦优化中，</span><span style="background-color:#ffffff;">任何一个单一设备所具有的数据量较少，且现代</span><span style="background-color:#ffffff;">移动设备</span><span style="background-color:#ffffff;">有相对快的处理器</span><span style="background-color:#ffffff;">，</span><span style="background-color:#ffffff;">所以这里更关注</span><span style="background-color:#ffffff;">通信开销</span><span style="background-color:#ffffff;">。</span><span style="background-color:#ffffff;">因此，我们想要</span><span style="background-color:#ffffff;">使用额外的计算来减少训练模型所需通信的轮次</span><span style="background-color:#ffffff;">。</span><span style="background-color:#ffffff;">主要有两个方法，分别是提高并行度以及增加每个客户端的计算量。</span></span></p> 
 <h4 id="3.3.1%20%E5%A2%9E%E5%8A%A0%E5%B9%B6%E8%A1%8C%E6%80%A7" style="margin-left:.0001pt;text-align:left;"><span style="color:#0d0016;"><span style="background-color:#ffffff;">3.3.1 增加并行性</span></span></h4> 
 <p><span style="color:#0d0016;">固定参数E，对C和B进行讨论。</span></p> 
 <p class="img-center"><img alt="" height="287" src="https://images2.imgbox.com/68/23/4flSs8AH_o.png" width="544"></p> 
</div> 
<ul><li><span style="color:#0d0016;"> 当B=∞时，增加客户端比例，效果提升的优势较小；</span></li><li><span style="color:#0d0016;">当B=10时，有显著改善，特别是在Non-IID情况下；</span></li><li><span style="color:#0d0016;">在B=10，当C≥0.1时，收敛速度有明显改进，当用户达到一定数量时，收敛增加的速度不再明显。</span></li></ul> 
<h4 id="3.3.2%C2%A0%E5%A2%9E%E5%8A%A0%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%AE%A1%E7%AE%97%E9%87%8F"><span style="color:#0d0016;">3.3.2 增加客户端计算量</span></h4> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;"><span style="background-color:#ffffff;">对于增加每个客户端的计算量，可以通过减小B或者增加E来实现。</span></span></p> 
<p class="img-center"><img alt="" height="357" src="https://images2.imgbox.com/c2/a8/W5MOg1r1_o.png" width="489"></p> 
<ul><li><span style="color:#0d0016;">每轮增加更多的本地SGD更新可以显著降低通信成本；</span></li><li><span style="color:#0d0016;">对于Unbalanced-Non-IDD的莎士比亚数据减少通信轮数倍数更多，推测可能某些客户端有相对较大的本地数据集，使得增加本地训练更有价值；</span></li></ul> 
<p><span style="color:#0d0016;"> <span style="background-color:#ffffff;">将上述实验结果用折线图的形式展示，这里蓝色线表示的是联邦随机梯度下降的结果：</span></span></p> 
<p class="img-center"><img alt="" height="228" src="https://images2.imgbox.com/41/61/5cJsWxYJ_o.png" width="429"></p> 
<p class="img-center"><img alt="" height="238" src="https://images2.imgbox.com/79/60/O40HlQyG_o.png" width="420"></p> 
<ul><li><span style="color:#0d0016;">FedAvg相比FedSGD不仅降低通信轮数，还具有更高的测试精度。推测是平均模型产生了类似Dropout的正则化效益； </span></li></ul> 
<h3 id="%C2%A03.4%C2%A0%E6%8E%A2%E7%A9%B6%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%BF%87%E5%BA%A6%E4%BC%98%E5%8C%96"><span style="color:#0d0016;"> 3.4 <span style="background-color:#ffffff;">探究客户端数据集的过度优化</span></span></h3> 
<p class="img-center"><img alt="" height="273" src="https://images2.imgbox.com/6c/10/eo2F9X2B_o.png" width="462"></p> 
<p><span style="color:#0d0016;"><span style="background-color:#ffffff;">        在E=5以及E=25的设置下，</span>对于大的本地更新次数而言，联邦平均的训练损失会停滞或发散；因此在实际应用时，对于一些模型，在训练后期减少本地训练周期将有助于收敛。 </span></p> 
<h3 id="3.5%C2%A0CIFAR%E5%AE%9E%E9%AA%8C"><span style="color:#0d0016;">3.5 CIFAR实验</span></h3> 
<p><span style="color:#0d0016;">在CTFAR数据集上进行实验，模型是TensorFlow教程中的模型包括两个卷积层，两个全连接层和一个线性传输层，大约10^6个参数。下表给出了baselineSGD、FedSGD和FedAvg达到三种不同精度目标的通信轮数。</span></p> 
<p class="img-center"><img alt="" height="88" src="https://images2.imgbox.com/7c/fb/frOxCkk1_o.png" width="446"></p> 
<p><span style="color:#0d0016;">不同学习率下FedSGD和FedAvg的曲线：</span></p> 
<p class="img-center"><img alt="" height="237" src="https://images2.imgbox.com/8d/08/pQOVdqGr_o.png" width="408"></p> 
<h3 id="3.6%C2%A0%E5%A4%A7%E8%A7%84%E6%A8%A1LSTM%E5%AE%9E%E9%AA%8C"><span style="color:#0d0016;">3.6 大规模LSTM实验</span></h3> 
<p><span style="color:#0d0016;"> 为了证明我们的方法对于解决实际问题的有效性，我们进行了一项大规模单词预测任务。</span></p> 
<p><span style="color:#0d0016;">训练集包含来自大型社交网络的100万个公共帖子。我们根据作者对帖子进行分组，总共有超过50个客户端。我们将每个客户的数据集限制为最多5000个单词。模型是一个256节点的LSTM，其词汇量为10000个单词。每个单词的输入和输出嵌入为192维，并与模型共同训练；总共有4950544个参数，使用10个字符的unroll。</span></p> 
<p><span style="color:#0d0016;">对于联邦平均和联邦随机梯度下降的最佳学习率曲线：</span></p> 
<p class="img-center"><img alt="" height="282" src="https://images2.imgbox.com/63/c1/vzLLTy8p_o.png" width="350"></p> 
<ul><li><span style="color:#0d0016;">相同准确率的情况下，FedAvg的通信轮数更少；测试精度方差更小；</span></li><li><span style="color:#0d0016;">E=1比E=5的表现效果更好； </span></li></ul> 
<h2 id="4.%20%E6%80%BB%E7%BB%93%E5%B1%95%E6%9C%9B" style="background-color:transparent;"><span style="color:#0d0016;">4 总结展望</span></h2> 
<p><span style="color:#0d0016;">         我们的实验表明，联邦学习可以在实践中实现，因为它可以使用相对较少的几轮通信来训练高质量的模型，这一点在各种模型体系结构上得到了证明：一个多层感知器、两个不同的卷积NNs、一个两层LSTM和一个大规模LSTM。虽然联邦学习提供了许多实用的隐私保护，但是通过差分隐私、安全多方计算提供了可以提供更有力的保障，或者他们的组合是未来工作的一个有趣方向。请注意，这两类技术最自然地应用于像FedAvg这样的同步算法。</span></p> 
<p></p> 
<p><span style="color:#0d0016;">参考文章：</span></p> 
<p><a class="link-info" href="https://blog.csdn.net/qq_41605740/article/details/124584939?spm=1001.2014.3001.5506" title="https://blog.csdn.net/qq_41605740/article/details/124584939?spm=1001.2014.3001.5506">https://blog.csdn.net/qq_41605740/article/details/124584939?spm=1001.2014.3001.5506</a></p> 
<p><a class="link-info" href="https://blog.csdn.net/weixin_45662974/article/details/119464191?spm=1001.2014.3001.5506" title="https://blog.csdn.net/weixin_45662974/article/details/119464191?spm=1001.2014.3001.5506">https://blog.csdn.net/weixin_45662974/article/details/119464191?spm=1001.2014.3001.5506</a> </p> 
<p><a class="link-info" href="https://zhuanlan.zhihu.com/p/515756280" rel="nofollow" title="https://zhuanlan.zhihu.com/p/515756280">https://zhuanlan.zhihu.com/p/515756280</a> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b8c40dfe727c99194c5b8eea909a0926/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">VMware虚拟机Linux系统与Windows系统共享文件夹出现“Transport endpoint is not connected“解决方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/32c4731b85b7048f52a6d971bd339b3a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">centos 7 YUM源配置</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>