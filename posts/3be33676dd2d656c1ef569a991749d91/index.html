<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文精读3】基于历史抽取信息的摘要抽取方法 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文精读3】基于历史抽取信息的摘要抽取方法" />
<meta property="og:description" content="前言 论文分享 今天分享的是来自2018ACL的长文本抽取式摘要方法论文，作者来自哈尔滨工业大学和微软，引用数369
Neural Document Summarization by Jointly Learning to Score and Select Sentences
摘要抽取通常分为两个部分，句子打分和句子选择。句子打分的方法有基于特征的，比如字概率，TF-IDF权重，句子位置和句子长度特征，基于图的方法比如TextRank和LexRank。在句子选择阶段，有基于最大边际相关性（maximal marginal relevance）的方法，选择分数最高，冗余最小的摘要；以摘要长度为限制，基于整数线性规划（Integer Linear Programming）的方法；用于发现最优句子子集的优化方法子模块方程（submodular function）；基于神经网络的方法等
这些方法都是将句子打分和句子选择分为两个步骤，即先打分后选择，论文提出一种将打分和选择进行联合学习的模型。作者的创新点在于构建的模型在每一次选择句子的时候，会同时考虑句子的重要性和之前已选择的句子。
模型 作者提出的模型一共包括3个GRU，第一个GRU用于编码字级别特征，将GRU的最后一层作为句子编码，第二个GRU对所有句子进行学习，得到文档级的句子编码，第三个GRU用于学习历史抽取信息，前两个GRU的公式比较简单，这里就不详细描述了，学习历史信息，得到隐层状态，然后利用历史信息的隐层状态，去影响剩余句子的打分过程，方式如下
s t − 1 s_{t-1} st−1​为上一时刻采集的句子向量， h t − 1 h_{t-1} ht−1​为上一时刻隐层状态，GRU得到当前时刻的隐层状态，然后通过当前隐层状态对剩余的句子进行打分，得到分数 δ ( S i ) \delta(S_i) δ(Si​)，需要注意的是，在抽取第一个句子的时候， s 0 s_0 s0​向量为0，隐层状态计算如下
用于计算隐层状态的 s 1 ← \mathop{s_1}\limits ^{\leftarrow} s1​←​为文档级向量的最后一个反向隐层向量
目标函数 目标函数为预测段落的得分和添加每一个句子可获得的ROUGE分数提升之间的KL-Divergence
预测段落的得分
添加每一个句子可获得的ROUGE分数提升，MinMax归一化后进行softmax
实验 作者比较了几个当时最好的抽取式模型，NEUSUM均优于baseline
作者比较了NN-SE和NeuSum抽取的句子和贪婪算法（ORACLE）得到的参考句子，在抽取的前3个句子的准确性，NeuSum是要优于NN-SE，并且发现越往后抽取，准确率是越下降的，如何高效的利用历史抽取是一个可提升的点
作者还比较了抽取句子位置的统计信息，得出NN-SE抽取的句子80.91%的概率来自前三个句子，而NeuSum只有58.64%，NeuSum利用历史抽取信息的方法能够避免依赖前面的段落。ORACLE中句子的分布是比较均匀的，而算法倾向于抽取前几个句子
总结 NeuSum提出了一种基于历史抽取信息的摘要抽取方法，其只能抽取固定数量的段落和目标函数在MemSum中得到了优化" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/3be33676dd2d656c1ef569a991749d91/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-02T09:13:33+08:00" />
<meta property="article:modified_time" content="2023-08-02T09:13:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文精读3】基于历史抽取信息的摘要抽取方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>前言</h3> 
<p><strong>论文分享</strong> 今天分享的是来自2018ACL的长文本抽取式摘要方法论文，作者来自哈尔滨工业大学和微软，引用数369<br> <a href="https://aclanthology.org/P18-1061/" rel="nofollow">Neural Document Summarization by Jointly Learning to Score and Select Sentences</a></p> 
<p>摘要抽取通常分为两个部分，句子打分和句子选择。句子打分的方法有基于特征的，比如字概率，TF-IDF权重，句子位置和句子长度特征，基于图的方法比如TextRank和LexRank。在句子选择阶段，有基于最大边际相关性（maximal marginal relevance）的方法，选择分数最高，冗余最小的摘要；以摘要长度为限制，基于整数线性规划（Integer Linear Programming）的方法；用于发现最优句子子集的优化方法子模块方程（submodular function）；基于神经网络的方法等</p> 
<p>这些方法都是将句子打分和句子选择分为两个步骤，即先打分后选择，论文提出一种将打分和选择进行联合学习的模型。作者的创新点在于构建的模型在每一次选择句子的时候，会同时考虑句子的重要性和之前已选择的句子。</p> 
<h3><a id="_7"></a>模型</h3> 
<p><img src="https://images2.imgbox.com/9a/9b/xkOZqTrn_o.png" alt="在这里插入图片描述"><br> 作者提出的模型一共包括3个GRU，第一个GRU用于编码字级别特征，将GRU的最后一层作为句子编码，第二个GRU对所有句子进行学习，得到文档级的句子编码，第三个GRU用于学习历史抽取信息，前两个GRU的公式比较简单，这里就不详细描述了，学习历史信息，得到隐层状态，然后利用历史信息的隐层状态，去影响剩余句子的打分过程，方式如下<br> <img src="https://images2.imgbox.com/7f/f1/gqQxvk3f_o.png" alt="在这里插入图片描述"><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          s 
         
         
         
           t 
          
         
           − 
          
         
           1 
          
         
        
       
      
        s_{t-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6389em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span>为上一时刻采集的句子向量，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          h 
         
         
         
           t 
          
         
           − 
          
         
           1 
          
         
        
       
      
        h_{t-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9028em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span>为上一时刻隐层状态，GRU得到当前时刻的隐层状态，然后通过当前隐层状态对剩余的句子进行打分，得到分数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         δ 
        
       
         ( 
        
        
        
          S 
         
        
          i 
         
        
       
         ) 
        
       
      
        \delta(S_i) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0379em;">δ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，需要注意的是，在抽取第一个句子的时候，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          s 
         
        
          0 
         
        
       
      
        s_0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>向量为0，隐层状态计算如下<br> <img src="https://images2.imgbox.com/fc/4e/Oj4d6bCA_o.png" alt="在这里插入图片描述"><br> 用于计算隐层状态的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
          
          
            s 
           
          
            1 
           
          
         
        
          ← 
         
        
       
      
        \mathop{s_1}\limits ^{\leftarrow} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1374em; vertical-align: -0.15em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9874em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class=""><span class="mop"><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.6306em; margin-left: 0em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">←</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span>为文档级向量的最后一个反向隐层向量<br> <strong>目标函数</strong> 目标函数为预测段落的得分和添加每一个句子可获得的ROUGE分数提升之间的KL-Divergence<br> <img src="https://images2.imgbox.com/d9/ac/1EkdouaX_o.png" alt="在这里插入图片描述"></p> 
<p>预测段落的得分<br> <img src="https://images2.imgbox.com/d6/f2/pj3rddnI_o.png" alt="在这里插入图片描述"><br> 添加每一个句子可获得的ROUGE分数提升，MinMax归一化后进行softmax<br> <img src="https://images2.imgbox.com/9b/71/niZHWMIG_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5d/b9/xUdVmAZ8_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_22"></a>实验</h3> 
<p><img src="https://images2.imgbox.com/a2/e6/1Coju77E_o.png" alt="在这里插入图片描述"><br> 作者比较了几个当时最好的抽取式模型，NEUSUM均优于baseline<br> <img src="https://images2.imgbox.com/5e/83/ruRJDYGg_o.png" alt="在这里插入图片描述"><br> 作者比较了NN-SE和NeuSum抽取的句子和贪婪算法（ORACLE）得到的参考句子，在抽取的前3个句子的准确性，NeuSum是要优于NN-SE，并且发现越往后抽取，准确率是越下降的，如何高效的利用历史抽取是一个可提升的点<br> <img src="https://images2.imgbox.com/18/a2/L1Psebaq_o.png" alt="在这里插入图片描述"><br> 作者还比较了抽取句子位置的统计信息，得出NN-SE抽取的句子80.91%的概率来自前三个句子，而NeuSum只有58.64%，NeuSum利用历史抽取信息的方法能够避免依赖前面的段落。ORACLE中句子的分布是比较均匀的，而算法倾向于抽取前几个句子</p> 
<h3><a id="_29"></a>总结</h3> 
<p>NeuSum提出了一种基于历史抽取信息的摘要抽取方法，其只能抽取固定数量的段落和目标函数在<a href="https://blog.csdn.net/qiongyaoxinpo/article/details/131974628?spm=1001.2014.3001.5501">MemSum</a>中得到了优化</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/87f0e747985fb4c8329a96301605df26/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">使用 Docker Compose 部署 Redis Cluster 集群，轻松搭建高可用分布式缓存</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a06687c572997ad0db79f245e3324e78/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mysql常用的查询语法~！(个人收藏)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>