<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【调参Tricks】WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【调参Tricks】WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach" />
<meta property="og:description" content="【调参Tricks】WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach
目录 总述模型实验及结论代码优化参考 总述 该文主要介绍了三种使用BERT做Sentence Embedding的小Trick，分别为：
应该使用所有token embedding的average作为句子表示，而非只使用[CLS]对应位置的表示。在BERT中应该使用多层的句向量叠加，而非只使用最后一层。在通过余弦相似度做句子相似度判定的时候，可以使用Whitening操作来统一sentence embedding的向量分布，从而可以获得更好的句子表示。 模型 文中介绍的前两点均不涉及到模型，只有第三点Whitening操作可以做简要介绍。
出发点： 以余弦相似度作为向量相似度衡量的指标的是建立在“标准正交基”的基础上的，基向量不同，向量中各个数值所代表的的意义也变不一样。然后经过BERT抽取之后的句向量所处的坐标系可能并非基于同一个“标准正交基”的坐标系。
解决方案： 将各个向量归一化到同一个标准正交基的坐标系中。一个猜测是，预训练语言模型生成的各个句向量应该在坐标系中的各个位置是相对均匀的，即表现出各项同性。基于这个猜测，我们可以将所有句向量做归一化，使之满足各向同性。一个可行的方案是将句向量的分布规约成正态分布，因为正态分布满足各项同性（数学定理）。
做法：
内容截图自苏神的博客： 链接
实验及结论 应该使用所有token embedding的average作为句子表示，而非只使用[CLS]对应位置的表示。
叠加BERT的1,2，12层这三层的向量效果表现最好。
Whiten操作对于多数预训练语言模型而言均有效果。
代码 def whitening_torch_final(embeddings): # For torch &lt; 1.10 mu = torch.mean(embeddings, dim=0, keepdim=True) cov = torch.mm((embeddings - mu).t(), (embeddings - mu)) # For torch &gt;= 1.10 cov = torch.cov(embedding) u, s, vt = torch.svd(cov) W = torch.mm(u, torch.diag(1/torch.sqrt(s))) embeddings = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/21f86d3c2a0bec890583af1e97114cdf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-20T15:23:47+08:00" />
<meta property="article:modified_time" content="2022-10-20T15:23:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【调参Tricks】WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><a href="https://blog.csdn.net/lwgkzl/article/details/124884678">【调参Tricks】WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach</a></p> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_3" rel="nofollow">总述</a></li><li><a href="#_9" rel="nofollow">模型</a></li><li><a href="#_20" rel="nofollow">实验及结论</a></li><li><a href="#_28" rel="nofollow">代码</a></li><li><a href="#_45" rel="nofollow">优化</a></li><li><a href="#_60" rel="nofollow">参考</a></li></ul> 
</div> 
<p></p> 
<h2><a id="_3"></a>总述</h2> 
<p>该文主要介绍了三种使用BERT做Sentence Embedding的小Trick，分别为：</p> 
<ol><li>应该使用所有token embedding的average作为句子表示，而非只使用[CLS]对应位置的表示。</li><li>在BERT中应该使用多层的句向量叠加，而非只使用最后一层。</li><li>在通过余弦相似度做句子相似度判定的时候，可以使用Whitening操作来统一sentence embedding的向量分布，从而可以获得更好的句子表示。</li></ol> 
<h2><a id="_9"></a>模型</h2> 
<p>文中介绍的前两点均不涉及到模型，只有第三点Whitening操作可以做简要介绍。</p> 
<p><strong>出发点：</strong> 以余弦相似度作为向量相似度衡量的指标的是建立在“标准正交基”的基础上的，基向量不同，向量中各个数值所代表的的意义也变不一样。然后经过BERT抽取之后的句向量所处的坐标系可能并非基于同一个“标准正交基”的坐标系。</p> 
<p><strong>解决方案：</strong> 将各个向量归一化到同一个标准正交基的坐标系中。一个猜测是，预训练语言模型生成的各个句向量应该在坐标系中的各个位置是相对均匀的，即表现出各项同性。基于这个猜测，我们可以将所有句向量做归一化，使之满足各向同性。一个可行的方案是将句向量的分布规约成正态分布，因为正态分布满足各项同性（数学定理）。</p> 
<p><strong>做法：</strong><br> <img src="https://images2.imgbox.com/05/e7/zJ7QXfzE_o.png" alt="在这里插入图片描述"><br> 内容截图自苏神的博客： <a href="https://spaces.ac.cn/archives/8069" rel="nofollow">链接</a></p> 
<h2><a id="_20"></a>实验及结论</h2> 
<ol><li> <p>应该使用所有token embedding的average作为句子表示，而非只使用[CLS]对应位置的表示。</p> </li><li> <p>叠加BERT的1,2，12层这三层的向量效果表现最好。<br> <img src="https://images2.imgbox.com/2b/af/BfB93lYs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1e/f4/bcX7vMPh_o.png" alt="在这里插入图片描述"></p> </li><li> <p>Whiten操作对于多数预训练语言模型而言均有效果。</p> </li></ol> 
<h2><a id="_28"></a>代码</h2> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">whitening_torch_final</span><span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># For torch &lt; 1.10</span>
    mu <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>embeddings<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    cov <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span><span class="token punctuation">(</span>embeddings <span class="token operator">-</span> mu<span class="token punctuation">)</span><span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>embeddings <span class="token operator">-</span> mu<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># For torch &gt;= 1.10</span>
    cov <span class="token operator">=</span> torch<span class="token punctuation">.</span>cov<span class="token punctuation">(</span>embedding<span class="token punctuation">)</span>
    
    u<span class="token punctuation">,</span> s<span class="token punctuation">,</span> vt <span class="token operator">=</span> torch<span class="token punctuation">.</span>svd<span class="token punctuation">(</span>cov<span class="token punctuation">)</span>
    W <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>u<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">/</span>torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>embeddings <span class="token operator">-</span> mu<span class="token punctuation">,</span> W<span class="token punctuation">)</span>
    <span class="token keyword">return</span> embeddings
</code></pre> 
<p>在经过bert encoder之后的向量，送入whitening_torch_final函数中即可完成whitening的操作。</p> 
<h2><a id="_45"></a>优化</h2> 
<p>根据苏神的博客，只保留SVD提取出来的前N个特征值可以提升进一步的效果。并且，由于只保留了前N个特征，故与PCA的原理类似，相当于对句向量做了一步降维的操作。<br> 代码修改为：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">whitening_torch_final</span><span class="token punctuation">(</span>embeddings<span class="token punctuation">,</span> keep_dim<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># For torch &gt;= 1.10</span>
    cov <span class="token operator">=</span> torch<span class="token punctuation">.</span>cov<span class="token punctuation">(</span>embedding<span class="token punctuation">)</span> <span class="token comment"># emb_dim * emb_dim</span>
    
    u<span class="token punctuation">,</span> s<span class="token punctuation">,</span> vt <span class="token operator">=</span> torch<span class="token punctuation">.</span>svd<span class="token punctuation">(</span>cov<span class="token punctuation">)</span>
    <span class="token comment"># u : emb_dim * emb_dim,  s: emb_dim</span>
    W <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>u<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">/</span>torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># W: emb_dim * emb_dim</span>
    embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>embeddings <span class="token operator">-</span> mu<span class="token punctuation">,</span> W<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span>keep_dim<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 截断</span>
    <span class="token keyword">return</span> embeddings <span class="token comment"># bs * keep_dim</span>
</code></pre> 
<h2><a id="_60"></a>参考</h2> 
<p><a href="https://blog.csdn.net/lwgkzl/article/details/124884678">【调参Tricks】WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9f0bd3c760bbabaa9c94701f306d4096/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JAVA的@EXCEL导出导入常用注解汇总</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ee1b7d0534209ebacd72e0961fbb36f4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">[061量化交易]python使用baostock下载全部行情数据</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>