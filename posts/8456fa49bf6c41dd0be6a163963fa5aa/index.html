<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Transformer——Attention 注意力机制 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Transformer——Attention 注意力机制" />
<meta property="og:description" content="注意力机制 Transformer的注意力机制借鉴了人类的注意力机制。人类通过眼睛的视觉单元去扫描图像，其中的重点区域会被大脑的神经元处理从而获得更多的信息，这是人类长期精华所获得的一种能力。
以论文中的例子来看，红色区域表示我们人脑视觉更为关注的区域。而Attention 机制则是模拟这一人脑机制，让计算机能够正确的从总舵信息中选择出对当前任务更为重要的信息。
Attention 机制原理 人类视觉注意力机制的原理为：从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略不重要的信息。
计算机的注意力机制模型就是从大量信息（Values）中筛选出少量的重要信息，这个重要信息对于另外一个信息（Query）是重要的。即注意力模型的主要作用就是通过Query从Values中筛选出重要信息。
结合下图，Attention可以翻译为如下的描述，其将Query和KV（把value拆分为key-value信息，这两个值看作等同的）映射到输出上。其中Q(query)、K(key)、V(value)都是一个向量，输出 V ， V^， V，则是所有value的加权，其中权重是由Q和每个K计算出来的。
KV 是怎么来的？ 对于上述图像来说，我们将其分割为不同的小块，其中每个块的向量化表示则是向量K
详细计算过程 刚刚提到，输出 V ， V^， V，是对所有value的加权，是由 Query 和每个 key 计算出来的，计算方法分为三步：
第一步：计算Q和K的相似度，可以用 F F F来表示： f ( Q , K i ) , i = 1 , 2 , . . . , n f(Q,K_i),i=1,2,...,n f(Q,Ki​),i=1,2,...,n 计算方法一般分为四种： 点乘： f ( Q , K i ) = Q T K i f(Q,K_i)=Q^TK_i f(Q,Ki​)=QTKi​权重： f ( Q , K i ) = Q T K i W f(Q,K_i)=Q^TK_iW f(Q,Ki​)=QTKi​W拼接权重： f ( Q , K i ) = [ Q T K i ] W f(Q,K_i)=[Q^TK_i]W f(Q,Ki​)=[QTKi​]W感知器： f ( Q , K i ) = V T t a n h ( W Q &#43; U K i ) f(Q,K_i)=V^Ttanh(WQ&#43;UK_i) f(Q,Ki​)=VTtanh(WQ&#43;UKi​) 第二布：将第一步得到的相似度进行SoftMax操作，进行归一化： α i = s o f t m a x ( F ( Q , K i ) d k ) \alpha_i=softmax(\frac{F(Q,K_i)}{\sqrt{d_k}}) αi​=softmax(dk​ ​F(Q,Ki​)​) 这一步进行归一化是避免得到的相似度F1 = 50 和 F2 = 1 之间的差值多大影响模型效果，归一化之后相似度可能就会变成F1 = 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/8456fa49bf6c41dd0be6a163963fa5aa/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-10T18:16:21+08:00" />
<meta property="article:modified_time" content="2024-03-10T18:16:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Transformer——Attention 注意力机制</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>注意力机制</h3> 
<p>Transformer的注意力机制借鉴了人类的注意力机制。人类通过眼睛的视觉单元去扫描图像，其中的重点区域会被大脑的神经元处理从而获得更多的信息，这是人类长期精华所获得的一种能力。</p> 
<p>以论文中的例子来看，红色区域表示我们人脑视觉更为关注的区域。而Attention 机制则是模拟这一人脑机制，<strong>让计算机能够正确的从总舵信息中选择出对当前任务更为重要的信息</strong>。<br> <img src="https://images2.imgbox.com/ee/91/KVplU5Ba_o.jpg" alt="请添加图片描述"></p> 
<h4><a id="Attention__5"></a>Attention 机制原理</h4> 
<p>人类视觉注意力机制的原理为：<strong>从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略不重要的信息</strong>。</p> 
<p>计算机的注意力机制模型就是从大量信息（Values）中筛选出少量的重要信息，这个重要信息对于另外一个信息（Query）是重要的。即<strong>注意力模型的主要作用就是通过Query从Values中筛选出重要信息</strong>。</p> 
<p>结合下图，Attention可以翻译为如下的描述，其将Query和KV（把value拆分为key-value信息，这两个值看作等同的）映射到输出上。其中Q(query)、K(key)、V(value)都是一个向量，输出<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          V 
         
        
          ， 
         
        
       
      
        V^， 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord cjk_fallback mtight">，</span></span></span></span></span></span></span></span></span></span></span></span>则是所有value的加权，其中权重是由Q和每个K计算出来的。<br> <img src="https://images2.imgbox.com/1a/17/zGuzJP4r_o.png" alt="请添加图片描述"></p> 
<h5><a id="KV__12"></a>KV 是怎么来的？</h5> 
<p>对于上述图像来说，我们将其分割为不同的小块，其中每个块的向量化表示则是向量K<br> <img src="https://images2.imgbox.com/db/b9/qVPmvkUp_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_16"></a>详细计算过程</h4> 
<p>刚刚提到，输出<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          V 
         
        
          ， 
         
        
       
      
        V^， 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord cjk_fallback mtight">，</span></span></span></span></span></span></span></span></span></span></span></span>是对所有value的加权，是由 Query 和每个 key 计算出来的，计算方法分为三步：</p> 
<ol><li>第一步：计算Q和K的相似度，可以用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          F 
         
        
       
         F 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">F</span></span></span></span></span>来表示：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          f 
         
        
          ( 
         
        
          Q 
         
        
          , 
         
         
         
           K 
          
         
           i 
          
         
        
          ) 
         
        
          , 
         
        
          i 
         
        
          = 
         
        
          1 
         
        
          , 
         
        
          2 
         
        
          , 
         
        
          . 
         
        
          . 
         
        
          . 
         
        
          , 
         
        
          n 
         
        
       
         f(Q,K_i),i=1,2,...,n 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8389em; vertical-align: -0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">n</span></span></span></span></span> 
  <ul><li>计算方法一般分为四种： 
    <ul><li>点乘：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
          
           
            
            
              f 
             
            
              ( 
             
            
              Q 
             
            
              , 
             
             
             
               K 
              
             
               i 
              
             
            
              ) 
             
            
              = 
             
             
             
               Q 
              
             
               T 
              
             
             
             
               K 
              
             
               i 
              
             
            
           
             f(Q,K_i)=Q^TK_i 
            
           
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.0358em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></li><li>权重：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
          
           
            
            
              f 
             
            
              ( 
             
            
              Q 
             
            
              , 
             
             
             
               K 
              
             
               i 
              
             
            
              ) 
             
            
              = 
             
             
             
               Q 
              
             
               T 
              
             
             
             
               K 
              
             
               i 
              
             
            
              W 
             
            
           
             f(Q,K_i)=Q^TK_iW 
            
           
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.0358em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span></span></span></span></span></li><li>拼接权重：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
          
           
            
            
              f 
             
            
              ( 
             
            
              Q 
             
            
              , 
             
             
             
               K 
              
             
               i 
              
             
            
              ) 
             
            
              = 
             
            
              [ 
             
             
             
               Q 
              
             
               T 
              
             
             
             
               K 
              
             
               i 
              
             
            
              ] 
             
            
              W 
             
            
           
             f(Q,K_i)=[Q^TK_i]W 
            
           
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.0913em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">]</span><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span></span></span></span></span></li><li>感知器：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
          
           
            
            
              f 
             
            
              ( 
             
            
              Q 
             
            
              , 
             
             
             
               K 
              
             
               i 
              
             
            
              ) 
             
            
              = 
             
             
             
               V 
              
             
               T 
              
             
            
              t 
             
            
              a 
             
            
              n 
             
            
              h 
             
            
              ( 
             
            
              W 
             
            
              Q 
             
            
              + 
             
            
              U 
             
             
             
               K 
              
             
               i 
              
             
            
              ) 
             
            
           
             f(Q,K_i)=V^Ttanh(WQ+UK_i) 
            
           
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.0913em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">t</span><span class="mord mathnormal">anh</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">U</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></li></ul> </li></ul> </li><li>第二布：将第一步得到的相似度进行SoftMax操作，进行归一化：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           α 
          
         
           i 
          
         
        
          = 
         
        
          s 
         
        
          o 
         
        
          f 
         
        
          t 
         
        
          m 
         
        
          a 
         
        
          x 
         
        
          ( 
         
         
          
          
            F 
           
          
            ( 
           
          
            Q 
           
          
            , 
           
           
           
             K 
            
           
             i 
            
           
          
            ) 
           
          
          
           
           
             d 
            
           
             k 
            
           
          
         
        
          ) 
         
        
       
         \alpha_i=softmax(\frac{F(Q,K_i)}{\sqrt{d_k}}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.548em; vertical-align: -0.538em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.01em;"><span class="" style="top: -2.5864em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8622em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mtight" style="padding-left: 0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.3488em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.8222em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail mtight" style="min-width: 0.853em; height: 1.08em;"> 
                    <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                     <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path> 
                    </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1778em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.485em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">F</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">Q</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: -0.0715em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.538em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span> 
  <ul><li>这一步进行归一化是避免得到的相似度<code>F1 = 50</code> 和 <code>F2 = 1</code> 之间的差值多大影响模型效果，归一化之后相似度可能就会变成<code>F1 = 0.8</code> 和 <code>F2 = 0.1</code>。</li></ul> </li><li>第三步：针对计算出来的权重<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           α 
          
         
           i 
          
         
        
       
         \alpha^i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>，对V中的所有values进行加权求和计算，得到Attention 向量：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          A 
         
        
          t 
         
        
          t 
         
        
          e 
         
        
          n 
         
        
          t 
         
        
          i 
         
        
          o 
         
        
          n 
         
        
          = 
         
         
         
           ∑ 
          
          
          
            i 
           
          
            = 
           
          
            1 
           
          
         
           m 
          
         
         
         
           α 
          
         
           i 
          
         
         
         
           V 
          
         
           i 
          
         
        
       
         Attention=\sum_{i=1}^{m}\alpha_iV_i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.104em; vertical-align: -0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8043em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.2222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></li></ol> 
<blockquote> 
 <p>注1：softmax函数原理：https://zhuanlan.zhihu.com/p/503321685<br> 注2：为什么要除<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            d 
           
          
            k 
           
          
         
        
       
         \sqrt{d_k} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em; vertical-align: -0.1828em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8572em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.8172em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
            <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
             <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path> 
            </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1828em;"><span class=""></span></span></span></span></span></span></span></span></span>：softmax函数的输入由key向量和query向量之间的点积组成,key向量和query向量的维度 越大，点积往往越大, 原文论中12个head对应的大小是64，作者在原论文中采用的补救措施，是将点积除以key和query维度的平方根</p> 
</blockquote> 
<h4><a id="Attention_30"></a>Attention计算实战</h4> 
<p>假设我们现在有下面这么一组数据，现在我们的问题是：腰围57其对应的体重是多少（query）</p> 
<table><thead><tr><th>腰围（key）</th><th>体重（value）</th></tr></thead><tbody><tr><td>51</td><td>40</td></tr><tr><td>56</td><td>43</td></tr><tr><td>58</td><td>48</td></tr></tbody></table> 
<p>对于单维度场景，我们认为57 腰围所对应的体重在43~48之间，同时考虑到51 腰围这个key，所以我们的公式可以表示为如下的形式：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          f 
         
        
          ( 
         
        
          57 
         
        
          ) 
         
        
          = 
         
         
         
           ∑ 
          
          
          
            i 
           
          
            = 
           
          
            1 
           
          
         
           3 
          
         
         
         
           α 
          
         
           i 
          
         
         
         
           V 
          
         
           i 
          
         
        
          = 
         
        
          α 
         
        
          ( 
         
        
          57 
         
        
          , 
         
        
          51 
         
        
          ) 
         
        
          ∗ 
         
        
          40 
         
        
          + 
         
        
          α 
         
        
          ( 
         
        
          57 
         
        
          , 
         
        
          56 
         
        
          ) 
         
        
          ∗ 
         
        
          43 
         
        
          + 
         
        
          α 
         
        
          ( 
         
        
          57 
         
        
          , 
         
        
          58 
         
        
          ) 
         
        
          ∗ 
         
        
          48 
         
        
       
         f(57) =\sum_{i=1}^{3}\alpha_iV_i= \alpha(57,51)*40+\alpha(57,56)*43+\alpha(57,58)*48 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord">57</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 3.0788em; vertical-align: -1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.8011em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.3em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.2222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mopen">(</span><span class="mord">57</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">51</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">40</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mopen">(</span><span class="mord">57</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">56</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">43</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mopen">(</span><span class="mord">57</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">58</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">48</span></span></span></span></span></span></p> 
<blockquote> 
 <p>注： 同样的，对于多维向量，我们采用点乘的方式依然可以得到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          f 
         
        
       
         f 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span></span></span></span></span></p> 
</blockquote> 
<h4><a id="SelfAttention__43"></a>Self-Attention 自注意力机制</h4> 
<p>Self-Attention是Attenion机制的特化。Self-Attention 同样也有着三个输入Q、K、V：对于Self-Attention，Q、K、V均来自句子X的词向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span>的线性转化。<br> 下面我们以“我有一只猫”这个过程来举例：</p> 
<blockquote> 
 <p>注：<br> 在真实的Transformer模型中，查询、键和值是通过与不同的权重矩阵相乘得到的，而且通常会使用多头注意力来并行计算多个独立的注意力分布，并将它们的结果拼接起来。</p> 
</blockquote> 
<ol><li><strong>输入词向量</strong></li></ol> 
<p>假设经过位置编码之后我们得到了如下表格所述的词向量：</p> 
<table><thead><tr><th align="center">词</th><th align="center">嵌入向量（模拟值）</th></tr></thead><tbody><tr><td align="center">我</td><td align="center">[0.1, 0.2]</td></tr><tr><td align="center">有</td><td align="center">[0.3, 0.4]</td></tr><tr><td align="center">一只</td><td align="center">[0.5, 0.6]</td></tr><tr><td align="center">猫</td><td align="center">[0.7, 0.8]</td></tr></tbody></table> 
<ol start="2"><li> <p><strong>计算Q、K、V</strong><br> 在自注意力机制中，我们需要为每个输入向量计算查询（Q）、键（K）和值（V）。然而，在这个简化的例子中，我们假设查询、键和值都与输入嵌入相同。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            Q 
           
          
            = 
           
          
            K 
           
          
            = 
           
          
            V 
           
          
            = 
           
          
            i 
           
          
            n 
           
          
            p 
           
          
            u 
           
          
            t 
           
          
         
           Q=K=V=input 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.854em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">in</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span></span></span></span></span></span></p> </li><li> <p><strong>计算注意力分数</strong><br> 对于每个查询（即每个词），我们需要计算它与所有键的点积，然后应用softmax函数来获得归一化的注意力权重。</p> </li></ol> 
<p>以第一个词“我”为例，计算过程如下：</p> 
<pre><code class="prism language-bash">注意力分数（我） <span class="token operator">=</span> softmax<span class="token punctuation">(</span><span class="token punctuation">[</span>
    Q<span class="token punctuation">(</span>我<span class="token punctuation">)</span> · K<span class="token punctuation">(</span>我<span class="token punctuation">)</span>,
    Q<span class="token punctuation">(</span>我<span class="token punctuation">)</span> · K<span class="token punctuation">(</span>有<span class="token punctuation">)</span>,
    Q<span class="token punctuation">(</span>我<span class="token punctuation">)</span> · K<span class="token punctuation">(</span>一只<span class="token punctuation">)</span>,
    Q<span class="token punctuation">(</span>我<span class="token punctuation">)</span> · K<span class="token punctuation">(</span>猫<span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">=</span> softmax<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">0.1</span>, <span class="token number">0.2</span><span class="token punctuation">]</span> · <span class="token punctuation">[</span><span class="token number">0.1</span>, <span class="token number">0.2</span><span class="token punctuation">]</span>,
    <span class="token punctuation">[</span><span class="token number">0.1</span>, <span class="token number">0.2</span><span class="token punctuation">]</span> · <span class="token punctuation">[</span><span class="token number">0.3</span>, <span class="token number">0.4</span><span class="token punctuation">]</span>,
    <span class="token punctuation">[</span><span class="token number">0.1</span>, <span class="token number">0.2</span><span class="token punctuation">]</span> · <span class="token punctuation">[</span><span class="token number">0.5</span>, <span class="token number">0.6</span><span class="token punctuation">]</span>,
    <span class="token punctuation">[</span><span class="token number">0.1</span>, <span class="token number">0.2</span><span class="token punctuation">]</span> · <span class="token punctuation">[</span><span class="token number">0.7</span>, <span class="token number">0.8</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">=</span> softmax<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.05</span>, <span class="token number">0.14</span>, <span class="token number">0.26</span>, <span class="token number">0.38</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

假设softmax后的结果为 <span class="token punctuation">[</span><span class="token number">0.1</span>, <span class="token number">0.2</span>, <span class="token number">0.3</span>, <span class="token number">0.4</span><span class="token punctuation">]</span>，这些就是“我”这个词对其他词的注意力权重。
</code></pre> 
<ol start="4"><li><strong>加权求和</strong><br> 最后，我们使用计算得到的注意力权重对值向量进行加权求和，得到每个词的自注意力输出。</li></ol> 
<pre><code class="prism language-bash">输出向量（我） <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token number">0.1</span> * V<span class="token punctuation">(</span>我<span class="token punctuation">)</span> + <span class="token number">0.2</span> * V<span class="token punctuation">(</span>有<span class="token punctuation">)</span> + <span class="token number">0.3</span> * V<span class="token punctuation">(</span>一只<span class="token punctuation">)</span> + <span class="token number">0.4</span> * V<span class="token punctuation">(</span>猫<span class="token punctuation">)</span>
<span class="token punctuation">]</span>

<span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token number">0.1</span> * <span class="token punctuation">[</span><span class="token number">0.1</span>, <span class="token number">0.2</span><span class="token punctuation">]</span> + <span class="token number">0.2</span> * <span class="token punctuation">[</span><span class="token number">0.3</span>, <span class="token number">0.4</span><span class="token punctuation">]</span> + <span class="token number">0.3</span> * <span class="token punctuation">[</span><span class="token number">0.5</span>, <span class="token number">0.6</span><span class="token punctuation">]</span> + <span class="token number">0.4</span> * <span class="token punctuation">[</span><span class="token number">0.7</span>, <span class="token number">0.8</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span>

<span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.46</span>, <span class="token number">0.58</span><span class="token punctuation">]</span>
</code></pre> 
<p>SelfAttention可以很好的帮我们解决机器不能理解语句之间语义的问题。以“我有一只猫”这句话为例，Self-attention的作用可以体现在以下几个方面：</p> 
<ul><li><strong>捕捉元素之间的依赖关系</strong>：在这句话中，“我”、“有”、“一只”和“猫”这四个元素之间存在依赖关系。通过Self-attention，模型可以计算出这些元素之间的注意力分数，从而理解它们之间的关系。例如，“我”和“有”之间的关系可能比较紧密，因为它们构成了一个主谓结构，而“一只”和“猫”之间的关系也比较紧密，因为它们构成了一个数量短语。通过捕捉这些依赖关系，模型可以更好地理解这句话的含义。</li><li><strong>实现长距离依赖建模</strong>：虽然这句话比较短，但在处理更长的序列时，Self-attention的优势就更加明显了。传统的循环神经网络（RNN）在处理长序列时容易出现梯度消失或爆炸的问题，导致无法对长距离依赖进行建模。而Self-attention机制可以直接计算序列中任意两个元素之间的关系，无论它们之间的距离有多远，因此可以很好地处理长序列并实现长距离依赖建模。</li><li><strong>降低计算复杂度</strong>：Self-attention机制通过矩阵计算和并行计算等技术实现高效的自注意力计算，从而降低计算复杂度并提高模型的训练和推理速度。</li></ul> 
<h4><a id="Masked_SelfAttention_107"></a>Masked Self-Attention</h4> 
<p>Masked Self-Attention（掩码自注意力机制）是自注意力机制的一个变种，它在某些特定情况下非常有用，比如在处理序列数据时，我们希望模型只关注到序列中的某些部分，而忽略其他部分。其在自注意力的基础上增加了一个掩码操作，这个掩码用于遮挡（或忽略）序列中的某些元素，使得它们在计算注意力分数时不被考虑。</p> 
<p>在“我有一只猫”这个例子中，如果我们使用掩码自注意力机制，并且假设我们想要模型在处理“有”这个词时只关注到“我”，而忽略后面的词，我们就可以通过掩码来实现这一点(掩码：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1100 
        
       
      
        1100 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">1100</span></span></span></span></span>)。具体来说，我们可以在计算注意力分数时，将“有”与“一只”和“猫”之间的分数设置为一个非常小的值（比如负无穷），这样在计算softmax函数时，这些分数就会接近于零，从而被忽略。</p> 
<h5><a id="Masked_SelfAttention__112"></a>Masked Self-Attention 实战</h5> 
<p>我们还是以“我有一只猫”来进行举例，首先假设初始词向量如下：</p> 
<pre><code class="prism language-bash">我:    <span class="token punctuation">[</span><span class="token number">0.5</span>, <span class="token number">0.2</span>, -0.1<span class="token punctuation">]</span>
有:    <span class="token punctuation">[</span><span class="token number">0.3</span>, -0.4, <span class="token number">0.2</span><span class="token punctuation">]</span>
一只:  <span class="token punctuation">[</span>-0.1, <span class="token number">0.5</span>, <span class="token number">0.3</span><span class="token punctuation">]</span>
猫:    <span class="token punctuation">[</span><span class="token number">0.2</span>, <span class="token number">0.1</span>, -0.3<span class="token punctuation">]</span>
</code></pre> 
<p>接下来，我们将计算注意力分数。但在计算之前，我们需要为每个位置定义一个掩码。掩码可以是一个二进制矩阵，其中1表示可关注的位置，0表示被掩码掉的位置。在这个例子中，如果我们正在计算“猫”，那么掩码矩阵将会是这样的:</p> 
<pre><code class="prism language-bash">我     有     一只   猫
<span class="token number">1</span>      <span class="token number">1</span>      <span class="token number">1</span>      <span class="token number">1</span>
</code></pre> 
<p>然后我们进入自注意力计算过程：</p> 
<ol><li>计算Q、K、V，还是假设这三者一致：</li></ol> 
<pre><code class="prism language-bash">Q <span class="token operator">=</span> K <span class="token operator">=</span> V <span class="token operator">=</span> <span class="token punctuation">[</span>
  <span class="token punctuation">[</span><span class="token number">0.5</span>, <span class="token number">0.2</span>, -0.1<span class="token punctuation">]</span>,   <span class="token comment"># 我</span>
  <span class="token punctuation">[</span><span class="token number">0.3</span>, -0.4, <span class="token number">0.2</span><span class="token punctuation">]</span>,   <span class="token comment"># 有</span>
  <span class="token punctuation">[</span>-0.1, <span class="token number">0.5</span>, <span class="token number">0.3</span><span class="token punctuation">]</span>,   <span class="token comment"># 一只</span>
  <span class="token punctuation">[</span><span class="token number">0.2</span>, <span class="token number">0.1</span>, -0.3<span class="token punctuation">]</span>    <span class="token comment"># 猫</span>
<span class="token punctuation">]</span>
</code></pre> 
<ol start="2"><li>计算未掩码的注意力分数</li></ol> 
<pre><code class="prism language-bash"><span class="token comment"># 这里我们只计算了最后一个词“猫”对其他词的注意力分数</span>
猫对我   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.2</span>, <span class="token number">0.1</span>, -0.3<span class="token punctuation">]</span> · <span class="token punctuation">[</span><span class="token number">0.5</span>, <span class="token number">0.2</span>, -0.1<span class="token punctuation">]</span>^T <span class="token operator">=</span> <span class="token number">0.2</span>*0.5 + <span class="token number">0.1</span>*0.2 + <span class="token punctuation">(</span>-0.3<span class="token punctuation">)</span>*<span class="token punctuation">(</span>-0.1<span class="token punctuation">)</span>
猫对有   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.2</span>, <span class="token number">0.1</span>, -0.3<span class="token punctuation">]</span> · <span class="token punctuation">[</span><span class="token number">0.3</span>, -0.4, <span class="token number">0.2</span><span class="token punctuation">]</span>^T
猫对一只 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.2</span>, <span class="token number">0.1</span>, -0.3<span class="token punctuation">]</span> · <span class="token punctuation">[</span>-0.1, <span class="token number">0.5</span>, <span class="token number">0.3</span><span class="token punctuation">]</span>^T
</code></pre> 
<ol start="3"><li>Softmax</li></ol> 
<pre><code class="prism language-bash">猫对我   <span class="token operator">=</span> <span class="token number">0.4</span>
猫对有   <span class="token operator">=</span> <span class="token number">0.3</span>
猫对一只 <span class="token operator">=</span> <span class="token number">0.3</span>
</code></pre> 
<ol start="4"><li>概率加权V矩阵中的向量</li></ol> 
<pre><code class="prism language-bash">猫的新表示 <span class="token operator">=</span> <span class="token number">0.4</span> * <span class="token punctuation">[</span><span class="token number">0.5</span>, <span class="token number">0.2</span>, -0.1<span class="token punctuation">]</span> + <span class="token number">0.3</span> * <span class="token punctuation">[</span><span class="token number">0.3</span>, -0.4, <span class="token number">0.2</span><span class="token punctuation">]</span> + <span class="token number">0.3</span> * <span class="token punctuation">[</span>-0.1, <span class="token number">0.5</span>, <span class="token number">0.3</span><span class="token punctuation">]</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7445884db2377429036701d01c1096b6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">《vtk9 book》 官方web版 第3章 - 计算机图形基础 （5 / 5）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/25e0a7a04eabe9820dc6e52cc3961092/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MySQL学习六：子查询</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>