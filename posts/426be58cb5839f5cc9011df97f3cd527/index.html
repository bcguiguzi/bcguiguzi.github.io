<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大模型训练技巧｜单卡&amp;多卡｜训练性能评测 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大模型训练技巧｜单卡&amp;多卡｜训练性能评测" />
<meta property="og:description" content="原视频：【单卡、多卡 BERT、GPT2 训练性能【100亿模型计划】】
此笔记主要参考了李沐老师的视频，感兴趣的同学也可以去看视频～
视频较长，这里放上笔记，与大家分享～
大模型对于计算资源的要求越来越高，如何在有限的资源下开展训练？
对于公司尤其是个人开发者来说，是一个非常有价值的问题。
本文将主要介绍大模型训练技巧，在单卡和多卡上的不同策略，以及对于性能的评测。
文章目录 1.GPU训练性能的测试脚本2.设置3.单卡性能内存消耗（这里指GPU的内存，即显存）实验现象性能优化1.kernal fusion操作：2.grad_accum3.丢弃中间结果 Megatron模型库优化总结 4.多卡性能数据并行张量并行ZeRO并行 5.结论1.使用足够大的批量大小。2.并行运算3.设备性能以上是针对大模型的一些训练技巧，相信可以给大家日常的训练带来一些有价值的建议～欢迎大家关注我～分享有用的AI算法笔记～ 1.GPU训练性能的测试脚本 2.设置 画红框的部分，会影响GPU的性能。
16位是半精度，32位是单精度，64位是双精度。bf16和fp16是精度。
optim是优化器。（adamw其实是一个多此一举的优化器。直接使用adam优化器就行。）
grad_accum表示是否要做梯度的累加。
steps表示要跑多少次的模型更新。
deepspeed是一种跑分布式的方式。
计算量：TFLOPS
huggingface是一个模型库
log文件
读取和解析log文件
核心是读取两个参数：gpu显存的峰值，和每秒读取的样本数
3.单卡性能 内存消耗（这里指GPU的内存，即显存） 1.模型的参数（大模型参数量大，占用空间大）
2.每一层的输出，即前向运算的中间计算结果，也叫activation
3.用的库，背后所占的内存。比如通信、cudnn
一般来说，占大头的是前两个。
注意：使用16位计算，并不能节省模型占用的空间。因为模型还是用32位来存的，模型的权重还是会转化为32位，进行更新。
但是如果使用16位运算，前向计算的中间结果（即activation）是16位的，这样就节省了空间。而且activation的大小和batchsize、序列长度、浮点运算量呈正比，所以如果使用16位运算，可以提高batchsize或者序列长度。
优先使用bf16，其次是fp16。
实验现象 fp32换为了fp16，性能并没有翻倍，说明还有别的地方在使用内存。
可能是内存带宽，造成了GPU性能的瓶颈。
性能优化 1.kernal fusion操作： 麻烦的python操作，用c&#43;&#43;的for loop重写一遍。
目的：减少中间变量的读写过程，同时减少调用python运算产生的额外开销。
一般不需要自己重写，直接调用apex库就可以。
2.grad_accum 每次处理完一个batch，不直接更新梯度。而是计算多个batch，将梯度进行累加，再做梯度更新。
如果batchsize=10，grad_accum=4，那么会在总的10*4=40个batch后，才会进行梯度更新。
但是grad_accum或者总的批量大小不能太大，批量大小太大会影响算法的收敛。
做微调的时候，批量大小不能太大，因为数据集本来就不大；如果做预训练，批量大小可以大一些，因为数据集本身就很大。
3.丢弃中间结果 前向运算的时候，每一层的中间结果都会被保存。
可以将中间的一些结果丢弃，节省内存消耗。等运算完最后输出，进行梯度反传的时候，再重新进行前向计算，重新得到中间结果。
增加一部分计算量，换取一部分内存空间，至少能让模型跑起来。
当模型真的非常大的时候，这一操作特别有用！可以用来增加批量大小。
Megatron模型库 Megatron模型库性能好，是因为自己手写了很多算子：
优化总结 1.尽量增加批量大小（提高训练效率）
2.尽量使用16位的浮点数（降低中间结果占用内存数目）
3.使用Megatron这样的模型库（对重点算子，进行了手写，优化性能）
4.多卡性能 NV4表示两张卡使用四条nv-link进行两张GPU的连接。
使用一个脚本进行gpu之间的带宽的测试。
由于除了前向计算和反向计算，通信和模型更新还占用了新能。
所以一张GPU变为两张GPU，性能并没有翻倍。（只有一张GPU，并不需要GPU间的通信）
数据并行 没有使用nv-link，导致通信减慢，耗时明显增加。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/426be58cb5839f5cc9011df97f3cd527/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-18T14:35:31+08:00" />
<meta property="article:modified_time" content="2023-06-18T14:35:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大模型训练技巧｜单卡&amp;多卡｜训练性能评测</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>原视频：【单卡、多卡 BERT、GPT2 训练性能【100亿模型计划】】<br> 此笔记主要参考了李沐老师的视频，感兴趣的同学也可以去看视频～<br> 视频较长，这里放上笔记，与大家分享～</p> 
</blockquote> 
<p><strong>大模型对于计算资源的要求越来越高，如何在有限的资源下开展训练？</strong></p> 
<p><strong>对于公司尤其是个人开发者来说，是一个非常有价值的问题。</strong></p> 
<p><strong>本文将主要介绍大模型训练技巧，在单卡和多卡上的不同策略，以及对于性能的评测。</strong></p> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#1GPU_11" rel="nofollow">1.GPU训练性能的测试脚本</a></li><li><a href="#2_13" rel="nofollow">2.设置</a></li><li><a href="#3_39" rel="nofollow">3.单卡性能</a></li><li><ul><li><a href="#GPU_40" rel="nofollow">内存消耗（这里指GPU的内存，即显存）</a></li><li><a href="#_54" rel="nofollow">实验现象</a></li><li><a href="#_57" rel="nofollow">性能优化</a></li><li><ul><li><a href="#1kernal_fusion_58" rel="nofollow">1.kernal fusion操作：</a></li><li><a href="#2grad_accum_64" rel="nofollow">2.grad_accum</a></li><li><a href="#3_72" rel="nofollow">3.丢弃中间结果</a></li></ul> 
   </li><li><a href="#Megatron_82" rel="nofollow">Megatron模型库</a></li><li><a href="#_87" rel="nofollow">优化总结</a></li></ul> 
  </li><li><a href="#4_93" rel="nofollow">4.多卡性能</a></li><li><ul><li><a href="#_102" rel="nofollow">数据并行</a></li><li><a href="#_111" rel="nofollow">张量并行</a></li><li><a href="#ZeRO_123" rel="nofollow">ZeRO并行</a></li></ul> 
  </li><li><a href="#5_128" rel="nofollow">5.结论</a></li><li><ul><li><a href="#1_130" rel="nofollow">1.使用足够大的批量大小。</a></li><li><a href="#2_141" rel="nofollow">2.并行运算</a></li><li><a href="#3_149" rel="nofollow">3.设备性能</a></li><li><a href="#_155" rel="nofollow">以上是针对大模型的一些训练技巧，相信可以给大家日常的训练带来一些有价值的建议～</a></li><li><a href="#AI_156" rel="nofollow">欢迎大家关注我～分享有用的AI算法笔记～</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="1GPU_11"></a>1.GPU训练性能的测试脚本</h2> 
<p><img src="https://images2.imgbox.com/1b/91/1tz6NSNc_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="2_13"></a>2.设置</h2> 
<p><img src="https://images2.imgbox.com/2b/68/bZTx5Tu8_o.png" alt="在这里插入图片描述"><br> 画红框的部分，会影响GPU的性能。</p> 
<p>16位是半精度，32位是单精度，64位是双精度。bf16和fp16是精度。</p> 
<p>optim是优化器。（adamw其实是一个多此一举的优化器。直接使用adam优化器就行。）</p> 
<p>grad_accum表示是否要做梯度的累加。</p> 
<p>steps表示要跑多少次的模型更新。</p> 
<p>deepspeed是一种跑分布式的方式。</p> 
<p><img src="https://images2.imgbox.com/d9/2a/NgD6GXyJ_o.png" alt="在这里插入图片描述"><br> 计算量：TFLOPS<br> <img src="https://images2.imgbox.com/e1/2c/ug7FyLkV_o.png" alt="在这里插入图片描述"><br> huggingface是一个模型库</p> 
<p><img src="https://images2.imgbox.com/a7/cf/RVNuHcAR_o.png" alt="在这里插入图片描述"><br> log文件<br> <img src="https://images2.imgbox.com/58/d9/AbWpZDkC_o.png" alt="在这里插入图片描述"><br> 读取和解析log文件</p> 
<p>核心是读取两个参数：gpu显存的峰值，和每秒读取的样本数<br> <img src="https://images2.imgbox.com/4b/49/G0lkjyhd_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="3_39"></a>3.单卡性能</h2> 
<h3><a id="GPU_40"></a>内存消耗（这里指GPU的内存，即显存）</h3> 
<p>1.模型的参数（大模型参数量大，占用空间大）</p> 
<p>2.每一层的输出，即前向运算的中间计算结果，也叫activation</p> 
<p>3.用的库，背后所占的内存。比如通信、cudnn</p> 
<p>一般来说，占大头的是前两个。<br> <img src="https://images2.imgbox.com/61/81/7b30Gdjd_o.png" alt="在这里插入图片描述"><br> 注意：使用16位计算，并不能节省模型占用的空间。因为模型还是用32位来存的，模型的权重还是会转化为32位，进行更新。</p> 
<p>但是如果使用16位运算，前向计算的中间结果（即activation）是16位的，这样就节省了空间。而且activation的大小和batchsize、序列长度、浮点运算量呈正比，所以如果使用16位运算，可以提高batchsize或者序列长度。</p> 
<p>优先使用bf16，其次是fp16。</p> 
<h3><a id="_54"></a>实验现象</h3> 
<p>fp32换为了fp16，性能并没有翻倍，说明还有别的地方在使用内存。<br> 可能是内存带宽，造成了GPU性能的瓶颈。</p> 
<h3><a id="_57"></a>性能优化</h3> 
<h4><a id="1kernal_fusion_58"></a>1.kernal fusion操作：</h4> 
<p>麻烦的python操作，用c++的for loop重写一遍。</p> 
<p>目的：减少中间变量的读写过程，同时减少调用python运算产生的额外开销。<br> <img src="https://images2.imgbox.com/4b/5b/Z3cfaKSC_o.png" alt="在这里插入图片描述"><br> 一般不需要自己重写，直接调用apex库就可以。</p> 
<h4><a id="2grad_accum_64"></a>2.grad_accum</h4> 
<p>每次处理完一个batch，不直接更新梯度。而是计算多个batch，将梯度进行累加，再做梯度更新。<br> <img src="https://images2.imgbox.com/35/75/3GLifb5c_o.png" alt="在这里插入图片描述"><br> 如果batchsize=10，grad_accum=4，那么会在总的10*4=40个batch后，才会进行梯度更新。</p> 
<p>但是grad_accum或者总的批量大小不能太大，批量大小太大会影响算法的收敛。</p> 
<p>做微调的时候，批量大小不能太大，因为数据集本来就不大；如果做预训练，批量大小可以大一些，因为数据集本身就很大。</p> 
<h4><a id="3_72"></a>3.丢弃中间结果</h4> 
<p>前向运算的时候，每一层的中间结果都会被保存。</p> 
<p>可以将中间的一些结果丢弃，节省内存消耗。等运算完最后输出，进行梯度反传的时候，再重新进行前向计算，重新得到中间结果。</p> 
<p>增加一部分计算量，换取一部分内存空间，至少能让模型跑起来。</p> 
<p>当模型真的非常大的时候，这一操作特别有用！可以用来增加批量大小。</p> 
<p><img src="https://images2.imgbox.com/5b/63/xgK2buOb_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Megatron_82"></a>Megatron模型库</h3> 
<p><img src="https://images2.imgbox.com/9d/aa/Ex0H7f83_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/82/91/9cbpOgNz_o.png" alt="在这里插入图片描述"><br> Megatron模型库性能好，是因为自己手写了很多算子：<br> <img src="https://images2.imgbox.com/72/a7/wYk9WEUv_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_87"></a>优化总结</h3> 
<p>1.尽量增加批量大小（提高训练效率）</p> 
<p>2.尽量使用16位的浮点数（降低中间结果占用内存数目）</p> 
<p>3.使用Megatron这样的模型库（对重点算子，进行了手写，优化性能）</p> 
<h2><a id="4_93"></a>4.多卡性能</h2> 
<p><img src="https://images2.imgbox.com/06/17/PgfXB2x3_o.png" alt="在这里插入图片描述"><br> NV4表示两张卡使用四条nv-link进行两张GPU的连接。<br> <img src="https://images2.imgbox.com/3f/46/mZ1Aq1Op_o.png" alt="在这里插入图片描述"><br> 使用一个脚本进行gpu之间的带宽的测试。<br> <img src="https://images2.imgbox.com/f5/f2/uWzP3sLB_o.png" alt="在这里插入图片描述"><br> 由于除了前向计算和反向计算，通信和模型更新还占用了新能。</p> 
<p>所以一张GPU变为两张GPU，性能并没有翻倍。（只有一张GPU，并不需要GPU间的通信）</p> 
<h3><a id="_102"></a>数据并行</h3> 
<p><img src="https://images2.imgbox.com/d1/dc/HarofnpY_o.png" alt="在这里插入图片描述"><br> 没有使用nv-link，导致通信减慢，耗时明显增加。</p> 
<p>nv-link可以增加带块，每次通信可以多传输信息，减少每一轮的通信次数。<br> <img src="https://images2.imgbox.com/4c/b4/QN98W3JJ_o.png" alt="在这里插入图片描述"><br> 数据并行的时候，每次梯度更新，GPU间都要进行通信。</p> 
<p>所以使用梯度累加（grad-accum），减少梯度更新次数，从而减少通信次数，可以提高性能。</p> 
<h3><a id="_111"></a>张量并行</h3> 
<p>如果有2张GPU，则将每一层的张量计算，拆为两部分。每个GPU计算完一部分张量后，再进行通信的交互。</p> 
<p>好处是每张GPU的计算量减少了，坏处是需要计算和通信必须是串行的。<br> <img src="https://images2.imgbox.com/23/c1/GYFgnwHj_o.png" alt="在这里插入图片描述"><br> 这里无法计算通信事件，因为通信时间都在前向计算和反向计算里了。<br> <img src="https://images2.imgbox.com/87/af/pzFdzCkD_o.png" alt="在这里插入图片描述"><br> TP表示张量并行。TP的时候，梯度累加效果很小。因为通信次数已经很多了，就算减少梯度更新的次数，也没用。<br> <img src="https://images2.imgbox.com/8c/75/NoMGVwEm_o.png" alt="在这里插入图片描述"><br> 张量并行的好处是，可以训练大模型（比如1.3B的参数量）。将每一层的运算在多个卡上计算。否则，批量大小为1也跑不起来。</p> 
<p>由于参数量太大，所以梯度更新、进行优化的时间占比就比较大。所以使用梯度累加，还是有一些效果的。</p> 
<h3><a id="ZeRO_123"></a>ZeRO并行</h3> 
<p><img src="https://images2.imgbox.com/cc/cc/XfGWcSzH_o.png" alt="在这里插入图片描述"><br> 使用ZeRO2，是将整个模型和adam里的状态，将梯度进行切分，每个GPU只需要维护一部分。</p> 
<p>这样可以显著降低模型相关的内存占用，可以训练更大的模型。</p> 
<h2><a id="5_128"></a>5.结论</h2> 
<p>训练大模型的方法</p> 
<h3><a id="1_130"></a>1.使用足够大的批量大小。</h3> 
<p>一方面可以使得单个的算子的性能上升。另一方面可以降低模型梯度更新和通信带来的额外开销。</p> 
<p>处理方法：1.GPU内存更大。2.使用16位运算。3.kernal fusion合并运算。4.梯度累加。5.梯度的ckpt记录中间结果。</p> 
<p>但是特别大批量大小，会使得算法收敛变慢，需要更多的迭代才能使得算法收敛。</p> 
<p>微调的时候，数据规模小，就不能使用太大的批量大小。预训练的时候，可以使用更大的批量大小。</p> 
<p>当GPU数目特别大（成百上千）的时候，每张卡分配到的批量大小也会缩小，这个也需要改变。</p> 
<h3><a id="2_141"></a>2.并行运算</h3> 
<p>优先在单卡内部做数据并行。因为通信消耗少。</p> 
<p>多卡并行。</p> 
<p>ZeRO并行。划分模型和中间状态。</p> 
<p>张量并行。将每一层的计算切开。</p> 
<h3><a id="3_149"></a>3.设备性能</h3> 
<p>nvi-link优化通信。</p> 
<p>更大内存的GPU。</p> 
<h3><a id="_155"></a>以上是针对大模型的一些训练技巧，相信可以给大家日常的训练带来一些有价值的建议～</h3> 
<h3><a id="AI_156"></a>欢迎大家关注我～分享有用的AI算法笔记～</h3>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2b28b4e347a80dee6b8f338fa69f2735/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">chatgpt赋能python：Python跨平台：让你的代码无处不在</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bd0e10f20fa8c744a1e9e7f2292c07fb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">虚拟机ssh连接网络时显示未连接原因有哪些以及解决方案</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>