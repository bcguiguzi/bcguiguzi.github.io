<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch异常——loss异常，不断增大，并且loss出现inf - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch异常——loss异常，不断增大，并且loss出现inf" />
<meta property="og:description" content="文章目录 异常报错异常截图异常代码原因解释修正代码执行结果 异常报错 epoch1:loss3667.782471 epoch2:loss65358620.000000 epoch3:loss14979486720.000000 epoch4:loss1739650891776.000000 epoch5:loss12361745880317952.000000 epoch6:loss2740315398365287284736.000000 epoch7:loss1176857261847129541794856960.000000 epoch8:loss7211548287231028836649926656.000000 epoch9:loss7537356298471407320145204346880.000000 epoch10:lossinf 异常截图 异常代码 # 初始化模型的参数,使用正态分布来初始化权重参数，将偏置设置为0 net[0].weight.data.normal_(0,0.01) net[0].bias.data.fill_(0) # 定义损失函数 loss = nn.MSELoss() # 定义优化算法 trainer = torch.optim.SGD(net.parameters(),lr = 0.03) # 训练 # 训练过程：遍历完整的数据集，每一次都是抽取一个batch_size，然后在进行前向传播计算对应的loss,然后将loss反向传播，计算梯度，然后根据梯度优化参数 num_epochs = 10 for epoch in range(num_epochs): for X,y in data_iter: l = loss(net(X),y) l.backward() trainer.step() l = loss(net(features),labels) print(f&#39;epoch{epoch&#43;1}:loss{l:f}&#39;) 原因解释 每一个batch_size之后，都没有进行梯度清零，模型参数更新是基于之前所有的mini_batch，并不是基于当前的mini_batch
导致如下问题
梯度爆炸：如果梯度值在每次迭代中都相对较大，那么累积梯度可能会迅速变得非常大，导致权重更新太过极端。这通常会导致损失值变成 NaN 或 Inf训练不稳定：如果梯度值在每次迭代中都相对较大，那么累积梯度可能会迅速变得非常大，导致权重更新太过极端。这通常会导致损失值变成 NaN 或 Inf 梯度下降的基本假设：
每次更新都是基于最近一次计算出的梯度， 修正代码 # 初始化模型的参数,使用正态分布来初始化权重参数，将偏置设置为0 net[0]." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/765d08b16c7c146eeff69d2d4a79d41a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-30T21:49:52+08:00" />
<meta property="article:modified_time" content="2023-08-30T21:49:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch异常——loss异常，不断增大，并且loss出现inf</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#_1" rel="nofollow">异常报错</a></li><li><a href="#_18" rel="nofollow">异常截图</a></li><li><a href="#_22" rel="nofollow">异常代码</a></li><li><a href="#_48" rel="nofollow">原因解释</a></li><li><a href="#_57" rel="nofollow">修正代码</a></li><li><a href="#_84" rel="nofollow">执行结果</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="_1"></a>异常报错</h3> 
<pre><code class="prism language-bash">epoch1:loss3667.782471
epoch2:loss65358620.000000
epoch3:loss14979486720.000000
epoch4:loss1739650891776.000000
epoch5:loss12361745880317952.000000
epoch6:loss2740315398365287284736.000000
epoch7:loss1176857261847129541794856960.000000
epoch8:loss7211548287231028836649926656.000000
epoch9:loss7537356298471407320145204346880.000000
epoch10:lossinf



</code></pre> 
<h3><a id="_18"></a>异常截图</h3> 
<p><img src="https://images2.imgbox.com/d3/51/ZRVGPa3b_o.png" alt=""></p> 
<h3><a id="_22"></a>异常代码</h3> 
<pre><code class="prism language-python"><span class="token comment"># 初始化模型的参数,使用正态分布来初始化权重参数，将偏置设置为0</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0.01</span><span class="token punctuation">)</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment"># 定义损失函数</span>
loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># 定义优化算法</span>
trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.03</span><span class="token punctuation">)</span>

<span class="token comment"># 训练</span>
<span class="token comment"># 训练过程：遍历完整的数据集，每一次都是抽取一个batch_size，然后在进行前向传播计算对应的loss,然后将loss反向传播，计算梯度，然后根据梯度优化参数</span>
num_epochs <span class="token operator">=</span> <span class="token number">10</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span>y<span class="token punctuation">)</span>
        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        trainer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">,</span>labels<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'epoch</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">:loss</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>l<span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_48"></a>原因解释</h3> 
<ul><li> <p>每一个batch_size之后，都没有进行梯度清零，模型参数更新是基于之前所有的mini_batch，并不是基于当前的mini_batch</p> </li><li> <p>导致如下问题</p> 
  <ul><li><strong>梯度爆炸</strong>：如果梯度值在每次迭代中都相对较大，那么累积梯度可能会迅速变得非常大，导致权重更新太过极端。这通常会导致<strong>损失值变成 NaN 或 Inf</strong></li><li><strong>训练不稳定</strong>：如果梯度值在每次迭代中都相对较大，那么累积梯度可能会迅速变得非常大，导致权重更新太过极端。这通常会导致损失值变成 NaN 或 Inf</li></ul> </li><li> <p>梯度下降的基本假设：</p> 
  <ul><li><strong>每次更新都是基于最近一次计算出的梯度，</strong></li></ul> </li></ul> 
<h3><a id="_57"></a>修正代码</h3> 
<pre><code class="prism language-python"><span class="token comment"># 初始化模型的参数,使用正态分布来初始化权重参数，将偏置设置为0</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0.01</span><span class="token punctuation">)</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment"># 定义损失函数</span>
loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># 定义优化算法</span>
trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.03</span><span class="token punctuation">)</span>

<span class="token comment"># 训练</span>
<span class="token comment"># 训练过程：遍历完整的数据集，每一次都是抽取一个batch_size，然后在进行前向传播计算对应的loss,然后将loss反向传播，计算梯度，然后根据梯度优化参数</span>
num_epochs <span class="token operator">=</span> <span class="token number">10</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span>y<span class="token punctuation">)</span>
        trainer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        trainer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">,</span>labels<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'epoch</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">:loss</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>l<span class="token punctuation">:</span><span class="token format-spec">f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_84"></a>执行结果</h3> 
<p><img src="https://images2.imgbox.com/e4/e7/yNfkXkBj_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3e92fb1f07443e71edc5d15ef9e4470d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vue使用webrtcstreamer实现rtsp无转码播放实时监控</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/457b59770d99a3765fe58190adea6b27/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">一种解决方法：Can‘t locate FindBin.pm in @INC (you may need to install the FindBin module)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>