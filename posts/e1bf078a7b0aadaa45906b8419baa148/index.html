<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>论文阅读_参数微调_P-tuning_v2 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="论文阅读_参数微调_P-tuning_v2" />
<meta property="og:description" content="1 P-Tuning PLAINTEXT
1 2 3 4 5 6 7 英文名称: GPT Understands, Too 中文名称: GPT也懂 链接: https://arxiv.org/abs/2103.10385 作者: Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang 机构: 清华大学, 麻省理工学院 日期: 2021-03-18 引用次数: 426 目标：大模型的 Prompt 构造方式严重影响下游任务的效果。离散化的 token 的搜索出来的结果可能并不是最优的，导致性能不稳定。本篇论文旨在探讨，如何提升预训练语言模型进行自然语言提示的有效性。
方法：作者提出了 P-Tuning，设计了一种连续可微的 virtual token（同 Prefix-Tuning 类似）。将 Prompt 转换为可以学习的 Embedding 层，用 MLP&#43;LSTM 的方式来对 Prompt Embedding 进行处理。
结论：弥合 GPT 和 NLU 应用程序之间的差距 (2021 年)，P 调参后的 GPT 可以比在 NLU 调参的类似大小的 BERT 效果更好。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/e1bf078a7b0aadaa45906b8419baa148/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-17T14:53:43+08:00" />
<meta property="article:modified_time" content="2024-03-17T14:53:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">论文阅读_参数微调_P-tuning_v2</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3 id="p-tuning">1 P-Tuning</h3> 
<p>PLAINTEXT</p> 
<table><tbody><tr><td> <pre>1
2
3
4
5
6
7
</pre> </td><td> <pre>英文名称: GPT Understands, Too
中文名称: GPT也懂
链接: https://arxiv.org/abs/2103.10385
作者: Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang
机构: 清华大学, 麻省理工学院
日期: 2021-03-18
引用次数: 426
</pre> </td></tr></tbody></table> 
<p>目标：大模型的 Prompt 构造方式严重影响下游任务的效果。离散化的 token 的搜索出来的结果可能并不是最优的，导致性能不稳定。本篇论文旨在探讨，如何提升预训练语言模型进行自然语言提示的有效性。</p> 
<p>方法：作者提出了 P-Tuning，设计了一种连续可微的 virtual token（同 Prefix-Tuning 类似）。将 Prompt 转换为可以学习的 Embedding 层，用 MLP+LSTM 的方式来对 Prompt Embedding 进行处理。</p> 
<p>结论：弥合 GPT 和 NLU 应用程序之间的差距 (2021 年)，P 调参后的 GPT 可以比在 NLU 调参的类似大小的 BERT 效果更好。</p> 
<p></p> 
<p class="img-center"><a href="https://www.xyan666.com/support/attachments_2024/Pasted%20image%2020240306090445.png" rel="nofollow"><img alt="" height="284" src="https://images2.imgbox.com/02/b6/1KKEw2v2_o.png" width="1123"></a></p> 
<p>主图：一个关于“英国的首都是 [MASK]”的提示搜索的例子。在蓝色区域表示上下文（“英国”），红色区域表示目标（“[MASK]”），橙色区域表示提示。在（a）中，提示生成器只接收离散的奖励；在（b）中，连续的提示嵌入和提示编码器可以通过可微的方式进行优化。</p> 
<h3 id="p-tuning-v2">2 P-Tuning v2</h3> 
<p>PLAINTEXT</p> 
<table><tbody><tr><td> <pre>1
2
3
4
5
6
7
</pre> </td><td> <pre>英文名称: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
中文名称: P-Tuning v2：提示调整可以在各种规模和任务上普遍与微调相媲美
链接: http://arxiv.org/abs/2110.07602v3
作者: Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang
机构: 清华大学, 北京人工智能学会, 上海启智研究院
日期: 2021-10-14
引用次数: 310
</pre> </td></tr></tbody></table> 
<p></p> 
<p class="img-center"><a href="https://www.xyan666.com/support/attachments_2024/Pasted%20image%2020240306085218.png" rel="nofollow"><img alt="" height="576" src="https://images2.imgbox.com/da/5f/IquMTrNg_o.png" width="1200"></a></p> 
<p>目标：研究的目的是探索如何通过优化提示调整方法，在各种模型规模和自然语言理解任务中实现普遍有效的目标。</p> 
<p>方法：最显著的改进是在预训练模型的每一层应用连续提示，而不仅仅是输入层。深度提示调整增加了连续提示的容量，并缩小了不同设置下微调差距的范围，尤其适用于小型模型和复杂任务。</p> 
<p>结论：研究发现，经过适当优化的提示调整方法可以在各种模型规模和自然语言理解任务中达到与微调相当的性能，而只需调整 0.1%-3% 的参数（P-tuning 调整 0.01% 参数）。P-Tuning v2 被认为可以作为微调的替代方法，并为未来研究提供了一个强有力的基准。</p> 
<p></p> 
<p class="img-center"><a href="https://www.xyan666.com/support/attachments_2024/Pasted%20image%2020240306084908.png" rel="nofollow"><img alt="" height="269" src="https://images2.imgbox.com/3e/9d/HmJwNfok_o.png" width="1133"></a></p> 
<p>主图：P-tuning 到 P-tuning v2 对比。橙色块（即 h0，…，hi）指的是可训练的提示嵌入；蓝色块是由冻结的预训练语言模型存储或计算的嵌入。</p> 
<h3 id="实际使用">3 实际使用</h3> 
<ul><li>ChatGLM-6B 62 亿参数；</li><li>INT4 量化级别下最低只需 6GB 显存即可运行；</li><li>INT4 量化级别下最低只需 7GB 显存即可 p-tuning v2 微调；</li></ul> 
<p></p> 
<p class="img-center"><a href="https://www.xyan666.com/support/attachments_2024/Pasted%20image%2020240306083637.png" rel="nofollow"><img alt="" height="287" src="https://images2.imgbox.com/69/c6/meodExDp_o.png" width="1054"></a></p> 
<ul><li>参考：<a href="https://baijiahao.baidu.com/s?id=1765123631287305087&amp;wfr=spider&amp;for=pc" rel="nofollow" title="ChatGLM-6B 部署与 P-Tuning 微调实战">ChatGLM-6B 部署与 P-Tuning 微调实战</a></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f1651164bdc22a68efabcb6582b212b6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">简单高效多语言请求的主流电商平台API数据采集实时接口如何采集数据</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b7eb0605d1f78c9a8df99c88517423cc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">阿里云-零基础入门推荐系统 【特征工程】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>