<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>torch.Tensor的几种乘法介绍 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="torch.Tensor的几种乘法介绍" />
<meta property="og:description" content="点乘 a与b做*乘法，原则是如果a与b的size不同，则以某种方式将a或b进行复制，使得复制后的a和b的size相同，然后再将a和b做element-wise的乘法。
* 标量 Tensor与标量k做*乘法的结果是Tensor的每个元素乘以k（相当于把k复制成与lhs大小相同，元素全为k的Tensor）。
&gt;&gt;&gt; a = torch.ones(3,4) &gt;&gt;&gt; a tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) &gt;&gt;&gt; a * 2 tensor([[2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.]]) * 一维向量 Tensor与行向量做*乘法的结果是每列乘以行向量对应列的值（相当于把行向量的行复制）。 注意此时要求Tensor的列数与行向量的列数相等。
&gt;&gt;&gt; a = torch.ones(3,4) &gt;&gt;&gt; a tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) &gt;&gt;&gt; b = torch.Tensor([1,2,3,4]) &gt;&gt;&gt; b tensor([1., 2., 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/cb8c72b16e5f43a2169236bc4dde5f3b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-03-09T21:32:05+08:00" />
<meta property="article:modified_time" content="2021-03-09T21:32:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">torch.Tensor的几种乘法介绍</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>点乘</h3> 
<p>a与b做*乘法，原则是如果a与b的size不同，则以某种方式将a或b进行复制，使得复制后的a和b的size相同，然后再将a和b做<strong>element-wise的乘法</strong>。</p> 
<h4>* 标量</h4> 
<p>Tensor与标量k做*乘法的结果是Tensor的每个元素乘以k（相当于把k复制成与lhs大小相同，元素全为k的Tensor）。</p> 
<pre><code class="language-python">&gt;&gt;&gt; a = torch.ones(3,4)
&gt;&gt;&gt; a
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
&gt;&gt;&gt; a * 2
tensor([[2., 2., 2., 2.],
        [2., 2., 2., 2.],
        [2., 2., 2., 2.]])
</code></pre> 
<h4>* 一维向量</h4> 
<p><strong>Tensor与行向量做*乘法的结果是每列乘以行向量对应列的值（相当于把行向量的行复制）。 注意此时要求Tensor的列数与行向量的列数相等。</strong></p> 
<pre><code class="language-python">&gt;&gt;&gt; a = torch.ones(3,4)
&gt;&gt;&gt; a
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
&gt;&gt;&gt; b = torch.Tensor([1,2,3,4])
&gt;&gt;&gt; b
tensor([1., 2., 3., 4.])
&gt;&gt;&gt; a * b
tensor([[1., 2., 3., 4.],
        [1., 2., 3., 4.],
        [1., 2., 3., 4.]])
</code></pre> 
<p><strong>Tensor与列向量做*乘法的结果是每行乘以列向量对应行的值（相当于把列向量的列复制，成为与lhs维度相同的Tensor）. 注意此时要求Tensor的行数与列向量的行数相等。</strong></p> 
<pre><code class="language-python">&gt;&gt;&gt; a = torch.ones(3,4)
&gt;&gt;&gt; a
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
&gt;&gt;&gt; b = torch.Tensor([1,2,3]).reshape((3,1))
&gt;&gt;&gt; b
tensor([[1.],
        [2.],
        [3.]])
&gt;&gt;&gt; a * b
tensor([[1., 1., 1., 1.],
        [2., 2., 2., 2.],
        [3., 3., 3., 3.]])</code></pre> 
<p><strong> broadcast</strong><br> 点积是broadcast的。broadcast是torch的一个概念，简单理解就是在一定的规则下允许高维Tensor和低维Tensor之间的运算。broadcast的概念稍显复杂，在此不做展开，可以参考官方文档关于broadcast的介绍.。<br> 这里举一个点积broadcast的例子。在例子中，a是二维Tensor，b是三维Tensor，但是a的维度与b的后两位相同，那么a和b仍然可以做点积，点积结果是一个和b维度一样的三维Tensor，运算规则是：若c = a * b, 则c[i,*,*] = a * b[i, *, *]，即沿着b的第0维做二维Tensor点积，或者可以理解为运算前将a沿着b的第0维也进行了expand操作，即a = a.expand(b.size()); a * b。<br>  </p> 
<p><strong>对应点相乘，torch.mul(x, y) ，即点乘操作，点乘不求和操作，又可以叫作Hadamard product；点乘再求和，即为卷积。  用法与*乘法相同，也是element-wise的乘法，也是支持broadcast的。</strong></p> 
<pre><code class="language-python">&gt;&gt;&gt; a = torch.Tensor([[1,2], [3,4], [5, 6]])
&gt;&gt;&gt; a
tensor([[1., 2.],
        [3., 4.],
        [5., 6.]])
&gt;&gt;&gt; torch.mul(a, a)
tensor([[ 1.,  4.],
        [ 9., 16.],
        [25., 36.]])
 
# a*a等价于torch.mul(a, a)</code></pre> 
<pre><code class="language-python">&gt;&gt;&gt; a = torch.tensor([[1, 2], [2, 3]])
&gt;&gt;&gt; torch.mul(a,a)
tensor([[1, 4],
        [4, 9]])


&gt;&gt;&gt; a = torch.tensor([[1, 2], [2, 3]])
&gt;&gt;&gt; b = torch.tensor([[[1,2],[2,3]],[[-1,-2],[-2,-3]]])
&gt;&gt;&gt; torch.mul(a,b)
tensor([[[ 1,  4],
         [ 4,  9]],

        [[-1, -4],
         [-4, -9]]])
</code></pre> 
<p> </p> 
<p><strong>矩阵相乘，torch.mm(x, y) ， 矩阵大小需满足： (i, n)x(n, j)。数学里的矩阵乘法，要求两个Tensor的维度满足矩阵乘法的要求。​​​</strong></p> 
<pre><code class="language-python">&gt;&gt;&gt; a
tensor([[1., 2.],
        [3., 4.],
        [5., 6.]])
&gt;&gt;&gt; b = a.t()  # 转置
&gt;&gt;&gt; b
tensor([[1., 3., 5.],
        [2., 4., 6.]])
 
&gt;&gt;&gt; torch.mm(a, b)
tensor([[ 5., 11., 17.],
        [11., 25., 39.],</code></pre> 
<p><strong>1. 二维矩阵乘法 torch.mm()</strong></p> 
<pre><code>torch.mm(mat1, mat2, out=None)</code></pre> 
<p><img alt="" height="35" src="https://images2.imgbox.com/2a/42/P7hbGJUG_o.png" width="524"></p> 
<p>该函数一般只用来计算两个二维矩阵的矩阵乘法，并且不支持broadcast操作。</p> 
<p><strong>2. 三维带batch的矩阵乘法 torch.bmm()</strong></p> 
<p>由于神经网络训练一般采用mini-batch，经常输入的时三维带batch的矩阵，所以提供</p> 
<pre><code>torch.bmm(bmat1, bmat2, out=None)</code></pre> 
<p><img alt="" height="50" src="https://images2.imgbox.com/56/56/IDWqRBXS_o.png" width="607"></p> 
<p>该函数的两个输入必须是三维矩阵并且第一维相同（表示Batch维度）， 不支持broadcast操作<br><br><strong>3. 多维矩阵乘法 torch.matmul()  torch.mm的broadcast版本.​​​​​​​</strong></p> 
<pre><code>torch.matmul(input, other, out=None)</code></pre> 
<pre><code class="language-python">&gt;&gt;&gt; a = torch.ones(3,4)
&gt;&gt;&gt; b = torch.ones(5,4,2)
&gt;&gt;&gt; torch.matmul(a, b)
tensor([[[4., 4.],
         [4., 4.],
         [4., 4.]],

        [[4., 4.],
         [4., 4.],
         [4., 4.]],

        [[4., 4.],
         [4., 4.],
         [4., 4.]],

        [[4., 4.],
         [4., 4.],
         [4., 4.]],

        [[4., 4.],
         [4., 4.],
         [4., 4.]]])
</code></pre> 
<p> 支持broadcast操作，使用起来比较复杂。针对多维数据 matmul() 乘法，可以认为该乘法使用使用两个参数的后两个维度来计算，其他的维度都可以认为是batch维度。假设两个输入的维度分别是<code>input</code>(1000×500×99×111000×500×99×11), <code>other</code>(500×11×99500×11×99)那么我们可以认为<code>torch.matmul(input, other, out=None)</code>乘法首先是进行后两位矩阵乘法得到(99×11)×(11×99)⇒(99×99)(99×11)×(11×99)⇒(99×99) ，然后分析两个参数的batch size分别是 (1000×500)(1000×500) 和 500500 , 可以广播成为 (1000×500)(1000×500)， 因此最终输出的维度是(1000×500×99×991000×500×99×99)。</p> 
<p><strong>4. 矩阵逐元素(Element-wise)乘法 torch.mul()</strong></p> 
<pre><code>torch.mul(mat1, other, out=None)</code></pre> 
<p>其中 other 乘数可以是标量，也可以是任意维度的矩阵， 只要满足最终相乘是可以broadcast的即可。</p> 
<p><strong>5. torch.einsum()</strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/05a4f467923dd5ebe6973b29a81517af/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">网站字体颜色大小php代码,html字体颜色的设置方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/79dbe24f7445a309c6ef34eb63006835/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">如何将php改成mp4,php使用memcoder将视频转成mp4格式的方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>