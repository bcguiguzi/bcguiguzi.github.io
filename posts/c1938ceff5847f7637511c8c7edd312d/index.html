<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>零基础小白也行，只用一行命令在自己的电脑跑大模型 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="零基础小白也行，只用一行命令在自己的电脑跑大模型" />
<meta property="og:description" content="什么是Ollama Ollama是一款免费开源的工具，拥有开箱即用的大模型，省去安装环境和下载模型的步骤，让零基础的人也能用起大模型。
项目地址
下载方法 通过下载链接可以找到对应的操作系统的下载版本，而且访问该网站不受限制，不需要神秘力量。
对于Mac和Win，可以直接通过点击下载桌面客户端，非常方便。
而Linux也可以通过一条命令完成安装。
# linux系统安装 curl -fsSL https://ollama.com/install.sh | sh 使用方法 下载完客户端并安装成功之后，打开命令行终端，就会显示ollama命令已经可以使用了。以后无需再打开客户端软件，只需要在命令行输入相关命令即可。
# 参数说明 Large language model runner Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model from a Modelfile show Show information for a model run Run a model pull Pull a model from a registry push Push a model to a registry list List models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/c1938ceff5847f7637511c8c7edd312d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-12T18:00:00+08:00" />
<meta property="article:modified_time" content="2024-03-12T18:00:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">零基础小白也行，只用一行命令在自己的电脑跑大模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Ollama_1"></a>什么是Ollama</h2> 
<p>Ollama是一款免费开源的工具，拥有开箱即用的大模型，省去安装环境和下载模型的步骤，让零基础的人也能用起大模型。<br> <a href="https://github.com/ollama/ollama">项目地址</a></p> 
<h2><a id="_5"></a>下载方法</h2> 
<p>通过<a href="https://ollama.com/download" rel="nofollow">下载链接</a>可以找到对应的操作系统的下载版本，而且访问该网站不受限制，不需要神秘力量。</p> 
<p>对于Mac和Win，可以直接通过点击下载桌面客户端，非常方便。</p> 
<p><img src="https://images2.imgbox.com/19/90/ZH3rxo4V_o.jpg" alt="下载"><br> 而Linux也可以通过一条命令完成安装。</p> 
<pre><code class="prism language-bash"><span class="token comment"># linux系统安装</span>
<span class="token function">curl</span> <span class="token parameter variable">-fsSL</span> https://ollama.com/install.sh <span class="token operator">|</span> <span class="token function">sh</span>
</code></pre> 
<h2><a id="_20"></a>使用方法</h2> 
<p>下载完客户端并安装成功之后，打开命令行终端，就会显示<code>ollama</code>命令已经可以使用了。以后无需再打开客户端软件，只需要在命令行输入相关命令即可。</p> 
<pre><code class="prism language-bash"><span class="token comment"># 参数说明</span>
Large language model runner

Usage:
  ollama <span class="token punctuation">[</span>flags<span class="token punctuation">]</span>
  ollama <span class="token punctuation">[</span>command<span class="token punctuation">]</span>

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information <span class="token keyword">for</span> a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  <span class="token function">cp</span>          Copy a model
  <span class="token function">rm</span>          Remove a model
  <span class="token builtin class-name">help</span>        Help about any <span class="token builtin class-name">command</span>

Flags:
  -h, <span class="token parameter variable">--help</span>      <span class="token builtin class-name">help</span> <span class="token keyword">for</span> ollama
  -v, <span class="token parameter variable">--version</span>   Show version information

Use <span class="token string">"ollama [command] --help"</span> <span class="token keyword">for</span> <span class="token function">more</span> information about a command.
</code></pre> 
<p>查看已经下载的模型</p> 
<pre><code class="prism language-bash">ollama list
</code></pre> 
<p>下载某个模型</p> 
<pre><code class="prism language-bash">ollama pull <span class="token operator">&lt;</span>MODEL_NAME<span class="token operator">&gt;</span>
</code></pre> 
<p>快速启动模型对话</p> 
<pre><code class="prism language-bash">ollama run <span class="token operator">&lt;</span>MODEL_NAME<span class="token operator">&gt;</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/46/64/tL0XAwck_o.jpg" alt="部分已经支持的模型"><br> <a href="https://ollama.com/library" rel="nofollow">完整模型列表</a></p> 
<h2><a id="_67"></a>找不到想要的模型？自行定制吧！</h2> 
<h3><a id="GGUF_68"></a>使用GGUF的模型文件</h3> 
<blockquote> 
 <p>.GGUF是一种二进制格式，用于快速加载和保存大语言模型。</p> 
</blockquote> 
<pre><code class="prism language-bash"><span class="token comment"># 1 创建一个名为Modelfile的文件，填入如下内容，表示从本地导入vicuna-33b经过4bit量化的模型文件</span>
FROM ./vicuna-33b.Q4_0.gguf
<span class="token comment"># 2 从Modelfile创建模型</span>
ollama create <span class="token operator">&lt;</span>YOUR_MODEL_NAME<span class="token operator">&gt;</span> <span class="token parameter variable">-f</span> Modelfile
<span class="token comment"># 运行模型</span>
ollama run <span class="token operator">&lt;</span>YOUR_MODEL_NAME<span class="token operator">&gt;</span>
</code></pre> 
<h3><a id="PyTorch__Safetensors_81"></a>PyTorch &amp; Safetensors</h3> 
<p>虽然也能把pytorch.model.bin或者.safetensors的模型导入，但会比较耗时，本质上也是通过llama.cpp库对原始模型进行量化，然后再通过Modelfile文件创建。<a href="https://github.com/ollama/ollama/blob/main/docs/import.md">详细可以查看</a></p> 
<h2><a id="API_85"></a>API接口</h2> 
<blockquote> 
 <p>在启动ollama后，本机的11434端口会启动相应的API。方便开发者搭建自己的大模型应用，比如与文档对话或者与表格对话等，可参考之前的文章。</p> 
</blockquote> 
<h3><a id="_89"></a>普通的生成接口</h3> 
<pre><code class="prism language-bash">POST /api/generate 
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment"># 参数说明</span>
model: <span class="token punctuation">(</span>required<span class="token punctuation">)</span> the model name
prompt: the prompt to generate a response <span class="token keyword">for</span>
images: <span class="token punctuation">(</span>optional<span class="token punctuation">)</span> a list of base64-encoded images <span class="token punctuation">(</span>for multimodal models such as llava<span class="token punctuation">)</span>
Advanced parameters <span class="token punctuation">(</span>optional<span class="token punctuation">)</span>:

format: the <span class="token function">format</span> to <span class="token builtin class-name">return</span> a response in. Currently the only accepted value is json
options: additional model parameters listed <span class="token keyword">in</span> the documentation <span class="token keyword">for</span> the Modelfile such as temperature
system: system message to <span class="token punctuation">(</span>overrides what is defined <span class="token keyword">in</span> the Modelfile<span class="token punctuation">)</span>
template: the prompt template to use <span class="token punctuation">(</span>overrides what is defined <span class="token keyword">in</span> the Modelfile<span class="token punctuation">)</span>
context: the context parameter returned from a previous request to /generate, this can be used to keep a short conversational memory
stream: <span class="token keyword">if</span> <span class="token boolean">false</span> the response will be returned as a single response object, rather than a stream of objects
raw: <span class="token keyword">if</span> <span class="token boolean">true</span> no formatting will be applied to the prompt. You may choose to use the raw parameter <span class="token keyword">if</span> you are specifying a full templated prompt <span class="token keyword">in</span> your request to the API
keep_alive: controls how long the model will stay loaded into memory following the request <span class="token punctuation">(</span>default: 5m<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>stream 参数较关键，用于控制返回的信息是否为流式</p> 
</blockquote> 
<pre><code class="prism language-bash"><span class="token comment"># 样例</span>
<span class="token function">curl</span> http://localhost:11434/api/generate <span class="token parameter variable">-d</span> <span class="token string">'{
  "model": "llama2",
  "prompt":"Why is the sky blue?"
}'</span>
</code></pre> 
<h3><a id="_123"></a>对话接口</h3> 
<pre><code class="prism language-bash">POST /api/chat
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment"># 参数说明</span>
model: <span class="token punctuation">(</span>required<span class="token punctuation">)</span> the model name
messages: the messages of the chat, this can be used to keep a chat memory
The message object has the following fields:

role: the role of the message, either system, user or assistant
content: the content of the message
images <span class="token punctuation">(</span>optional<span class="token punctuation">)</span>: a list of images to include <span class="token keyword">in</span> the message <span class="token punctuation">(</span>for multimodal models such as llava<span class="token punctuation">)</span>
Advanced parameters <span class="token punctuation">(</span>optional<span class="token punctuation">)</span>:

format: the <span class="token function">format</span> to <span class="token builtin class-name">return</span> a response in. Currently the only accepted value is json
options: additional model parameters listed <span class="token keyword">in</span> the documentation <span class="token keyword">for</span> the Modelfile such as temperature
template: the prompt template to use <span class="token punctuation">(</span>overrides what is defined <span class="token keyword">in</span> the Modelfile<span class="token punctuation">)</span>
stream: <span class="token keyword">if</span> <span class="token boolean">false</span> the response will be returned as a single response object, rather than a stream of objects
keep_alive: controls how long the model will stay loaded into memory following the request <span class="token punctuation">(</span>default: 5m<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>大部分参数与generate接口类似，只是把prompt参数改为messages</p> 
</blockquote> 
<pre><code class="prism language-bash"><span class="token comment"># 样例</span>
<span class="token function">curl</span> http://localhost:11434/api/chat <span class="token parameter variable">-d</span> <span class="token string">'{
  "model": "mistral",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'</span>
</code></pre> 
<h3><a id="_163"></a>带历史对话信息</h3> 
<pre><code class="prism language-bash"><span class="token function">curl</span> http://localhost:11434/api/chat <span class="token parameter variable">-d</span> <span class="token string">'{
  "model": "llama2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    },
    {
      "role": "assistant",
      "content": "due to rayleigh scattering."
    },
    {
      "role": "user",
      "content": "how is that different than mie scattering?"
    }
  ]
}'</span>
</code></pre> 
<h3><a id="_184"></a>图文对话</h3> 
<pre><code class="prism language-bash"><span class="token function">curl</span> http://localhost:11434/api/chat <span class="token parameter variable">-d</span> <span class="token string">'{
  "model": "llava",
  "messages": [
    {
      "role": "user",
      "content": "what is in this image?",
      "images": ["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"]
    }
  ]
}'</span>
</code></pre> 
<p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md">完整用法请看</a></p> 
<h2><a id="_202"></a>结语</h2> 
<p>目前该项目更新非常快，从路线图看后期还会支持更多模型和功能。相比于研究更强大的模型，减少技术的使用门槛对于普通人来说，显得更有实际意义。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8a4e3d52e53fea3de0fbc925a29783df/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据结构中的平衡搜索树 --- AVL树是怎样进行旋转处理的？（平衡因子版本）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e4c50a2921d0b81f8f15572e8c5f683f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Vits2.3-Extra-v2:中文特化，如何训练及推理（新手教程）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>