<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【自然语言处理六-最重要的模型-transformer-下】 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【自然语言处理六-最重要的模型-transformer-下】" />
<meta property="og:description" content="自然语言处理六-最重要的模型-transformer-下 transformer decoderMasked multi-head attentionencoder和decoder的连接部分-cross attentiondecoder的输出AT(Autoregresssive)NAT transformer decoder 今天接上一篇文章讲的encoder 自然语言处理六-最重要的模型-transformer-上，继续讲transformer的decoder，也就是下图中的红框部分
可以看出encoder和decoder部分去掉粉红色框的部分，结构几乎一样，下面分三部分介绍不同点
Masked multi-head attention decoder的注意力是masked的注意力，什么是masked的attention呢？ 下面是self attention:
需要注意的是：
selfattention中注意力bi的输出是需要关注所有的输入，也就是下面那一整排向量
但如果是masked self-attention，注意力是这样子的：
这个与普通的self attention的区别：
bi只能关注a0到ai的输入，不能包括ai&#43;1后的输入，那么为什么需要masked attention呢？
用下面的语音辨识，举个例子说明一下：
encoder是把一次性把所有的输入都输入到模型，计算注意力分数，但是对于decoder来说，它是一个字一个字产生：
比如decoder计算第一个位置应该输入什么的时候，它并不知道下一个的输入是“機”，所以必须遮蔽右边的输入，因此又叫masked self-attention。
decoder中下一次的输入是在本次输入BEGIN计算出来以后“機”这个字，作为下一次的输入。
需要说明的一点是：
实际上我们在训练的时候是知道每个输入的，因为这些信息是训练资料提供的，但真正测试使用的时候，是无法知晓的。
encoder和decoder的连接部分-cross attention 下面是encoder和decoder的互连部分：
相同的Add和Norm不再赘述，下面是attention部分，这个attention部分的输入分为3部分:
有两个箭头来自encoder的输出（这部分用作self attention中的k和v）
一个箭头来自decoder上一层的输出（这一部分用作q）
所以计算attention的流程是这样的：
左边这边encoder的输出，用于生成k v，右边decoder上一层的输出，用作q
按照普通的attention计算注意力分数后，最终生成v
然后进行add 残差连接和norm 归一化后，作为这一层的输出
然后继续输入到FC（feed forward netword）中
除了上面几部分不同，还需要关注的decoder如何处理输出。
decoder的输出 decoder输出的序列长度应该是多长呢？
比还是以语音辨识为例，输入一段语音究竟应该输出多少个字符根本无法确认，那么decoder究竟是怎么确定输出的长度的呢？有两种做法AT和NAT (AT是Autoregresssive的缩写)
AT(Autoregresssive) 这种做法就是让机器自己决定要输多少长度的sequence，当模型输出END的时候，就认为decoder输出完毕
NAT 这种情况下有几种方法确定decoder输出的长度：
1.添加一个网络来预测输出的长度
2.输入一排BEGIN向量，输出一排向量即可，最终的输出截止到输出为END
通常情况下，我们都是用AT，效果更好一些" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/150bb21c00fac13f352fb83b4d0e54f4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-04T14:23:29+08:00" />
<meta property="article:modified_time" content="2024-03-04T14:23:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【自然语言处理六-最重要的模型-transformer-下】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>自然语言处理六-最重要的模型-transformer-下</h4> 
 <ul><li><a href="#transformer_decoder_2" rel="nofollow">transformer decoder</a></li><li><ul><li><a href="#Masked_multihead_attention_9" rel="nofollow">Masked multi-head attention</a></li><li><a href="#encoderdecodercross_attention_29" rel="nofollow">encoder和decoder的连接部分-cross attention</a></li><li><a href="#decoder_46" rel="nofollow">decoder的输出</a></li><li><ul><li><a href="#ATAutoregresssive_50" rel="nofollow">AT(Autoregresssive)</a></li><li><a href="#NAT_53" rel="nofollow">NAT</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="transformer_decoder_2"></a>transformer decoder</h2> 
<p>今天接上一篇文章讲的encoder <a href="https://blog.csdn.net/zishuijing_dd/article/details/136416702?spm=1001.2014.3001.5501">自然语言处理六-最重要的模型-transformer-上</a>，继续讲transformer的decoder，也就是下图中的红框部分<br> <img src="https://images2.imgbox.com/d5/54/RVi9Rj8O_o.png" alt="在这里插入图片描述"></p> 
<p>可以看出encoder和decoder部分去掉粉红色框的部分，结构几乎一样，下面分三部分介绍不同点</p> 
<h3><a id="Masked_multihead_attention_9"></a>Masked multi-head attention</h3> 
<p>decoder的注意力是masked的注意力，什么是masked的attention呢？ 下面是self attention:</p> 
<p><img src="https://images2.imgbox.com/3a/24/Q2zO77Gn_o.png" alt="在这里插入图片描述"><br> 需要注意的是：<br> selfattention中注意力b<sup>i</sup>的输出是需要关注<strong>所有的输入</strong>，也就是下面那一整排向量</p> 
<p>但如果是masked self-attention，注意力是这样子的：<br> <img src="https://images2.imgbox.com/24/f4/gAnxMiiT_o.png" alt="在这里插入图片描述"><br> 这个与普通的self attention的<strong>区别</strong>：<br> b<sup>i</sup>只能关注a<sup>0</sup>到a<sup>i</sup>的输入，不能包括a<sup>i+1</sup>后的输入，那么为什么需要masked attention呢？<br> 用下面的语音辨识，举个例子说明一下：<br> <img src="https://images2.imgbox.com/7d/29/MwzOkP6Y_o.png" alt="在这里插入图片描述"></p> 
<p>encoder是把一次性把所有的输入都输入到模型，计算注意力分数，但是对于decoder来说，它是一个字一个字产生：<br> 比如decoder计算第一个位置应该输入什么的时候，它并不知道下一个的输入是“機”，所以必须遮蔽右边的输入，因此又叫masked self-attention。<br> decoder中下一次的输入是在本次输入BEGIN计算出来以后“機”这个字，作为下一次的输入。<br> 需要说明的一点是：<br> 实际上我们在训练的时候是知道每个输入的，因为这些信息是训练资料提供的，但真正测试使用的时候，是无法知晓的。</p> 
<h3><a id="encoderdecodercross_attention_29"></a>encoder和decoder的连接部分-cross attention</h3> 
<p>下面是encoder和decoder的互连部分：<br> <img src="https://images2.imgbox.com/3b/04/smF1tyvI_o.png" alt="在这里插入图片描述"><br> 相同的Add和Norm不再赘述，下面是attention部分，这个attention部分的输入分为3部分:<br> 有两个箭头来自encoder的输出（这部分用作self attention中的k和v）<br> 一个箭头来自decoder上一层的输出（这一部分用作q）</p> 
<p>所以计算attention的流程是这样的：</p> 
<p><img src="https://images2.imgbox.com/76/c4/MCHC1v2a_o.png" alt="在这里插入图片描述"><br> 左边这边encoder的输出，用于生成k v，右边decoder上一层的输出，用作q<br> 按照普通的attention计算注意力分数后，最终生成v<br> 然后进行add 残差连接和norm 归一化后，作为这一层的输出<br> 然后继续输入到FC（feed forward netword）中</p> 
<p>除了上面几部分不同，还需要关注的decoder如何处理输出。</p> 
<h3><a id="decoder_46"></a>decoder的输出</h3> 
<p>decoder输出的序列长度应该是多长呢？<br> 比还是以语音辨识为例，输入一段语音究竟应该输出多少个字符根本无法确认，那么decoder究竟是怎么确定输出的长度的呢？有两种做法AT和NAT (AT是Autoregresssive的缩写)</p> 
<h4><a id="ATAutoregresssive_50"></a>AT(Autoregresssive)</h4> 
<p>这种做法就是让机器自己决定要输多少长度的sequence，当模型输出END的时候，就认为decoder输出完毕</p> 
<h4><a id="NAT_53"></a>NAT</h4> 
<p><img src="https://images2.imgbox.com/c4/6f/Y1s8a6tF_o.png" alt="在这里插入图片描述"><br> 这种情况下有几种方法确定decoder输出的长度：<br> 1.添加一个网络来预测输出的长度<br> 2.输入一排BEGIN向量，输出一排向量即可，最终的输出截止到输出为END</p> 
<p>通常情况下，我们都是用AT，效果更好一些</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a5a3f0f287a448982aac520cffe4779a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Promise</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d5ad2a1ce5a0732d615c1775bd9a3713/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【自然语言处理六-最重要的模型-transformer-上】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>