<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CS143-project4基于滑窗的人脸检测 Face detection with a sliding window - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CS143-project4基于滑窗的人脸检测 Face detection with a sliding window" />
<meta property="og:description" content="利用一个窗口（模板）对图像进行滑动计算匹配。是目标检测中的传统方法，尤其是人脸检测。
1.hog和方法简介
2.获取积极(人脸)特征
3.获取随机消极(非人脸)特征
4.训练模型
5.检测验证
一.hog和方法简介
方向梯度直方图(Histogram of Oriented Gradient)，简称HOG 是用于目标检测的描述算子
将每个图像划分为若干个cells，计算每个cell中像素的梯度，然后统计每个cell中不同梯度的直方图，形成一个完整的hog特征算子
这里直接调用开源api:vl_feat的vl_hog算法，得到我们想要的hog算子
我们先对训练数据进行标记，分为两类：人脸(积极positive)和非人脸(消极negative),是人脸则标记为1，非人脸标记为-1
然后分别获取两类图像的hog特征算子
使用svm或其他算法对训练数据进行训练
使用训练好的模型对测试集进行验证
二.获取积极(人脸)特征 get_positive_features
project已经提供了一些头像的图片，只需要读入这些图像，并生成对应的hog特征
代码：
function features_pos = get_positive_features(train_path_pos, feature_params)
% &#39;train_path_pos&#39; is a string. This directory contains 36x36 images of
% faces
% &#39;feature_params&#39; is a struct, with fields
% feature_params.template_size (probably 36), the number of pixels %块的大小
% 36*36个像素，一个块=6个cell*6个像素 cell包含像素越小越好，但是性能变差
% spanned by each train / test template and
% feature_params." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/a187320da6b915639a119dd197acc6cc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-12-29T17:37:14+08:00" />
<meta property="article:modified_time" content="2016-12-29T17:37:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CS143-project4基于滑窗的人脸检测 Face detection with a sliding window</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>利用一个窗口（模板）对图像进行滑动计算匹配。是目标检测中的传统方法，尤其是人脸检测。</p> 
<p>1.hog和方法简介<br> 2.获取积极(人脸)特征<br> 3.获取随机消极(非人脸)特征<br> 4.训练模型<br> 5.检测验证</p> 
<p> </p> 
<p><strong>一.hog和方法简介<br> </strong>方向梯度直方图(Histogram of Oriented Gradient)，简称HOG 是用于目标检测的描述算子<br> 将每个图像划分为若干个cells，计算每个cell中像素的梯度，然后统计每个cell中不同梯度的直方图，形成一个完整的hog特征算子<br> 这里直接调用开源api:vl_feat的vl_hog算法，得到我们想要的hog算子<br> 我们先对训练数据进行标记，分为两类：人脸(积极positive)和非人脸(消极negative),是人脸则标记为1，非人脸标记为-1<br> 然后分别获取两类图像的hog特征算子<br> 使用svm或其他算法对训练数据进行训练<br> 使用训练好的模型对测试集进行验证</p> 
<p><br> <strong>二.获取积极(人脸)特征 get_positive_features<br> </strong>project已经提供了一些头像的图片，只需要读入这些图像，并生成对应的hog特征<br> 代码：<br> function features_pos = get_positive_features(train_path_pos, feature_params)</p> 
<p>% 'train_path_pos' is a string. This directory contains 36x36 images of<br> %   faces<br> % 'feature_params' is a struct, with fields<br> %   feature_params.template_size (probably 36), the number of pixels  %块的大小<br> %   36*36个像素，一个块=6个cell*6个像素 cell包含像素越小越好，但是性能变差<br> %      spanned by each train / test template and<br> %   feature_params.hog_cell_size (default 6), the number of pixels in each<br> %      HoG cell. template size should be evenly divisible by hog_cell_size.<br> %      Smaller HoG cell sizes tend to work better, but they make things<br> %      slower because the feature dimensionality increases and more<br> %      importantly the step size of the classifier decreases at test time.</p> 
<p><br> % 'features_pos' is N by D matrix where N is the number of faces and D<br> % is the template dimensionality, which would be<br> %   (feature_params.template_size / feature_params.hog_cell_size)^2 * 31<br> % if you're using the default vl_hog parameters</p> 
<p>% Useful functions:<br> % vl_hog, HOG = VL_HOG(IM, CELLSIZE)<br> %  <a target="_blank" href="http://www.vlfeat.org/matlab/vl_hog.html" rel="nofollow noopener noreferrer">http://www.vlfeat.org/matlab/vl_hog.html</a>  (API)<br> %  <a target="_blank" href="http://www.vlfeat.org/overview/hog.html" rel="nofollow noopener noreferrer">http://www.vlfeat.org/overview/hog.html</a>   (Tutorial)<br> % rgb2gray</p> 
<p>image_files = dir( fullfile( train_path_pos, '*.jpg') ); %Caltech Faces stored as .jpg<br> num_images = length(image_files);<br> features_pos = [];<br> for i = 1:num_images %循环每幅头像图像 <br>     im = image_files(i);<br>     image_path = fullfile(train_path_pos,im.name);<br>     image = imread(image_path); %读入头像<br>     if(size(image,3) &gt; 1)<br>         image = rgb2gray(image);<br>     end<br>     hog = vl_hog(single(image), feature_params.hog_cell_size); %获取hog算子<br> %     imhog = vl_hog('render', hog, 'verbose') ;<br> %     clf ; <br> %     imagesc(imhog) ; <br> %     colormap gray ;<br>     features_pos = cat(1,features_pos,reshape(hog,1,1116)); %拼接成N(头像数量)*D(6*6*31) N*1116的矩阵<br> end</p> 
<p><br> <strong>三.获取随机消极(非人脸)特征  get_positive_features<br> </strong>function features_pos = get_positive_features(train_path_pos, feature_params)<br> % 'train_path_pos' is a string. This directory contains 36x36 images of<br> %   faces<br> % 'feature_params' is a struct, with fields<br> %   feature_params.template_size (probably 36), the number of pixels  %块的大小<br> %   36*36个像素，一个块=6个cell*6个像素 cell包含像素越小越好，但是性能变差<br> %      spanned by each train / test template and<br> %   feature_params.hog_cell_size (default 6), the number of pixels in each<br> %      HoG cell. template size should be evenly divisible by hog_cell_size.<br> %      Smaller HoG cell sizes tend to work better, but they make things<br> %      slower because the feature dimensionality increases and more<br> %      importantly the step size of the classifier decreases at test time.<br>  <br>  <br> % 'features_pos' is N by D matrix where N is the number of faces and D<br> % is the template dimensionality, which would be<br> %   (feature_params.template_size / feature_params.hog_cell_size)^2 * 31<br> % if you're using the default vl_hog parameters<br>  <br> % Useful functions:<br> % vl_hog, HOG = VL_HOG(IM, CELLSIZE)<br> %  <a target="_blank" href="http://www.vlfeat.org/matlab/vl_hog.html" rel="nofollow noopener noreferrer">http://www.vlfeat.org/matlab/vl_hog.html</a>  (API)<br> %  <a target="_blank" href="http://www.vlfeat.org/overview/hog.html" rel="nofollow noopener noreferrer">http://www.vlfeat.org/overview/hog.html</a>   (Tutorial)<br> % rgb2gray<br>  <br> image_files = dir( fullfile( train_path_pos, '*.jpg') ); %Caltech Faces stored as .jpg<br> num_images = length(image_files);<br> features_pos = [];<br> for i = 1:num_images %循环每幅头像图像<br>     im = image_files(i);<br>     image_path = fullfile(train_path_pos,im.name);<br>     image = imread(image_path); %读入头像<br>     if(size(image,3) &gt; 1)<br>         image = rgb2gray(image);<br>     end<br>     hog = vl_hog(single(image), feature_params.hog_cell_size); %获取hog算子</p> 
<p>    features_pos = cat(1,features_pos,reshape(hog,1,1116)); %拼接成N(头像数量)*D(6*6*31) N*1116的矩阵<br> end</p> 
<p><strong>四.对模型进行训练</strong></p> 
<p>代码<br> %% step 2. Train Classifier<br> % Use vl_svmtrain on your training features to get a linear classifier<br> % specified by 'w' and 'b'<br> % [w b] = vl_svmtrain(X, Y, lambda) <br> % <a target="_blank" href="http://www.vlfeat.org/sandbox/matlab/vl_svmtrain.html" rel="nofollow noopener noreferrer"> http://www.vlfeat.org/sandbox/matlab/vl_svmtrain.html</a><br> % 'lambda' is an important parameter, try many values. Small values seem to<br> % work best e.g. 0.0001, but you can try other values<br>  <br> %YOU CODE classifier training. Make sure the outputs are 'w' and 'b'.<br> % category = categories{i}; %当前分类<br> % labels = double(strcmp(category, train_labels)); %生成label变量 属于此类别值为1，否则是-1<br> % labels(find(labels == 0)) = -1;<br> % [W B] = vl_svmtrain(train_image_feats', labels, 0.00006); %调用vl_feat库函数训练svm<br> if ~exist('w.mat', 'file')<br>     pos_size = size(features_pos,1);<br>     neg_size = size(features_neg,1);<br>     labels = zeros(pos_size+neg_size,1);<br>     labels(1:pos_size,1)=1;<br>     labels(pos_size+1:pos_size+neg_size,1)=-1;<br>     train_feats = cat(1,features_pos,features_neg);<br>     %[w b] = vl_svmtrain(train_feats, labels, 0.00006); %调用vl_feat库函数训练svm<br>     [w b] = vl_svmtrain(train_feats', labels', 0.00006); %每列一个样品<br>     save('w.mat', 'w');<br>     save('b.mat', 'b');<br> else<br>     load('w.mat');<br>     load('b.mat');<br> end</p> 
<p><strong>五.将训练好的模型用于测试</strong></p> 
<p>代码<br> function [bboxes, confidences, image_ids] = .... <br>     run_detector(test_scn_path, w, b, feature_params)<br> % 'test_scn_path' is a string. This directory contains images which may or<br> %    may not have faces in them. This function should work for the MIT+CMU<br> %    test set but also for any other images (e.g. class photos) 测试数据<br> % 'w' and 'b' are the linear classifier parameters<br> % 'feature_params' is a struct, with fields<br> %   feature_params.template_size (probably 36), the number of pixels<br> %      spanned by each train / test template and<br> %   feature_params.hog_cell_size (default 6), the number of pixels in each<br> %      HoG cell. template size should be evenly divisible by hog_cell_size.<br> %      Smaller HoG cell sizes tend to work better, but they make things<br> %      slower because the feature dimensionality increases and more<br> %      importantly the step size of the classifier decreases at test time.<br>  <br> % 'bboxes' is Nx4. N is the number of detections. bboxes(i,:) is 探测器<br> %   [x_min, y_min, x_max, y_max] for detection i. <br> %   Remember 'y' is dimension 1 in Matlab!<br> % 'confidences' is Nx1. confidences(i) is the real valued confidence of<br> %   detection i.<br> % 'image_ids' is an Nx1 cell array. image_ids{i} is the image file name<br> %   for detection i. (not the full path, just 'albert.jpg')<br>  <br> % The placeholder version of this code will return random bounding boxes in<br> % each test image. It will even do non-maximum suppression on the random<br> % bounding boxes to give you an example of how to call the function.<br>  <br> % Your actual code should convert each test image to HoG feature space with<br> % a _single_ call to vl_hog for each scale. Then step over the HoG cells,<br> % taking groups of cells that are the same size as your learned template,<br> % and classifying them. If the classification is above some confidence,<br> % keep the detection and then pass all the detections for an image to<br> % non-maximum suppression. For your initial debugging, you can operate only<br> % at a single scale and you can skip calling non-maximum suppression.<br>  <br> test_scenes = dir( fullfile( test_scn_path, '*.jpg' ));<br>  <br> %initialize these as empty and incrementally expand them.<br> bboxes = zeros(0,4);<br> confidences = zeros(0,1);<br> image_ids = cell(0,1);<br>  <br> for i = 1:length(test_scenes)<br>       <br>     fprintf('Detecting faces in %s\n', test_scenes(i).name)<br>     img = imread( fullfile( test_scn_path, test_scenes(i).name ));<br>     img = single(img)/255;<br>     if(size(img,3) &gt; 1)<br>         img = rgb2gray(img);<br>     end<br>     <br>     %%%%%%%%%%%%%%%%%%%%%%%%%%<br>     %增加多尺度地判断<br>     %多尺度参考<br>     %deciding downsample parameters.<br>     edgesize = min(size(img,2),size(img,1));%最短的边<br>     maxdownsize = log(feature_params.template_size/edgesize)/log(0.9);%可以降多少次采样？<br>     cur_bboxes = [];<br>     cur_confidences = [];<br>     cur_image_ids = [];<br>     for downsize = 0:1:maxdownsize %min(maxdownsize,0) %0开始 step为1 <br>         downsample_scale = realpow(0.9,downsize); %0.7 的downsize次方<br>         timg = imresize(img,downsample_scale);%降采样<br>         %使用hog来表示image，如何滑动窗口（模板）检测confidences<br>         %hogs = vl_hog(img,feature_params.hog_cell_size);<br>         hogs = vl_hog(timg,feature_params.hog_cell_size);<br>         max_j = size(hogs,1) - 5;<br>         max_k = size(hogs,2) - 5;<br>         %blocks=min(floor(size(hogs,1)/6),floor(size(hogs,2)/6));<br>         <br>         for j = 1:max_j<br>             for k = 1:max_k<br>                 cur_hog=hogs(j:j+5,k:k+5,:);<br>                 cur_hog= reshape(cur_hog,1,1116);<br>                 confidence = cur_hog*w + b;<br>                 if confidence &gt; 0.5 %计算出来如果是大于阀值（是脸）<br>                     x_min = k*6/downsample_scale; %记录窗口的位置<br>                     y_min = j*6/downsample_scale;<br>                     x_max = (k+5)*6/downsample_scale;<br>                     y_max = (j+5)*6/downsample_scale;<br>                     cur_bboxes = cat(1,cur_bboxes,[x_min, y_min, x_max, y_max]);<br>                     cur_confidences = cat(1,cur_confidences,confidence);<br>                     cur_image_ids = cat(1,cur_image_ids,{test_scenes(i).name});<br>                 end<br>             end<br>         end<br>     end<br>  <br>  <br>  <br>     if size(cur_bboxes,1)==0<br>         continue;<br>     end<br>     <br>     %%%%%%%%%%%%%%%%%%%%%%%%%%<br>     %You can delete all of this below.<br>     % Let's create 15 random detections per image<br>     <br> %     cur_x_min = rand(15,1) * size(img,2);<br> %     cur_y_min = rand(15,1) * size(img,1);<br> %     cur_bboxes = [cur_x_min, cur_y_min, cur_x_min + rand(15,1) * 50, cur_y_min + rand(15,1) * 50];<br> %     cur_confidences = rand(15,1) * 4 - 2; %confidences in the range [-2 2]<br> %     cur_image_ids(1:15,1) = {test_scenes(i).name};<br>     <br>     %non_max_supr_bbox can actually get somewhat slow with thousands of<br>     %initial detections. You could pre-filter the detections by confidence,<br>     %e.g. a detection with confidence -1.1 will probably never be<br>     %meaningful. You probably _don't_ want to threshold at 0.0, though. You<br>     %can get higher recall with a lower threshold. You don't need to modify<br>     %anything in non_max_supr_bbox, but you can.<br>     [is_maximum] = non_max_supr_bbox(cur_bboxes, cur_confidences, size(img));<br>  <br>     cur_confidences = cur_confidences(is_maximum,:);<br>     cur_bboxes      = cur_bboxes(     is_maximum,:);<br>     cur_image_ids   = cur_image_ids(  is_maximum,:);<br>  <br>     bboxes      = [bboxes;      cur_bboxes];<br>     confidences = [confidences; cur_confidences];<br>     image_ids   = [image_ids;   cur_image_ids];<br>     <br> end<br> </p> 
<p>效果：</p> 
<p><img src="https://images2.imgbox.com/70/45/lnJvbnDJ_o.png" width="666" height="413" alt=""></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a7696caca2918f22a0e6f29c27fefa04/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux设备驱动五 (1)总线、设备和驱动</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1c78c82a565f2273bb85dd56e67e4375/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Android仿打字机打字效果</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>