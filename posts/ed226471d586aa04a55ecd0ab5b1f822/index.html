<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>r 语言初学者指南_阻止自然语言处理的初学者指南 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="r 语言初学者指南_阻止自然语言处理的初学者指南" />
<meta property="og:description" content="r 语言初学者指南
My job focuses almost exclusively on NLP. So I work with text. A lot.
我的工作几乎完全专注于NLP 。 所以我处理文本。 很多。
Text ML comes with its own challenges and tools. But one recurring problem is excessive dimensionality.
Text ML带有自己的挑战和工具。 但是一个反复出现的问题是尺寸过大。
In real life (not Kaggle), we often have a limited number of annotated examples on which to train models. And each example often contains a large amount of text.
在现实生活中(不是Kaggle)，我们经常会在有限的带注释的示例中训练模型。 每个示例通常包含大量文本。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/ed226471d586aa04a55ecd0ab5b1f822/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-11T00:01:12+08:00" />
<meta property="article:modified_time" content="2020-10-11T00:01:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">r 语言初学者指南_阻止自然语言处理的初学者指南</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <article style="font-size: 16px;"> 
 <p>r 语言初学者指南</p> 
 <div> 
  <section> 
   <div> 
    <div> 
     <p>My job focuses almost exclusively on <a href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener nofollow">NLP</a>. So I work with text. A lot.</p> 
     <p>我的工作几乎完全专注于<a href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener nofollow">NLP</a> 。 所以我处理文本。 很多。</p> 
     <p>Text ML comes with its own challenges and tools. But one recurring problem is excessive dimensionality.</p> 
     <p> Text ML带有自己的挑战和工具。 但是一个反复出现的问题是尺寸过大。</p> 
     <p>In real life (not Kaggle), we often have a limited number of annotated examples on which to train models. And each example often contains a large amount of text.</p> 
     <p> 在现实生活中(不是Kaggle)，我们经常会在有限的带注释的示例中训练模型。 每个示例通常包含大量文本。</p> 
     <p>This sparsity, and lack of common features, make it difficult to find patterns in data. We call this the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener nofollow">curse of dimensionality</a>.</p> 
     <p> 这种稀疏性以及缺乏通用功能，使得很难在数据中找到模式。 我们称其<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener nofollow">为维数</a>的<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener nofollow">诅咒</a>。</p> 
     <p>One technique for reducing dimensionality is <strong>stemming</strong>.</p> 
     <p> 降低尺寸的一种技术是<strong>阻止</strong>。</p> 
     <p>It removes the suffix from a word to get it’s “stem”.</p> 
     <p> 它从单词中删除后缀以得到其“词干”。</p> 
     <p>So <code>fisher</code> becomes <code>fish</code>.</p> 
     <p> 所以<code>fisher</code>变成了<code>fish</code> 。</p> 
     <p>Let’s learn how stemming works, why to use it, and how to use it.</p> 
     <p> 让我们学习词干的工作原理，为什么要使用它以及如何使用它。</p> 
    </div> 
   </div> 
  </section> 
  <section> 
   <div> 
    <div> 
     <h2> 干什么？ <span style="font-weight: bold;">(</span>What is stemming?<span style="font-weight: bold;">)</span></h2> 
     <h3> 后缀 <span style="font-weight: bold;">(</span>Suffixes<span style="font-weight: bold;">)</span></h3> 
     <p>Stemming reduces words to their stem (root) by removing suffixes.</p> 
     <p>词干通过删除后缀来减少词干(词根)。</p> 
     <p><strong>Suffixes</strong> are word endings which modify a word’s meaning. These include but are not limited to...</p> 
     <p> <strong>后缀</strong>是修饰词义的词尾。 这些包括但不限于...</p> 
     <pre><code class="has">-able<br>-ation<br>-ed<br>-er<br>-est<br>-iest<br>-ese<br>-ily<br>-ful<br>-ing<br>...</code></pre> 
     <p>Obviously the words <code>runner</code> and <code>running</code> having different meanings.</p> 
     <p> 显然，“ <code>runner</code>和“ <code>running</code>这两个词具有不同的含义。</p> 
     <p>But when it comes to text classification tasks, both words imply that a sentence is talking about something similar.</p> 
     <p> 但是当涉及到文本分类任务时，两个词都暗示一个句子在谈论类似的东西。</p> 
     <h3> 算法 <span style="font-weight: bold;">(</span>The Algorithm<span style="font-weight: bold;">)</span></h3> 
     <p>How do stemming algorithms work?</p> 
     <p>词干算法如何工作？</p> 
     <p>In a nutshell, algorithms contain a list of rules for removing suffixes, which are conditionally applied to input tokens.</p> 
     <p> 简而言之，算法包含用于删除后缀的规则列表，这些条件有条件地应用于输入令牌。</p> 
     <p>There’s nothing statistical going on here, so algorithms are pretty basic. That makes stemming fast to run on large bodies of text.</p> 
     <p> 这里没有任何统计信息，因此算法是非常基本的。 这使得词干快速可以在大量文本上运行。</p> 
     <p>If you’re interested, NLTK’s Porter stemming algorithm is freely available to dig into, <a href="https://www.nltk.org/_modules/nltk/stem/porter.html" rel="noopener nofollow">here</a>, and the paper explaining it, <a href="https://tartarus.org/martin/PorterStemmer/def.txt" rel="noopener nofollow">here</a>.</p> 
     <p> 如果您有兴趣，可以在<a href="https://www.nltk.org/_modules/nltk/stem/porter.html" rel="noopener nofollow">此处</a>自由地下载NLTK的Porter词干算法，并在<a href="https://tartarus.org/martin/PorterStemmer/def.txt" rel="noopener nofollow">此处进行</a>解释。</p> 
     <p>That said, there are different stemming algorithms. NLTK alone contains Porter, Snowball, and Lancaster. Each being more aggressive in stemming than the last.</p> 
     <p> 也就是说，存在不同的词干算法。 仅NLTK就包含Porter，Snowball和Lancaster。 每个都比最后一个更具侵略性。</p> 
    </div> 
   </div> 
  </section> 
  <section> 
   <div> 
    <div> 
     <h2> 阻止文字的原因 <span style="font-weight: bold;">(</span>Reasons for stemming text<span style="font-weight: bold;">)</span></h2> 
     <h3>语境<span style="font-weight: bold;">(</span>Context<span style="font-weight: bold;">)</span></h3> 
     <p>A large part of NLP is figuring out what a body of text is talking about.</p> 
     <p>NLP的很大一部分正在弄清楚一段文本在谈论什么。</p> 
     <p>While not always true, a sentence containing the word, <code>planting</code>, is often talking about something similar to another sentence containing the word, <code>plant</code>.</p> 
     <p> 虽然并不总是正确的，但包含<code>planting</code>一词的句子经常在谈论与包含<code>plant</code>一词的另一句话相似的事物。</p> 
     <p>Giving this, why not reduce all words to their stems before training a classification model on them.</p> 
     <p> 鉴于此，在训练分类模型之前，为什么不减少所有词的词干。</p> 
     <p>There’s also another benefit.</p> 
     <p> 还有另一个好处。</p> 
     <h3> 维数 <span style="font-weight: bold;">(</span>Dimensionality<span style="font-weight: bold;">)</span></h3> 
     <p>Large bodies of text contain huge numbers of different words. Combined with a limited number of training examples, sparsity makes it hard for models to find patterns and do accurate classifications.</p> 
     <p>大文本正文包含大量不同的单词。 结合有限数量的训练示例，稀疏性使模型很难找到模式并进行准确的分类。</p> 
     <p>Reducing words to their stem decreases sparsity and makes it easier to find patterns and make predictions.</p> 
     <p> 将词减少到词干会减少稀疏性，使查找样式和进行预测变得更加容易。</p> 
     <p>Stemming allows each string of text to be represented in a smaller bag of words.</p> 
     <p> 词干允许以较小的单词袋表示每个文本字符串。</p> 
     <p><strong>Example: </strong>After stemming, the sentence, <code>"the fishermen fished for fish"</code>, can be represented in a bag of words like this.</p> 
     <p> <strong>示例：</strong>词干之后，可以在这样的一袋单词中表示<code>"the fishermen fished for fish"</code>的句子。</p> 
     <pre><code class="has">[the, fisherman, fish, for]</code></pre> 
     <p>Instead of.</p> 
     <p> 代替。</p> 
     <pre><code class="has">[the, fisherman, fished, for, fish]</code></pre> 
     <p>This both decreases sparsity across examples, and increases training algorithm speed.</p> 
     <p> 这既降低了示例的稀疏性，又提高了训练算法的速度。</p> 
     <p><strong>In my experience</strong>, very little signal is lost via stemming, compared to say using a word embeddings.</p> 
     <p> <strong>根据我的经验</strong>，与使用词嵌入相比，通过词干丢失的信号很少。</p> 
    </div> 
   </div> 
  </section> 
  <section> 
   <div> 
    <div> 
     <h2> 一个Python例子 <span style="font-weight: bold;">(</span>A Python Example<span style="font-weight: bold;">)</span></h2> 
     <p>We’ll start with a string of words, each with the stem, “fish”.</p> 
     <p>我们将从一串单词开始，每个单词都带有词干“鱼”。</p> 
     <pre><code class="has">'fish fishing fishes fisher fished fishy'</code></pre> 
     <p>And then stem the tokens in that string.</p> 
     <p> 然后阻止该字符串中的标记。</p> 
     <pre><code class="has">from nltk.tokenize import word_tokenize<br>from nltk.stem.porter import PorterStemmer<br>text = 'fish fishing fishes fisher fished fishy'<br># Tokenize the string<br>tokens = word_tokenize(text)<br>print(tokens) <br>#=&gt; ['fish', 'fishing', 'fishes', 'fisher', 'fished', 'fishy']<br>stemmer = PorterStemmer()<br>stems = [stemmer.stem(w) for w in tokens]<br>print(stems)<br>#=&gt; ['fish', 'fish', 'fish', 'fisher', 'fish', 'fishi']</code></pre> 
     <p>The result is.</p> 
     <p> 结果是。</p> 
     <pre><code class="has">['fish', 'fish', 'fish', 'fisher', 'fish', 'fishi']</code></pre> 
     <p>Note that it didn’t reduce all tokens to the stem “fish”. Porter is one of the most conservative stemming algorithms. That said, this is still an improvement.</p> 
     <p> 请注意，它并没有减少词干“鱼”的所有标记。 波特是最保守的词干算法之一。 也就是说，这仍然是一种改进。</p> 
    </div> 
   </div> 
  </section> 
  <section> 
   <div> 
    <div> 
     <h2> 结论 <span style="font-weight: bold;">(</span>Conclusion<span style="font-weight: bold;">)</span></h2> 
     <p>Anecdotally, preprocessing is the most important (and neglected) part of the NLP pipeline.</p> 
     <p>有趣的是，预处理是NLP管道中最重要(也是被忽略)的部分。</p> 
     <p>It determines the shape of data that are eventually fed to ML models, and the difference between feeding a model quality data, and garbage.</p> 
     <p> 它确定最终馈送给ML模型的数据的形状，以及馈送模型质量数据和垃圾之间的区别。</p> 
     <p>Garbage in, garbage out — as they say.</p> 
     <p> 就像他们说的那样，垃圾进，垃圾出。</p> 
     <p>After down-casing, and removing punctuation and stopwords, stemming is a key component of most NLP pipelines.</p> 
     <p> 精简并删除标点符号和停用词后，阻止是大多数NLP管道的关键组成部分。</p> 
     <p>Stemming (and it’s cousin, <a href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener nofollow">lemmatization</a>), will give you better results, on less data, and decrease model training time.</p> 
     <p> 词干(表亲，<a href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener nofollow">词条还原</a>)将在更少的数据上提供更好的结果，并减少模型训练时间。</p> 
    </div> 
   </div> 
  </section> 
 </div> 
 <blockquote> 
  <p>翻译自: <a href="https://towardsdatascience.com/a-beginners-guide-to-stemming-in-natural-language-processing-34ddee4acd37" rel="nofollow">https://towardsdatascience.com/a-beginners-guide-to-stemming-in-natural-language-processing-34ddee4acd37</a></p> 
 </blockquote> 
 <p>r 语言初学者指南</p> 
</article>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b4ccbf4f0afb6c94113f1019c8f9d9cd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">em算法和gmm算法_ml gmm em算法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e62d1127ad68fa838c71017207e6eb4c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">但是什么是模型</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>