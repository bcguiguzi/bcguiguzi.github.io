<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>笔记｜李沐-动手学习机器学习｜CNN基础知识（视频19-23） - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="笔记｜李沐-动手学习机器学习｜CNN基础知识（视频19-23）" />
<meta property="og:description" content="李沐-动手学习机器学习｜CNN基础知识 卷积层（视频19）从全连接到卷积（卷积算子）进行图像识别的两个原则如何从全连接层出发，应用以上两个原则，得到卷积 卷积层二维交叉相关二维卷积层交叉相关v.s. 卷积一维、三维交叉相关 卷积层里的「填充」和「步幅」（对应视频20）填充步幅代码 卷积层里的多输入多输出通道（视频21）多个输入通道多个输出通道多个输入输出通道1*1 卷积层二维卷积层代码 池化层（视频22）二维最大池化层平均池化层代码 经典卷积神经网络LeNet（视频23）LetNet实现学习如何手动检查模型 李沐b站课程视频： 【完结】动手学深度学习 PyTorch版 卷积层（视频19） 从全连接到卷积（卷积算子） 进行图像识别的两个原则 平移不变性（不管目标物体在图像的哪个位置都能识别出来）局部性（要识别目标物体无需遍历很广部分就能识别） 如何从全连接层出发，应用以上两个原则，得到卷积 （回顾）全连接层时，虽然图像有高、宽，但是我们会将其转变成一个1维向量。即，输入和输出均为一维向量。
（现在）将图像还原成矩阵，因为需要考虑一些空间的信息。
将输入输出“变形”为矩阵（高度，宽度）将权重对应为4D张量（（h，w）-&gt;（h‘，w’），2*2 = 4）对w进行重新索引（方便后面看清实现卷积的过程） 那么，输出的h可以写为（对应x的下标也要进行变换）：
原则 #1 平移不变性
位置变换即i，j的变化。要满足原则1，需要v的值与i，j无关，即：
这就是「二维交叉相关」（容易错误说成“二维卷积”，但在数学上来说，这不叫卷积，而是交叉相关）
原则 #2 局部性
经过原则1，我们得到了
其中可以理解为以（i，j）为中心，需要遍历以外的所有a，b
但若要满足原则2，应该只需要遍历（i，j）附近的点（不应用到远离x_i,j 的参数），即：
总结：对全连接层应用「平移不变性」和「不变性」得到卷积层（所以也说卷积层是一种特殊的全连接层）
卷积层 二维交叉相关 kernel就是w
二维卷积层 一个神经网络可以去学习一些不同的kernel，来得到一些不同的图像处理效果
交叉相关v.s. 卷积 唯一的区别是卷积多了个负号。但在实际应用中，由于对称性两者没有区别。而且实际常用的是交叉相关而非卷积（虽然都会叫做卷积）
一维、三维交叉相关 常写作Cov2D、Cov3D…最常用的是Cov2D
总结
卷积层：将核矩阵和输入进行交叉相关，再加上偏差；
可学习的参数：核矩阵、偏移
超参数：矩阵的大小
卷积层里的「填充」和「步幅」（对应视频20） 两个控制输出大小的超参数
填充 对于给定的图像，卷积核越大，输入变小地越快。
如果不希望变小地这么快，就可以用到「填充」
填充：在输入周围加入额外的行和列
这样子可以使得输入和输出的维度不变
步幅 当图片特别大，卷积核不太大，需要大量的计算才能得到较小的输出。这时就可以调整「步幅」。
步幅：是指行/列的滑动步长
总结
填充和步幅是卷积层的超参数；
填充在输入周围加入额外的行和列，控制输出形状的减少；
步幅是每次窗口核滑动的行/列步长，来成倍的减小输出形状。
代码 设定padding、stride这两个参数
在下面的例子中，我们创建一个高度和宽度为3的二维卷积层，并(在所有侧边填充1个像素)。给定高度和宽度为8的输入，则输出的高度和宽度也是8。
1）准备
import torch from torch import nn 2）定义函数comp_conv2d" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/53ccfdbc79f8ea9e9285874d8b6ef66c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-09T16:36:16+08:00" />
<meta property="article:modified_time" content="2022-10-09T16:36:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">笔记｜李沐-动手学习机器学习｜CNN基础知识（视频19-23）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>李沐-动手学习机器学习｜CNN基础知识</h4> 
 <ul><li><a href="#19_2" rel="nofollow">卷积层（视频19）</a></li><li><ul><li><a href="#_3" rel="nofollow">从全连接到卷积（卷积算子）</a></li><li><ul><li><a href="#_4" rel="nofollow">进行图像识别的两个原则</a></li><li><a href="#_9" rel="nofollow">如何从全连接层出发，应用以上两个原则，得到卷积</a></li></ul> 
   </li><li><a href="#_38" rel="nofollow">卷积层</a></li><li><ul><li><a href="#_39" rel="nofollow">二维交叉相关</a></li><li><a href="#_43" rel="nofollow">二维卷积层</a></li><li><a href="#vs__49" rel="nofollow">交叉相关v.s. 卷积</a></li><li><a href="#_55" rel="nofollow">一维、三维交叉相关</a></li></ul> 
  </li></ul> 
  </li><li><a href="#20_67" rel="nofollow">卷积层里的「填充」和「步幅」（对应视频20）</a></li><li><ul><li><a href="#_71" rel="nofollow">填充</a></li><li><a href="#_80" rel="nofollow">步幅</a></li><li><a href="#_90" rel="nofollow">代码</a></li></ul> 
  </li><li><a href="#21_126" rel="nofollow">卷积层里的多输入多输出通道（视频21）</a></li><li><ul><li><a href="#_128" rel="nofollow">多个输入通道</a></li><li><a href="#_131" rel="nofollow">多个输出通道</a></li><li><a href="#_134" rel="nofollow">多个输入输出通道</a></li><li><a href="#11__139" rel="nofollow">1*1 卷积层</a></li><li><a href="#_144" rel="nofollow">二维卷积层</a></li><li><a href="#_151" rel="nofollow">代码</a></li></ul> 
  </li><li><a href="#22_181" rel="nofollow">池化层（视频22）</a></li><li><ul><li><a href="#_183" rel="nofollow">二维最大池化层</a></li><li><a href="#_191" rel="nofollow">平均池化层</a></li><li><a href="#_200" rel="nofollow">代码</a></li></ul> 
  </li><li><a href="#LeNet23_219" rel="nofollow">经典卷积神经网络LeNet（视频23）</a></li><li><ul><li><ul><li><a href="#LetNet_227" rel="nofollow">LetNet实现</a></li><li><a href="#_247" rel="nofollow">学习如何手动检查模型</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<br> 李沐b站课程视频： 
<a href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497" rel="nofollow">【完结】动手学深度学习 PyTorch版</a> 
<p></p> 
<h2><a id="19_2"></a>卷积层（视频19）</h2> 
<h3><a id="_3"></a>从全连接到卷积（卷积算子）</h3> 
<h4><a id="_4"></a>进行图像识别的两个原则</h4> 
<ul><li><strong>平移不变性</strong>（不管目标物体在图像的哪个位置都能识别出来）</li><li><strong>局部性</strong>（要识别目标物体无需遍历很广部分就能识别）</li></ul> 
<h4><a id="_9"></a>如何从全连接层出发，应用以上两个原则，得到卷积</h4> 
<p>（回顾）全连接层时，虽然图像有高、宽，但是我们会将其转变成一个1维向量。即，输入和输出均为一维向量。<br> （现在）将图像还原成矩阵，因为需要考虑一些空间的信息。</p> 
<ul><li>将输入输出“变形”为矩阵（高度，宽度）</li><li>将权重对应为4D张量（（h，w）-&gt;（h‘，w’），2*2 = 4）</li><li>对w进行重新索引（方便后面看清实现卷积的过程）</li></ul> 
<p><img src="https://images2.imgbox.com/09/6f/BAjPdSh4_o.png" alt="在这里插入图片描述" width="360"><br> 那么，输出的h可以写为（对应x的下标也要进行变换）：<br> <img src="https://images2.imgbox.com/dd/be/48d5J4nb_o.png" alt="在这里插入图片描述"></p> 
<p><strong>原则 #1 平移不变性</strong><br> 位置变换即i，j的变化。要满足原则1，需要v的值与i，j无关，即：<br> <img src="https://images2.imgbox.com/82/61/dEGGSpex_o.png" alt="在这里插入图片描述" width="200"><br> 这就是「二维交叉相关」（容易错误说成“二维卷积”，但在数学上来说，这不叫卷积，而是交叉相关）</p> 
<p><strong>原则 #2 局部性</strong><br> 经过原则1，我们得到了<img src="https://images2.imgbox.com/c8/b4/O4OzBSLO_o.png" alt="在这里插入图片描述"><br> 其中可以理解为以（i，j）为中心，需要遍历以外的所有a，b<br> 但若要满足原则2，应该只需要遍历（i，j）附近的点（不应用到远离x_i,j 的参数），即：<br> <img src="https://images2.imgbox.com/ce/0d/M6f00Xuy_o.png" alt="在这里插入图片描述" width="320"></p> 
<p><img src="https://images2.imgbox.com/6f/3b/gtO2mITB_o.png" alt="在这里插入图片描述" width="400"></p> 
<p>总结：对全连接层应用「平移不变性」和「不变性」得到卷积层（所以也说卷积层是一种特殊的全连接层）<br> <img src="https://images2.imgbox.com/47/a0/RSI1UpHT_o.png" alt="在这里插入图片描述" width="400"></p> 
<h3><a id="_38"></a>卷积层</h3> 
<h4><a id="_39"></a>二维交叉相关</h4> 
<p><img src="https://images2.imgbox.com/14/d1/NRZXgOql_o.png" alt="在这里插入图片描述"></p> 
<p>kernel就是w</p> 
<h4><a id="_43"></a>二维卷积层</h4> 
<p><img src="https://images2.imgbox.com/0b/9d/ngtfON4l_o.png" alt="在这里插入图片描述"></p> 
<p><strong>一个神经网络可以去学习一些不同的kernel，来得到一些不同的图像处理效果</strong><br> <img src="https://images2.imgbox.com/b1/18/OxhjME4H_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="vs__49"></a>交叉相关v.s. 卷积</h4> 
<p>唯一的区别是卷积多了个负号。但在实际应用中，由于对称性两者没有区别。而且实际常用的是交叉相关而非卷积（虽然都会叫做卷积）</p> 
<p><img src="https://images2.imgbox.com/4c/d2/TptQwCSI_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_55"></a>一维、三维交叉相关</h4> 
<p><img src="https://images2.imgbox.com/bd/29/ljbHlIGU_o.png" alt="在这里插入图片描述"><br> 常写作Cov2D、Cov3D…最常用的是Cov2D</p> 
<p><strong>总结<br> 卷积层：将核矩阵和输入进行交叉相关，再加上偏差；<br> 可学习的参数：核矩阵、偏移<br> 超参数：矩阵的大小</strong></p> 
<h2><a id="20_67"></a>卷积层里的「填充」和「步幅」（对应视频20）</h2> 
<p>两个控制输出大小的<strong>超参数</strong></p> 
<h3><a id="_71"></a>填充</h3> 
<p>对于给定的图像，卷积核越大，输入变小地越快。<br> 如果不希望变小地这么快，就可以用到「填充」<br> <strong>填充</strong>：在输入周围加入额外的行和列<br> <img src="https://images2.imgbox.com/1e/87/BRkXlZ6G_o.png" alt="在这里插入图片描述"><br> 这样子可以使得输入和输出的维度不变</p> 
<h3><a id="_80"></a>步幅</h3> 
<p>当图片特别大，卷积核不太大，需要大量的计算才能得到较小的输出。这时就可以调整「步幅」。<br> <strong>步幅</strong>：是指行/列的滑动步长<br> <img src="https://images2.imgbox.com/8e/96/WNt5wO6f_o.png" alt="在这里插入图片描述"></p> 
<p><strong>总结<br> 填充和步幅是卷积层的超参数；<br> 填充在输入周围加入额外的行和列，控制输出形状的减少；<br> 步幅是每次窗口核滑动的行/列步长，来成倍的减小输出形状。</strong></p> 
<h3><a id="_90"></a>代码</h3> 
<p>设定<code>padding</code>、<code>stride</code>这两个参数<br> 在下面的例子中，我们创建一个高度和宽度为3的二维卷积层，并(在所有侧边填充1个像素)。给定高度和宽度为8的输入，则输出的高度和宽度也是8。<br> 1）准备</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
</code></pre> 
<p>2）定义函数comp_conv2d</p> 
<pre><code class="prism language-python"><span class="token comment"># 为了方便起见，我们定义了一个计算卷积层的函数。</span>
<span class="token comment"># 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数</span>
<span class="token keyword">def</span> <span class="token function">comp_conv2d</span><span class="token punctuation">(</span>conv2d<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 这里的（1，1）表示批量大小和通道数都是1</span>
    X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    Y <span class="token operator">=</span> conv2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token comment"># 省略前两个维度：批量大小和通道</span>
    <span class="token keyword">return</span> Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># return Y.reshape(Y.shape)</span>
</code></pre> 
<p>3）填充<br> 对称填充，padding=1<br> <img src="https://images2.imgbox.com/6d/11/roGXZlx3_o.png" alt="在这里插入图片描述"></p> 
<p>填充不同的高度和宽度，padding=（2，1）<br> <img src="https://images2.imgbox.com/32/4b/gAqZ1tFb_o.png" alt="在这里插入图片描述"></p> 
<p>4）步幅<br> 行列一致<br> <img src="https://images2.imgbox.com/bc/0a/85AK63XI_o.png" alt="在这里插入图片描述"><br> 行列不一致<br> <img src="https://images2.imgbox.com/5f/e9/YaFn7VTY_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="21_126"></a>卷积层里的多输入多输出通道（视频21）</h2> 
<p>通常要仔细设置的参数</p> 
<h3><a id="_128"></a>多个输入通道</h3> 
<p><img src="https://images2.imgbox.com/b6/1f/vf5fegpd_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_131"></a>多个输出通道</h3> 
<p><img src="https://images2.imgbox.com/6a/31/yaVwNl5P_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_134"></a>多个输入输出通道</h3> 
<ul><li>每个输出可以识别特定的模式</li><li>输入通道核 识别 并 组合输入中的模式</li></ul> 
<h3><a id="11__139"></a>1*1 卷积层</h3> 
<p>不会识别空间信息，只是<strong>融合通道</strong>。也可以看成是全连接层。<br> <img src="https://images2.imgbox.com/04/77/SVltB6tY_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_144"></a>二维卷积层</h3> 
<p><img src="https://images2.imgbox.com/af/db/DcV5fvUV_o.png" alt="在这里插入图片描述"></p> 
<p>输出的m_h, m_w由输入以及padding、stride决定</p> 
<p><img src="https://images2.imgbox.com/92/08/tlDap6fe_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_151"></a>代码</h3> 
<p>1）准备</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">corr2d</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> K<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""计算二维互相关运算"""</span>
    h<span class="token punctuation">,</span> w <span class="token operator">=</span> K<span class="token punctuation">.</span>shape
    Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> h <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> w <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 这里是x的局部和kernel进行「点积」，即对应位置元素相乘，而非矩阵乘法！！</span>
            Y<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>X<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i <span class="token operator">+</span> h<span class="token punctuation">,</span> j<span class="token punctuation">:</span>j <span class="token operator">+</span> w<span class="token punctuation">]</span> <span class="token operator">*</span> K<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> Y
</code></pre> 
<p>2）多输入通道互相关运算</p> 
<p><img src="https://images2.imgbox.com/c0/95/gUInJalv_o.png" alt="在这里插入图片描述"></p> 
<p>3）多输出通道互相关运算<br> <img src="https://images2.imgbox.com/a1/ff/nElG8tqZ_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="22_181"></a>池化层（视频22）</h2> 
<p>（回顾）卷积有些问题，对位置非常敏感，比如稍微的抖动可能会影响很大。</p> 
<h3><a id="_183"></a>二维最大池化层</h3> 
<p><img src="https://images2.imgbox.com/19/01/ZaubWeHS_o.png" alt="在这里插入图片描述"></p> 
<ul><li>参数和卷积层很类似，都具有填充和步幅</li><li>没有可学习的参数</li><li>在每个输入通道应用池化层以得到相应的输出通道（没有通道融合）</li><li>输出通道数=输入通道数</li></ul> 
<h3><a id="_191"></a>平均池化层</h3> 
<p><img src="https://images2.imgbox.com/1e/ee/j8bjdVm5_o.png" alt="在这里插入图片描述"><br> 左图比较亮的部分是信号强的部分，明显右图就柔和很多。</p> 
<p>总结<br> 池化层返回窗口最大值/平均值；<br> 主要用于缓解卷积层的位置敏感性。池化层通常接在卷积层后面；<br> 池化层和卷积层一样，具有窗口大小、填充、步幅作为超参数。</p> 
<h3><a id="_200"></a>代码</h3> 
<p>实现池化层的前向传播</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

<span class="token keyword">def</span> <span class="token function">pool2d</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> pool_size<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'max'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    p_h<span class="token punctuation">,</span> p_w <span class="token operator">=</span> pool_size
    Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> p_h <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> p_w <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">'max'</span><span class="token punctuation">:</span>
                Y<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> X<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> p_h<span class="token punctuation">,</span> j<span class="token punctuation">:</span> j <span class="token operator">+</span> p_w<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> mode <span class="token operator">==</span> <span class="token string">'avg'</span><span class="token punctuation">:</span>
                Y<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> X<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> p_h<span class="token punctuation">,</span> j<span class="token punctuation">:</span> j <span class="token operator">+</span> p_w<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> Y
</code></pre> 
<h2><a id="LeNet23_219"></a>经典卷积神经网络LeNet（视频23）</h2> 
<p>背景：手写数字识别<br> 数据集：MNIST<br> <img src="https://images2.imgbox.com/47/79/U6mXIfrl_o.png" alt="在这里插入图片描述"><br> lenet是早期成功的神经网络<br> 先用卷积层学习图片空间信息，然后使用全连接层转换到列别空间</p> 
<p>当年的文章里有很多超前的内容到现在也没有实现。</p> 
<h4><a id="LetNet_227"></a>LetNet实现</h4> 
<p>我们对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 为了得到「非线性性」，在卷积后面加一个sigmoid函数</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment"># 卷积层出来是个4d，要变1d输入到全连接层</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/0a/e2/RmwO1OQx_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_247"></a>学习如何手动检查模型</h4> 
<p>可以用summerize，但是层数多网络复杂时，还是手动检查比较好</p> 
<pre><code class="prism language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    X <span class="token operator">=</span> layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape: \t'</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/73/c9/X1pXzTrr_o.png" alt="在这里插入图片描述"></p> 
<p><strong>核心思想</strong>：基本上可以看到卷积就是把层变小变小，通道变多变多。每个通道信息可以认为是一个pattern。<strong>不断地压缩空间信息，将能够抽出来的压缩信息放到通道里。</strong>（通道一直在增加，高宽在变小。在现在的NN里，通道可能会多到上千， 高宽最后会变成1）最后mlp就是将所有的pattern拿出来，通过一个svm模型， 训练得到最后的输出。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b58af15f4ada9d31031419b45f4b400c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">0基础学Linux-从小白到大牛（4）Linux命令大全</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/014fbd19d0ea17aa8f48f563d55a2e8e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">模态分析 Modal analysis</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>