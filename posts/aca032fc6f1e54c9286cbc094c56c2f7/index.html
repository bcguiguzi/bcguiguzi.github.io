<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Informer pytorch 代码解读（1）Encoder - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Informer pytorch 代码解读（1）Encoder" />
<meta property="og:description" content="目录
0.对整体的架构进行分析
整个架构和Transformer是差不多的，但是Encoder层有堆叠，对Encoder进行分析发现，他整个部分的结构大体分为
（1）白色的部分，稀疏注意力的计算。
（2）蓝色的卷积层，进行信息的提取。
1.整个Informer的部分的代码如下主要由三个部分组成
（1）红色部分是词编码
（2）绿色部分是Encoder
（3）蓝色部分是Decoder​
2.对于encoder
3.Conv_layer部分
0.对整体的架构进行分析 整个架构和Transformer是差不多的，但是Encoder层有堆叠，对Encoder进行分析发现，他整个部分的结构大体分为 （1）白色的部分，稀疏注意力的计算。 （2）蓝色的卷积层，进行信息的提取。 对Encoder部分进行展开如下图。
1.整个Informer的部分的代码如下主要由三个部分组成 （1）红色部分是词编码 （2）绿色部分是Encoder （3）蓝色部分是Decoder 2.对于encoder 红色部分的代码时EncoderLayer层，也就是主要的attention的计算层。
绿色部分是卷积层，主要是进行信息的提取和维度的变短。
查看Encoder包括的attn_layer的信息，发现attn_layer就是一个EncoderLayer，而一个EncoderLayer还包括一个注意力的计算和两个卷积层，以及两个标准化层和dropout层，两个卷积是512 - 2048 - 512 所以一个EncoderLayer层对于向量来说维度是不变的，在接下来也会有体现。但是Encoder还包括一个额外的卷积层也就是上图的绿色的部分，经过绿色的卷积层维度会变化 ，在接下来的代码中也会有体现。
一个Encoder包括两个EncoderLayer和一个卷积层，这个卷积层的数量是比EncoderLayer层的数量少一个的。 这时候传入的x的维度是（32，96，512），来计算attention。
进入atten.py来进行详细的计算
到达红色部分的时候query和keys以及values的尺度都是由
（32，96，512）-》（32，96，8，64）规范的格式是（32，96，H，d model / H）也就进行了分多头
这里传入的queries，keys以及values都是（32，96，8，64）的维度的。而经过transpose过后的都是（32，8，96，64）维度的。因为这里传入的都是encoder的输入，所以queries和keys都是一样的，所以L_Q和L_K也都是一样的，都是96的长度。U_part是对L_K进行log得到的是25，u是对L_Q进行log，得到的也是25。
接下来进行到了红色的这部分也就是前期的log以及queries和keys的准备工作都完成了，下面开始进行prob_QK的计算。传入的数据：
scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)，queries和keys都是原始的queries和keys，而sample_K是抽样出来为了计算n_top的K，sample_K是为了n_top来服务的。 接下来进入prob_QK的计算工作，前面都是在为了prob_QK的计算做参数的准备。
这里传入的Q，K的维度都是（32，8，96，64）
(1) K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E) 这一步得到K_expand，是（32，8，96，96，64）的维度的 (2) index_sample = torch.randint(L_K, (L_Q, sample_k)) 这一步得到的index_sample的维度是（96，25）的维度 (3) K_sample = K_expand[:, :, torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/aca032fc6f1e54c9286cbc094c56c2f7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-09T17:09:34+08:00" />
<meta property="article:modified_time" content="2022-04-09T17:09:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Informer pytorch 代码解读（1）Encoder</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="0.%E5%AF%B9%E6%95%B4%E4%BD%93%E7%9A%84%E6%9E%B6%E6%9E%84%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%EF%BC%8C%E6%95%B4%E4%B8%AA%E6%9E%B6%E6%9E%84%E5%92%8CTransformer%E6%98%AF%E5%B7%AE%E4%B8%8D%E5%A4%9A%E7%9A%84%EF%BC%8C%E4%BD%86%E6%98%AFEncoder%E5%B1%82%E6%9C%89%E5%A0%86%E5%8F%A0%EF%BC%8C%E5%AF%B9Encoder%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E5%8F%91%E7%8E%B0%EF%BC%8C%E4%BB%96%E6%95%B4%E4%B8%AA%E9%83%A8%E5%88%86%E7%9A%84%E7%BB%93%E6%9E%84%E5%A4%A7%E4%BD%93%E5%88%86%E4%B8%BA-toc" style="margin-left:0px;"><a href="#0.%E5%AF%B9%E6%95%B4%E4%BD%93%E7%9A%84%E6%9E%B6%E6%9E%84%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%EF%BC%8C%E6%95%B4%E4%B8%AA%E6%9E%B6%E6%9E%84%E5%92%8CTransformer%E6%98%AF%E5%B7%AE%E4%B8%8D%E5%A4%9A%E7%9A%84%EF%BC%8C%E4%BD%86%E6%98%AFEncoder%E5%B1%82%E6%9C%89%E5%A0%86%E5%8F%A0%EF%BC%8C%E5%AF%B9Encoder%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E5%8F%91%E7%8E%B0%EF%BC%8C%E4%BB%96%E6%95%B4%E4%B8%AA%E9%83%A8%E5%88%86%E7%9A%84%E7%BB%93%E6%9E%84%E5%A4%A7%E4%BD%93%E5%88%86%E4%B8%BA" rel="nofollow">0.对整体的架构进行分析</a></p> 
<p id="%E6%95%B4%E4%B8%AA%E6%9E%B6%E6%9E%84%E5%92%8CTransformer%E6%98%AF%E5%B7%AE%E4%B8%8D%E5%A4%9A%E7%9A%84%EF%BC%8C%E4%BD%86%E6%98%AFEncoder%E5%B1%82%E6%9C%89%E5%A0%86%E5%8F%A0%EF%BC%8C%E5%AF%B9Encoder%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E5%8F%91%E7%8E%B0%EF%BC%8C%E4%BB%96%E6%95%B4%E4%B8%AA%E9%83%A8%E5%88%86%E7%9A%84%E7%BB%93%E6%9E%84%E5%A4%A7%E4%BD%93%E5%88%86%E4%B8%BA-toc" style="margin-left:40px;"><a href="#%E6%95%B4%E4%B8%AA%E6%9E%B6%E6%9E%84%E5%92%8CTransformer%E6%98%AF%E5%B7%AE%E4%B8%8D%E5%A4%9A%E7%9A%84%EF%BC%8C%E4%BD%86%E6%98%AFEncoder%E5%B1%82%E6%9C%89%E5%A0%86%E5%8F%A0%EF%BC%8C%E5%AF%B9Encoder%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E5%8F%91%E7%8E%B0%EF%BC%8C%E4%BB%96%E6%95%B4%E4%B8%AA%E9%83%A8%E5%88%86%E7%9A%84%E7%BB%93%E6%9E%84%E5%A4%A7%E4%BD%93%E5%88%86%E4%B8%BA" rel="nofollow">整个架构和Transformer是差不多的，但是Encoder层有堆叠，对Encoder进行分析发现，他整个部分的结构大体分为</a></p> 
<p id="%EF%BC%881%EF%BC%89%E7%99%BD%E8%89%B2%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%8C%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E8%AE%A1%E7%AE%97%E3%80%82-toc" style="margin-left:80px;"><a href="#%EF%BC%881%EF%BC%89%E7%99%BD%E8%89%B2%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%8C%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E8%AE%A1%E7%AE%97%E3%80%82" rel="nofollow">（1）白色的部分，稀疏注意力的计算。</a></p> 
<p id="%EF%BC%882%EF%BC%89%E8%93%9D%E8%89%B2%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%8C%E8%BF%9B%E8%A1%8C%E4%BF%A1%E6%81%AF%E7%9A%84%E6%8F%90%E5%8F%96%E3%80%82-toc" style="margin-left:80px;"><a href="#%EF%BC%882%EF%BC%89%E8%93%9D%E8%89%B2%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%8C%E8%BF%9B%E8%A1%8C%E4%BF%A1%E6%81%AF%E7%9A%84%E6%8F%90%E5%8F%96%E3%80%82" rel="nofollow">（2）蓝色的卷积层，进行信息的提取。</a></p> 
<p id="1.%E6%95%B4%E4%B8%AAInformer%E7%9A%84%E9%83%A8%E5%88%86%E7%9A%84%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%E4%B8%BB%E8%A6%81%E7%94%B1%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90-toc" style="margin-left:0px;"><a href="#1.%E6%95%B4%E4%B8%AAInformer%E7%9A%84%E9%83%A8%E5%88%86%E7%9A%84%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%E4%B8%BB%E8%A6%81%E7%94%B1%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90" rel="nofollow">1.整个Informer的部分的代码如下主要由三个部分组成</a></p> 
<p id="%EF%BC%881%EF%BC%89%E7%BA%A2%E8%89%B2%E9%83%A8%E5%88%86%E6%98%AF%E8%AF%8D%E7%BC%96%E7%A0%81-toc" style="margin-left:40px;"><a href="#%EF%BC%881%EF%BC%89%E7%BA%A2%E8%89%B2%E9%83%A8%E5%88%86%E6%98%AF%E8%AF%8D%E7%BC%96%E7%A0%81" rel="nofollow">（1）红色部分是词编码</a></p> 
<p id="%EF%BC%882%EF%BC%89%E7%BB%BF%E8%89%B2%E9%83%A8%E5%88%86%E6%98%AFEncoder-toc" style="margin-left:40px;"><a href="#%EF%BC%882%EF%BC%89%E7%BB%BF%E8%89%B2%E9%83%A8%E5%88%86%E6%98%AFEncoder" rel="nofollow">（2）绿色部分是Encoder</a></p> 
<p id="%EF%BC%883%EF%BC%89%E8%93%9D%E8%89%B2%E9%83%A8%E5%88%86%E6%98%AFDecoder%E2%80%8B-toc" style="margin-left:40px;"><a href="#%EF%BC%883%EF%BC%89%E8%93%9D%E8%89%B2%E9%83%A8%E5%88%86%E6%98%AFDecoder%E2%80%8B" rel="nofollow">（3）蓝色部分是Decoder​</a></p> 
<p id="%C2%A02.%E5%AF%B9%E4%BA%8Eencoder-toc" style="margin-left:0px;"><a href="#%C2%A02.%E5%AF%B9%E4%BA%8Eencoder" rel="nofollow">2.对于encoder</a></p> 
<p id="3.Conv_layer%E9%83%A8%E5%88%86%EF%BC%8C%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%8F%88%E8%BF%9B%E5%85%A5%E5%88%B0%E4%BA%86Encoder%E5%B1%82%EF%BC%8C%E5%88%B0%E4%BA%86conv_layer%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%8C%E2%80%8B-toc" style="margin-left:0px;"><a href="#3.Conv_layer%E9%83%A8%E5%88%86%EF%BC%8C%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%8F%88%E8%BF%9B%E5%85%A5%E5%88%B0%E4%BA%86Encoder%E5%B1%82%EF%BC%8C%E5%88%B0%E4%BA%86conv_layer%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%8C%E2%80%8B" rel="nofollow">3.Conv_layer部分</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<h2 id="0.%E5%AF%B9%E6%95%B4%E4%BD%93%E7%9A%84%E6%9E%B6%E6%9E%84%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%EF%BC%8C%E6%95%B4%E4%B8%AA%E6%9E%B6%E6%9E%84%E5%92%8CTransformer%E6%98%AF%E5%B7%AE%E4%B8%8D%E5%A4%9A%E7%9A%84%EF%BC%8C%E4%BD%86%E6%98%AFEncoder%E5%B1%82%E6%9C%89%E5%A0%86%E5%8F%A0%EF%BC%8C%E5%AF%B9Encoder%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E5%8F%91%E7%8E%B0%EF%BC%8C%E4%BB%96%E6%95%B4%E4%B8%AA%E9%83%A8%E5%88%86%E7%9A%84%E7%BB%93%E6%9E%84%E5%A4%A7%E4%BD%93%E5%88%86%E4%B8%BA">0.对整体的架构进行分析</h2> 
<h3 id="%E6%95%B4%E4%B8%AA%E6%9E%B6%E6%9E%84%E5%92%8CTransformer%E6%98%AF%E5%B7%AE%E4%B8%8D%E5%A4%9A%E7%9A%84%EF%BC%8C%E4%BD%86%E6%98%AFEncoder%E5%B1%82%E6%9C%89%E5%A0%86%E5%8F%A0%EF%BC%8C%E5%AF%B9Encoder%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E5%8F%91%E7%8E%B0%EF%BC%8C%E4%BB%96%E6%95%B4%E4%B8%AA%E9%83%A8%E5%88%86%E7%9A%84%E7%BB%93%E6%9E%84%E5%A4%A7%E4%BD%93%E5%88%86%E4%B8%BA">整个架构和Transformer是差不多的，但是Encoder层有堆叠，对Encoder进行分析发现，他整个部分的结构大体分为</h3> 
<h4 id="%EF%BC%881%EF%BC%89%E7%99%BD%E8%89%B2%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%8C%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E8%AE%A1%E7%AE%97%E3%80%82">（1）白色的部分，稀疏注意力的计算。</h4> 
<h4 id="%EF%BC%882%EF%BC%89%E8%93%9D%E8%89%B2%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%8C%E8%BF%9B%E8%A1%8C%E4%BF%A1%E6%81%AF%E7%9A%84%E6%8F%90%E5%8F%96%E3%80%82">（2）蓝色的卷积层，进行信息的提取。</h4> 
<p><img alt="" height="938" src="https://images2.imgbox.com/c1/f3/fyH1WN6i_o.png" width="1200"></p> 
<p id="%C2%A0%E5%AF%B9Encoder%E9%83%A8%E5%88%86%E8%BF%9B%E8%A1%8C%E5%B1%95%E5%BC%80%E5%A6%82%E4%B8%8B%E5%9B%BE%E3%80%82%E2%80%8B"> 对Encoder部分进行展开如下图。<img alt="" height="937" src="https://images2.imgbox.com/99/ec/FI6tXquN_o.png" width="1200"></p> 
<h2 id="1.%E6%95%B4%E4%B8%AAInformer%E7%9A%84%E9%83%A8%E5%88%86%E7%9A%84%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%E4%B8%BB%E8%A6%81%E7%94%B1%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90">1.整个Informer的部分的代码如下主要由三个部分组成</h2> 
<h3 id="%EF%BC%881%EF%BC%89%E7%BA%A2%E8%89%B2%E9%83%A8%E5%88%86%E6%98%AF%E8%AF%8D%E7%BC%96%E7%A0%81">（1）红色部分是词编码</h3> 
<h3 id="%EF%BC%882%EF%BC%89%E7%BB%BF%E8%89%B2%E9%83%A8%E5%88%86%E6%98%AFEncoder">（2）绿色部分是Encoder</h3> 
<h3 id="%EF%BC%883%EF%BC%89%E8%93%9D%E8%89%B2%E9%83%A8%E5%88%86%E6%98%AFDecoder%E2%80%8B">（3）蓝色部分是Decoder<img alt="" height="1147" src="https://images2.imgbox.com/97/23/aBWQldst_o.png" width="883"></h3> 
<h2 id="%C2%A02.%E5%AF%B9%E4%BA%8Eencoder">2.对于encoder</h2> 
<p>红色部分的代码时EncoderLayer层，也就是主要的attention的计算层。</p> 
<p>绿色部分是卷积层，主要是进行信息的提取和维度的变短。</p> 
<p><img alt="" height="649" src="https://images2.imgbox.com/d8/70/Uw3qsQiV_o.png" width="1200"></p> 
<p> <img alt="" height="354" src="https://images2.imgbox.com/16/d5/QxYG3aIc_o.png" width="384"></p> 
<p> <img alt="" height="513" src="https://images2.imgbox.com/c0/84/JPJWVNq7_o.png" width="1097"></p> 
<p> </p> 
<p></p> 
<p>查看Encoder包括的attn_layer的信息，发现attn_layer就是一个EncoderLayer，而一个EncoderLayer还包括一个注意力的计算和两个卷积层，以及两个标准化层和dropout层，两个卷积是512 - 2048 - 512 所以一个EncoderLayer层对于向量来说维度是不变的，在接下来也会有体现。但是Encoder还包括一个额外的卷积层也就是上图的绿色的部分，经过绿色的卷积层维度会变化 ，在接下来的代码中也会有体现。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/34/5c/LymMKDLV_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/1b/c6/PcKTLW3r_o.png" width="1200"></p> 
<p><img alt="" height="710" src="https://images2.imgbox.com/32/c7/78YV3SC4_o.png" width="1200"></p> 
<p>一个Encoder包括两个EncoderLayer和一个卷积层，这个卷积层的数量是比EncoderLayer层的数量少一个的。 </p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/61/f2/lDVKrUJs_o.png" width="1200"></p> 
<p> 这时候传入的x的维度是（32，96，512），来计算attention。<img alt="" height="1083" src="https://images2.imgbox.com/d7/d4/Jim12iOg_o.png" width="1200"></p> 
<p> 进入atten.py来进行详细的计算<img alt="" height="1122" src="https://images2.imgbox.com/5a/b0/ZgIUiZmm_o.png" width="1200"></p> 
<p> 到达红色部分的时候query和keys以及values的尺度都是由</p> 
<p>（32，96，512）-》（32，96，8，64）规范的格式是（32，96，H，d model / H）也就进行了分多头<img alt="" height="1126" src="https://images2.imgbox.com/df/55/XVwDF0Wb_o.png" width="1200"></p> 
<p> 这里传入的queries，keys以及values都是（32，96，8，64）的维度的。而经过transpose过后的都是（32，8，96，64）维度的。因为这里传入的都是encoder的输入，所以queries和keys都是一样的，所以L_Q和L_K也都是一样的，都是96的长度。U_part是对L_K进行log得到的是25，u是对L_Q进行log，得到的也是25。</p> 
<p><img alt="" height="1074" src="https://images2.imgbox.com/cc/cf/f9NxDojP_o.png" width="1200"></p> 
<p> 接下来进行到了红色的这部分也就是前期的log以及queries和keys的准备工作都完成了，下面开始进行prob_QK的计算。传入的数据：</p> 
<pre>scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)，queries和keys都是原始的queries和keys，而sample_K是抽样出来为了计算n_top的K，sample_K是为了n_top来服务的。</pre> 
<p><img alt="" height="959" src="https://images2.imgbox.com/75/ba/cDpAc9o0_o.png" width="1200"></p> 
<p> 接下来进入prob_QK的计算工作，前面都是在为了prob_QK的计算做参数的准备。</p> 
<p>这里传入的Q，K的维度都是（32，8，96，64）</p> 
<pre>(1) K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E) 这一步得到K_expand，是（32，8，96，96，64）的维度的
(2) index_sample = torch.randint(L_K, (L_Q, sample_k)) 这一步得到的index_sample的维度是（96，25）的维度
(3) K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :],这里不知道用的什么原理，传入的index_sample是一个随机的数组，但是将这个index_sample传入了这个K_expand的第三个维度，将一个二维的传入了第三个维度，使用jupter发现，只是利用了这个的维度来进行sample，并没有说利用里面的数据来对K_expand进行产生实质的影响。
这里出来的K_sample的维度是（32，8，96，25，64）
(4) Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)在这里，Q.unsqueeze(-2) 使得Q由（32，8，96，64）=（32，8，96，1，64）K_sample.transpose(-2, -1)，使得K_sample由（32，8，96，25，64）变成了（32，8，96，64，25）进行matmul之后变成了（32，8，96，1，25），然后squeeze(-2)后变成了（32，8，96，25）所以最后乘出来的Q_K_sample就变成了（32，8，96，25）而如果利用原始的Q和K进行相乘应该是（32，8，96，64）的，所以还需要进行下一步的处理。
(5) M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)
    M_top = M.topk(n_top, sorted=False)[1]
    Q_reduce = Q[torch.arange(B)[:, None, None],
                 torch.arange(H)[None, :, None],
                 M_top,
                 :]经过这步之后，<code>  * torch.div()<span style="font-family:Consolas, Inconsolata, Courier, monospace;">方法将输入的每个元素除以一个常量，然后返回一个新的修改过的张量
</span>  * <span style="font-family:Consolas, Inconsolata, Courier, monospace;">M_top = 32 * 8 *25    topK是得到前k个以及其索引</span>然后得到reduce后的Q，也就是Q_reduce(6) <span style="font-family:Consolas, Inconsolata, Courier, monospace;">Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1)) 得到乘出来的稀疏的Q_K，这个Q_K是（32，8，25，96）维度的。</span></code></pre> 
<p>（3）对应的图如下，发现，只是利用了他的维度？？并没有对a里面的数据产生影响 </p> 
<p> <img alt="" height="1027" src="https://images2.imgbox.com/26/43/FtMxn33t_o.png" width="627"></p> 
<p> 接下来进入到绿色的部分，传入的是values和L_Q，也就是原始的编码和96这个长度。<img alt="" height="805" src="https://images2.imgbox.com/84/38/mWvdEUFG_o.png" width="1200"></p> 
<p> <img alt="" height="323" src="https://images2.imgbox.com/6d/25/iDiIjGBW_o.png" width="1088"></p> 
<p>cumsum的用法如下：<img alt="" height="178" src="https://images2.imgbox.com/0a/22/s34mqV7e_o.png" width="704"></p> 
<p> <img alt="" height="625" src="https://images2.imgbox.com/80/74/KNoCcNqA_o.png" width="1200"></p> 
<p> </p> 
<pre>这里context_in, V, scores, index, L_Q, attn_mask的维度，分别是：
context_in：（32，8，96，64）
V：（32，8，96，64）
scores：（32，8，25，96）
index：（32，8，25）
L_Q：{int} 96 
attn_mask:None
</pre> 
<pre>（1）attn = torch.softmax(scores, dim=-1)，这一步是对之前的scores进行softmax，经过softmax的矩阵的维度还是（32，8，25，96）
（2）context_in[torch.arange(B)[:, None, None],
           torch.arange(H)[None, :, None],
           index, :] = torch.matmul(attn, V).type_as(context_in)

        *（tensor1.type_as(tensor2)将1的数据<a href="https://so.csdn.net/so/search?q=%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2&amp;spm=1001.2101.3001.7020" title="类型转换">类型转换</a>为2的数据类型,这里的输出的context_in的                        维度是（32，8，96，64）。</pre> 
<p>到了这里，这个</p> 
<pre>out = out.transpose(2,1).contiguous()，out的维度是（32，8，96，64），再下一步。out = out.view(B, L, -1)，到了这里，这个out的维度就是（32，96，512）的维度的了。</pre> 
<p><img alt="" height="939" src="https://images2.imgbox.com/c8/39/RdKc6ZNy_o.png" width="1200"></p> 
<p> 再接下来就出到了EncoderLayer层的这个红色部分，</p> 
<pre>（1）x = x + self.dropout(new_x) 这个nex_x是做过稀疏矩阵相乘的attention，而x是原来的词编码，所以这一步类似于残差神经网络？</pre> 
<pre>（2）这一步将x进行标准化得到（标准化的x）和y，然后对y进行卷积，激活和卷积，再将（标准化的x）和（卷积后的y-》激活的y-》卷积的y）和（标准化的x）进行相加得到（x+y）然后再将（x+y）进行标准化后进行输出。
y = x = self.norm1(x) #y (32,96,512)
y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))  #(32,2048,96)
y = self.dropout(self.conv2(y).transpose(-1,1))                   #(32,96,512)

return self.norm2(x+y), attn</pre> 
<p><img alt="" height="1092" src="https://images2.imgbox.com/d5/ea/Vh0AghGd_o.png" width="1200"></p> 
<h2 id="3.Conv_layer%E9%83%A8%E5%88%86%EF%BC%8C%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%8F%88%E8%BF%9B%E5%85%A5%E5%88%B0%E4%BA%86Encoder%E5%B1%82%EF%BC%8C%E5%88%B0%E4%BA%86conv_layer%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%8C%E2%80%8B">3.Conv_layer部分</h2> 
<p>接下来又进入到了Encoder层，到了conv_layer的部分，<img alt="" height="922" src="https://images2.imgbox.com/43/30/lrqkQajP_o.png" width="1127"></p> 
<p> 到了这一层的ConvLayer层，输入的x的维度是（32，96，512）经过卷积后的x输出的维度是（32，48，512），所以经过这一层卷积是将x的维度变短了一半。<img alt="" height="669" src="https://images2.imgbox.com/f7/c4/TJLP5iUG_o.png" width="984"></p> 
<p> 经过Encoder这一个class的return的x的维度是（32，48，512）<img alt="" height="913" src="https://images2.imgbox.com/6b/30/q6Y3KhQ5_o.png" width="1200"></p> 
<p> 然后就到了Informer层的部分，红色部分是执行过的东西。这里enc_out的输出的维度是（32，48，512）。对比之前的词向量的输入，（32，96，512）在第二个维度上减少了一半。</p> 
<p>总结一下，即经过稀疏注意力层（32，96，512）-&gt;（32，96，512）而经过卷积层则变成了（32，48，512）。<img alt="" height="881" src="https://images2.imgbox.com/98/21/Yl6lHq7k_o.png" width="1200"></p> 
<p> 这里是重要的参数，记录一下为下面的部分做准备。<img alt="" height="518" src="https://images2.imgbox.com/49/28/JuUdckSP_o.png" width="752"></p> 
<p> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fce410cbebad78759f0b081f41d99f75/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">RTKLIB专题学习(六)---单点定位应用(二)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d57acbe9f763f04442413785159a53d4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">记录一波h5新增的一些js常用实用的api方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>