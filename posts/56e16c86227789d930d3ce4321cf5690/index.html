<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ARM架构服务器centos7.4上yum安装k8s教程 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ARM架构服务器centos7.4上yum安装k8s教程" />
<meta property="og:description" content="1.环境说明
[root@k8s-master ~]# uname -a Linux slave1 4.11.0-22.el7a.aarch64 #1 SMP Sun Sep 3 13:39:10 CDT 2017 aarch64 aarch64 aarch64 GNU/Linux [root@k8s-master ~]# cat /etc/redhat-release CentOS Linux release 7.4.1708 (AltArch) 主机名IP功能k8s-master10.2.152.78masterk8s-node110.2.152.72 node
2、修改master和node的hosts文件
[root@k8s-master ~]# vim /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 10.2.152.78	k8s-master 10.2.152.72	k8s-node1 3、安装ntp实现所有服务器间的时间同步
$：yum install ntp -y $：vim /etc/ntp.conf 21 server 10.2.152.72 iburst #目标服务器网络位置 22 #server 0.centos.pool.ntp.org iburst #一下三个是CentOS官方的NTP服务器，我们注释掉 23 #server 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/56e16c86227789d930d3ce4321cf5690/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-09-11T20:38:05+08:00" />
<meta property="article:modified_time" content="2018-09-11T20:38:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ARM架构服务器centos7.4上yum安装k8s教程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>1.环境说明</p> 
<pre class="has"><code>[root@k8s-master ~]# uname -a
Linux slave1 4.11.0-22.el7a.aarch64 #1 SMP Sun Sep 3 13:39:10 CDT 2017 aarch64 aarch64 aarch64 GNU/Linux
[root@k8s-master ~]# cat /etc/redhat-release 
CentOS Linux release 7.4.1708 (AltArch) 
</code></pre> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td>主机名</td><td>IP</td><td>功能</td></tr><tr><td>k8s-master</td><td>10.2.152.78</td><td>master</td></tr><tr><td>k8s-node1</td><td>10.2.152.72</td><td> <p>node</p> </td></tr></tbody></table> 
<p>2、修改master和node的hosts文件</p> 
<pre class="has"><code>[root@k8s-master ~]# vim /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

10.2.152.78	k8s-master
10.2.152.72	k8s-node1

</code></pre> 
<p>3、安装ntp实现所有服务器间的时间同步</p> 
<pre class="has"><code>$：yum install ntp -y
$：vim /etc/ntp.conf 
   21 server 10.2.152.72 iburst       #目标服务器网络位置
   22 #server 0.centos.pool.ntp.org iburst    #一下三个是CentOS官方的NTP服务器，我们注释掉
   23 #server 1.centos.pool.ntp.org iburst
   24 #server 2.centos.pool.ntp.org iburst
   25 #server 3.centos.pool.ntp.org iburst
$：systemctl start ntpd.service
$：systemctl enable ntpd.service
$：systemctl status ntpd.service
● ntpd.service - Network Time Service
   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2018-09-06 10:44:05 CST; 1 day 3h ago
 Main PID: 2334 (ntpd)
   CGroup: /system.slice/ntpd.service
           └─2334 /usr/sbin/ntpd -u ntp:ntp -g

Sep 06 11:01:33 slave1 ntpd[2334]: new interface(s) found: wak...r
Sep 06 11:06:54 slave1 ntpd[2334]: 0.0.0.0 0618 08 no_sys_peer
Sep 07 09:26:34 slave1 ntpd[2334]: Listen normally on 8 flanne...3
Sep 07 09:26:34 slave1 ntpd[2334]: Listen normally on 9 flanne...3
Sep 07 09:26:34 slave1 ntpd[2334]: new interface(s) found: wak...r
Sep 07 09:56:32 slave1 ntpd[2334]: Listen normally on 10 docke...3
Sep 07 09:56:32 slave1 ntpd[2334]: Listen normally on 11 flann...3
Sep 07 09:56:32 slave1 ntpd[2334]: Deleting interface #9 flann...s
Sep 07 09:56:32 slave1 ntpd[2334]: Deleting interface #7 docke...s
Sep 07 09:56:32 slave1 ntpd[2334]: new interface(s) found: wak...r
Hint: Some lines were ellipsized, use -l to show in full.
</code></pre> 
<p>4、关闭master和node的防火墙和selinux</p> 
<pre class="has"><code>$:sudo systemctl stop firewalld 
$:sudo systemctl disable firewalld
$:sudo vim /etc/selinux/config 

# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=disabled
# SELINUXTYPE= can take one of three two values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected. 
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted
 
$:reboot  //重启服务器</code></pre> 
<p>5、master和node上安装docker</p> 
<p>    (1)安装依赖包</p> 
<pre class="has"><code>$:yum install -y yum-utils device-mapper-persistent-data lvm2
</code></pre> 
<p>   (2)添加docker软件包的yum源</p> 
<pre class="has"><code>$:yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</code></pre> 
<p>   (3)关闭测试版本list（只显示稳定版）</p> 
<pre class="has"><code>$:yum-config-manager --enable docker-ce-edge
$:yum-config-manager --enable docker-ce-test</code></pre> 
<p>    (4)更新yum包索引</p> 
<pre class="has"><code>$:yum makecache fast</code></pre> 
<p>    (5)安装Docker</p> 
<p>         NO.1：直接安装Docker  CE（will  always install the highest possible version）</p> 
<pre class="has"><code>$:yum install docker-ce</code></pre> 
<p>         NO.2：安装指定版本的Docker CE</p> 
<pre class="has"><code>$:yum list docker-ce --showduplicates|sort -r      #找到需要安装的
$:sudo yum install docker-ce-18.06.0.ce -y    #启动docker
$:systemctl start docker &amp; systemctl enable docker</code></pre> 
<p>Error:</p> 
<p>     因为之前安装过旧版本的docker，安装时出现以下报错信息：</p> 
<p style="margin-left:0pt;"><span style="color:#000000;">Transaction check error:</span></p> 
<p style="margin-left:0pt;">  <span style="color:#0000ff;">file</span><span style="color:#000000;"> /usr/bin/docker from </span><span style="color:#0000ff;">install</span><span style="color:#000000;"> of docker-ce-</span><span style="color:#800080;">17.12</span><span style="color:#000000;">.</span><span style="color:#800080;">0</span><span style="color:#000000;">.ce-</span><span style="color:#800080;">1</span><span style="color:#000000;">.el7.centos.x86_64 conflicts with </span><span style="color:#0000ff;">file</span><span style="color:#000000;"> from package </span><strong><span style="color:#ff0000;"><strong>docker-common-2:1.12.6-68.gitec8512b.el7.centos.aarch_64</strong></span></strong></p> 
<p style="margin-left:0pt;"><strong>  </strong><span style="color:#0000ff;">file</span><span style="color:#000000;"> /usr/bin/docker-containerd from </span><span style="color:#0000ff;">install</span><span style="color:#000000;"> of docker-ce-</span><span style="color:#800080;">17.12</span><span style="color:#000000;">.</span><span style="color:#800080;">0</span><span style="color:#000000;">.ce-</span><span style="color:#800080;">1</span><span style="color:#000000;">.el7.centos.x86_64 conflicts with </span><span style="color:#0000ff;">file</span><span style="color:#000000;"> from package </span><strong><span style="color:#ff0000;"><strong>docker-common-2:1.12.6-68.gitec8512b.el7.centos.aarch_64</strong></span></strong></p> 
<p style="margin-left:0pt;"><strong>  </strong><span style="color:#0000ff;">file</span><span style="color:#000000;"> /usr/bin/docker-containerd-shim from </span><span style="color:#0000ff;">install</span><span style="color:#000000;"> of docker-ce-</span><span style="color:#800080;">17.12</span><span style="color:#000000;">.</span><span style="color:#800080;">0</span><span style="color:#000000;">.ce-</span><span style="color:#800080;">1</span><span style="color:#000000;">.el7.centos.x86_64 conflicts with </span><span style="color:#0000ff;">file</span><span style="color:#000000;"> from package </span><strong><span style="color:#ff0000;"><strong>docker-common-2:1.12.6-68.gitec8512b.el7.centos.aarch_64</strong></span></strong></p> 
<p style="margin-left:0pt;"><strong>  </strong><span style="color:#0000ff;">file</span><span style="color:#000000;"> /usr/bin/dockerd from </span><span style="color:#0000ff;">install</span><span style="color:#000000;"> of docker-ce-</span><span style="color:#800080;">17.12</span><span style="color:#000000;">.</span><span style="color:#800080;">0</span><span style="color:#000000;">.ce-</span><span style="color:#800080;">1</span><span style="color:#000000;">.el7.centos.x86_64 conflicts with </span><span style="color:#0000ff;">file</span><span style="color:#000000;"> from package </span><strong><span style="color:#ff0000;"><strong>docker-common-2:1.12.6-68.gitec8512b.el7.centos.aarch_64</strong></span></strong></p> 
<p>     卸载旧版本的docker包</p> 
<pre class="has"><code>$:yum erase docker-common-2:1.12.6-68.gitec8512b.el7.centos.aarch_64</code></pre> 
<p>     <span style="color:#000000;">再次安装docker！！！</span></p> 
<p><span style="color:#000000;">Error：</span></p> 
<p><span style="color:#000000;">    安装完docker用“docker  version”chak</span>查看docker版本报：</p> 
<p style="margin-left:0pt;"><span style="color:#000000;">Cannot connect </span><span style="color:#4f4f4f;">to</span> <span style="color:#000000;">the</span><span style="color:#000000;"> Docker daemon </span><span style="color:#000088;">at</span><span style="color:#000000;"> unix:</span><span style="color:#880000;">///var/run/docker.sock. Is the docker daemon running?</span></p> 
<p>     解决方法:</p> 
<p>            配置DOCKER_HOST</p> 
<pre class="has"><code>$:vim /etc/profile.d/docker.sh
内容如下
export DOCKER_HOST=tcp://localhost:2375 </code></pre> 
<p>             应用</p> 
<pre class="has"><code>$:source /etc/profile
$:source /etc/bashrc</code></pre> 
<p>            配置启动文件</p> 
<pre class="has"><code>$:vim /lib/systemd/system/docker.service
将
ExecStart=/usr/bin/dockerd
修改为
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock -H tcp://0.0.0.0:7654</code></pre> 
<p>     注：2375 是管理端口 ；7654 是备用端口</p> 
<p style="margin-left:0pt;">           重载配置和重启</p> 
<pre class="has"><code>$:systemctl daemon-reload 
$:systemctl restart docker.service</code></pre> 
<p>             查看</p> 
<pre class="has"><code>docker version
输出
Client:
     Version:      18.03.1-ce
     API version:  1.37
     Go version:   go1.9.5
     Git commit:   9ee9f40
     Built:        Thu Apr 26 07:20:16 2018
     OS/Arch:      linux/amd64
     Experimental: false
     Orchestrator: swarm
Server:
     Engine:
      Version:      18.03.1-ce
      API version:  1.37 (minimum version 1.12)
      Go version:   go1.9.5
      Git commit:   9ee9f40
      Built:        Thu Apr 26 07:23:58 2018
      OS/Arch:      linux/amd64
      Experimental: false</code></pre> 
<p>6、master和node上安装k8s</p> 
<p>   (1)更换yum源为阿里源</p> 
<pre class="has"><code>vim   /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-aarch64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
exclude=kube*</code></pre> 
<p>   （2）yum安装k8s</p> 
<pre class="has"><code>$:yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes</code></pre> 
<p>(3)启动k8s服务</p> 
<pre class="has"><code> systemctl enable kubelet &amp;&amp; systemctl start kubelet</code></pre> 
<p>（4）查看版本号</p> 
<pre class="has"><code>$:kubeadm version
kubeadm version: &amp;version.Info{Major:"1", Minor:"11", GitVersion:"v1.11.2", GitCommit:"bb9ffb1654d4a729bb4cec18ff088eacc153c239", GitTreeState:"clean", BuildDate:"2018-08-07T23:14:39Z", GoVersion:"go1.10.3", Compiler:"gc", Platform:"linux/arm64"}</code></pre> 
<p>（5）配置iptable</p> 
<pre class="has"><code>$:vim  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0

$:sysctl --system</code></pre> 
<p>7.关掉swap</p> 
<pre class="has"><code>$:sudo swapoff -a
 #要永久禁掉swap分区，打开如下文件注释掉swap那一行
 # sudo vi /etc/stab</code></pre> 
<p>8.安装etcd和flannel（master上安装etcd+flannel，node上只安装flannel）</p> 
<pre class="has"><code>$:yum  -y  install  etcd
$:systemctl start etcd;systemctl enable etcd

$:yum  -y  install  flannel</code></pre> 
<p>9.master上初始化镜像</p> 
<pre class="has"><code>$:kubeadm init --kubernetes-version=v1.11.2 --pod-network-cidr=10.2.0.0/16 --apiserver-advertise-address=10.2.152.78
#这里是之前所安装K8S的版本号；这里填写集群所在网段
输出：
[init] using Kubernetes version: v1.11.2
[preflight] running pre-flight checks
I0909 11:13:01.251094   31919 kernel_validator.go:81] Validating kernel version
I0909 11:13:01.252496   31919 kernel_validator.go:96] Validating kernel config
[preflight/images] Pulling images required for setting up a Kubernetes cluster
[preflight/images] This might take a minute or two, depending on the speed of your internet connection
[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[preflight] Activating the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [localhost.localdomain kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.2.152.78]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Generated etcd/ca certificate and key.
[certificates] Generated etcd/server certificate and key.
[certificates] etcd/server serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [127.0.0.1 ::1]
[certificates] Generated etcd/peer certificate and key.
[certificates] etcd/peer serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [10.2.152.78 127.0.0.1 ::1]
[certificates] Generated etcd/healthcheck-client certificate and key.
[certificates] Generated apiserver-etcd-client certificate and key.
[certificates] valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[controlplane] wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests" 
[init] this might take a minute or longer if the control plane images have to be pulled
[apiclient] All control plane components are healthy after 75.007389 seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.11" in namespace kube-system with the configuration for the kubelets in the cluster
[markmaster] Marking the node localhost.localdomain as master by adding the label "node-role.kubernetes.io/master=''"
[markmaster] Marking the node localhost.localdomain as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "localhost.localdomain" as an annotation
[bootstraptoken] using token: dlo2ec.ynlr9uyocy9vdnvr
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 10.2.152.78:6443 --token dlo2ec.ynlr9uyocy9vdnvr --discovery-token-ca-cert-hash sha256:0457cd2a8ffcf91707a71c4ef6d8717e2a8a6a2c13ad01fa1fc3f15575e28534</code></pre> 
<p>根据输出执行：</p> 
<p style="margin-left:0pt;">  mkdir -p $HOME/.kube</p> 
<p style="margin-left:0pt;">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</p> 
<p style="margin-left:0pt;">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</p> 
<p style="margin-left:0pt;"><span style="color:#333333;">集群主节点安装成功，这里要记得保存这条命令，以便之后各个节点加入集群：</span></p> 
<p style="margin-left:0pt;">You can now join any number of machines by running the following on each node</p> 
<p style="margin-left:0pt;">as root:</p> 
<p style="margin-left:0pt;"><span style="color:#00b050;">  kubeadm join 10.2.152.78:6443 --token dlo2ec.ynlr9uyocy9vdnvr --discovery-token-ca-cert-hash sha256:0457cd2a8ffcf91707a71c4ef6d8717e2a8a6a2c13ad01fa1fc3f15575e28534</span></p> 
<p>Error：</p> 
<p>执行初始化报错：</p> 
<p style="margin-left:0pt;"><span style="color:#0000ff;">ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists</span><br><span style="color:#0000ff;">[ERROR Port-10250]: Port 10250 is in use</span></p> 
<p>原因及解决方法：</p> 
<p style="margin-left:0pt;"><span style="color:#000000;">     kubeadm会自动检查当前环境是否有上次命令执行的“残留”。如果有,必须清理后再行执行init。我们可以通过”kubeadm reset”来清理环境,以备重来。</span></p> 
<p>10.配置kubetl认证信息</p> 
<pre class="has"><code>$:echo "export KUBECONFIG=/etc/kubernetes/admin.conf" &gt;&gt; ~/.bash_profile（本人采用）
或
$:export KUBECONFIG=/etc/kubernetes/admin.conf</code></pre> 
<p>11.配置flannel网络</p> 
<p>参考：<a href="https://github.com/coreos/flannel">https://github.com/coreos/flannel</a></p> 
<p><img alt="" class="has" height="202" src="https://images2.imgbox.com/bc/b6/vwU6Y0h2_o.png" width="900"></p> 
<pre class="has"><code>$:kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></pre> 
<p style="margin-left:0pt;"><span style="color:#333333;">这里就表示执行完毕了，可以去主节点执行命令：</span></p> 
<p style="margin-left:0pt;">[root@k8s-master ~]# kubectl get nodes<br> NAME         STATUS    ROLES     AGE       VERSION<br> k8s-master   Ready     master    8h        v1.11.2</p> 
<p>12.添加node节点到mastersh上</p> 
<p>在node上执行master初始化保存下来的输出：</p> 
<p><span style="color:#00b050;">kubeadm join 10.2.152.78:6443 --token dlo2ec.ynlr9uyocy9vdnvr --discovery-token-ca-cert-hash sha256:0457cd2a8ffcf91707a71c4ef6d8717e2a8a6a2c13ad01fa1fc3f15575e28534</span></p> 
<p>切换的master上执行：</p> 
<p>[root@k8s-master ~]# kubectl get nodes<br> NAME         STATUS    ROLES     AGE       VERSION<br> k8s-master   Ready     master    8h        v1.11.2<br> k8s-node1    Ready     &lt;none&gt;    7h        v1.11.2<br>     注：Ready显示需要等待一会</p> 
<p>13.docker补充设置（可有可无）</p> 
<pre class="has"><code>$:vim /usr/lib/systemd/system/docker.service
 Environment="HTTPS_PROXY=http://www.ik8s.io:10080"
 Environment="NO_PROXY=127.0.0.0/8,127.20.0.0.0/16"
 ExecStart=/usr/bin/dockerd
 
$:systemctl daemon-reload 
$:systemctl start docker
$: docker info
    Containers: 10
     Running: 0
     Paused: 0
     Stopped: 10
    Images: 14
    Server Version: 18.06.0-ce
    Storage Driver: overlay2
    Backing Filesystem: xfs
     Supports d_type: true
     Native Overlay Diff: true
    Logging Driver: json-file
    Cgroup Driver: cgroupfs
    Plugins:
     Volume: local
     Network: bridge host macvlan null overlay
     Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog
    Swarm: inactive
    Runtimes: runc
    Default Runtime: runc
    Init Binary: docker-init
    containerd version: d64c661f1d51c48782c9cec8fda7604785f93587
    runc version: 69663f0bd4b60df09991c08812a60108003fa340
    init version: fec3683
    Security Options:
     seccomp
      Profile: default
    Kernel Version: 4.11.0-22.el7a.aarch64
    Operating System: CentOS Linux 7 (AltArch)
    OSType: linux
    Architecture: aarch64
    CPUs: 40
    Total Memory: 95.15GiB
    Name: k8s-master
    ID: F7B7:H45H:DFR5:BLRY:6EKG:EFV5:JPMR:YOJW:MGMA:HUK2:UMBD:CM6B
    Docker Root Dir: /var/lib/docker
    Debug Mode (client): false
    Debug Mode (server): false
    Registry: https://index.docker.io/v1/
    Labels:
    Experimental: false
    Insecure Registries:
     127.0.0.0/8
    Live Restore Enabled: false
$:cat /proc/sys/net/bridge/bridge-nf-call-ip6tables 
1
$:cat /proc/sys/net/bridge/bridge-nf-call-iptables 
1

</code></pre> 
<p>14、服务器配置总结</p> 
<table><tbody><tr><td style="vertical-align:top;"> <p>节点</p> </td><td style="vertical-align:top;"> <p>运行服务</p> </td></tr><tr><td style="vertical-align:top;"> <p> </p> <p> </p> <p> </p> <p>Master</p> </td><td style="vertical-align:top;"> <p>etcd</p> <p>kube-apiserver</p> <p>kube-controller-manager</p> <p>kube-scheduler</p> <p>kube-proxy</p> <p>kubelet</p> <p>docker</p> <p>flanneld</p> </td></tr><tr><td style="vertical-align:top;"> <p> </p> <p> </p> <p>node</p> </td><td style="vertical-align:top;"> <p>flanneld</p> <p>docker</p> <p>kube-proxy</p> <p>kubelet</p> </td></tr></tbody></table> 
<p>15.k8s测试</p> 
<p><img alt="" class="has" height="92" src="https://images2.imgbox.com/e4/ec/4jcXtDUN_o.png" width="604"></p> 
<hr> 
<hr> 
<h4 id="37-部署dashboard插件">部署Dashboard插件</h4> 
<p>1、下载Dashboard插件配置文件</p> 
<pre class="has"><code>$:mkdir -p ~/k8s
$:cd ~/k8s
$:wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</code></pre> 
<p>2、编辑kubernetes-dashboard.yaml文件，在Dashboard Service中添加<span style="color:#f33b45;">type: NodePort</span>，暴露Dashboard服务</p> 
<pre class="has"><code># ------------------- Dashboard Service ------------------- #
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard</code></pre> 
<p>3、安装Dashboard插件</p> 
<pre class="has"><code>$:kubectl create -f kubernetes-dashboard.yaml</code></pre> 
<p>报错信息：</p> 
<p>Error from server (AlreadyExists): error when creating "kubernetes-dashboard.yaml": deployments.extensions "kubernetes-dashboard" already exists </p> 
<p>Error from server (AlreadyExists): error when creating "kubernetes-dashboard.yaml": services "kubernetes-dashboard" already exists </p> 
<p>原因及解决：</p> 
<p>services kubernetes-dashboard已经存在了，但是这个在kubectl get services 是看不到的，可以通过以下命令删除然后重新创建！</p> 
<pre class="has"><code>$:kubectl delete -f kubernetes-dashboard.yaml</code></pre> 
<p>4、授予Dashboard账户集群管理权限 <br> 创建一个kubernetes-dashboard-admin的ServiceAccount并授予集群admin的权限，创建kubernetes-dashboard-admin.rbac.yaml</p> 
<pre class="has"><code>---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard-admin
  labels:
    k8s-app: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard-admin
  namespace: kube-system</code></pre> 
<p>执行</p> 
<pre class="has"><code>[root@k8s-master ~]# kubectl create -f kubernetes-dashboard-admin.rbac.yaml
serviceaccount "kubernetes-dashboard-admin" created
clusterrolebinding "kubernetes-dashboard-admin" created</code></pre> 
<p>5、查看kubernete-dashboard-admin的token</p> 
<pre class="has"><code>[root@k8s-master ~]# kubectl -n kube-system get secret | grep kubernetes-dashboard-admin
kubernetes-dashboard-admin-token-jxq7l   kubernetes.io/service-account-token   3         22h
[root@k8s-master ~]# kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-jxq7l
Name:         kubernetes-dashboard-admin-token-jxq7l
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  kubernetes.io/service-account.name=kubernetes-dashboard-admin
              kubernetes.io/service-account.uid=686ee8e9-ce63-11e7-b3d5-080027d38be0
Type:  kubernetes.io/service-account-token
Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1qeHE3bCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjY4NmVlOGU5LWNlNjMtMTFlNy1iM2Q1LTA4MDAyN2QzOGJlMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.Ua92im86o585ZPBfsOpuQgUh7zxgZ2p1EfGNhr99gAGLi2c3ss-2wOu0n9un9LFn44uVR7BCPIkRjSpTnlTHb_stRhHbrECfwNiXCoIxA-1TQmcznQ4k1l0P-sQge7YIIjvjBgNvZ5lkBNpsVanvdk97hI_kXpytkjrgIqI-d92Lw2D4xAvHGf1YQVowLJR_VnZp7E-STyTunJuQ9hy4HU0dmvbRXBRXQ1R6TcF-FTe-801qUjYqhporWtCaiO9KFEnkcYFJlIt8aZRSL30vzzpYnOvB_100_DdmW-53fLWIGYL8XFnlEWdU1tkADt3LFogPvBP4i9WwDn81AwKg_Q
ca.crt:     1025 bytes</code></pre> 
<p>6、查看Dashboard服务端口</p> 
<ul><li> <pre class="has"><code>[root@master k8s]# kubectl get svc -n kube-system
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
kube-dns               ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP   1d
kubernetes-dashboard   NodePort    10.102.209.161   &lt;none&gt;        443:32513/TCP   21h</code></pre> <p> </p> </li></ul> 
<p>7.打开浏览器访问UI界面：<a href="https://192.168.56.2:32513/" rel="nofollow">https://10.2.152.78:3</a>0000</p> 
<p><img alt="" class="has" height="404" src="https://images2.imgbox.com/38/1a/E6la6NlL_o.png" width="993"></p> 
<p> </p> 
<p><img alt="" class="has" height="925" src="https://images2.imgbox.com/ea/9e/SvMymlNs_o.png" width="1200"></p> 
<p> </p> 
<p>出错：</p> 
<p>浏览器访问web界面失败</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p>8.kubectl使用</p> 
<pre class="has"><code>$:kubectl version
Client Version: version.Info{Major:"1", Minor:"11", GitVersion:"v1.11.2", GitCommit:"bb9ffb1654d4a729bb4cec18ff088eacc153c239", GitTreeState:"clean", BuildDate:"2018-08-07T23:17:28Z", GoVersion:"go1.10.3", Compiler:"gc", Platform:"linux/arm64"}
Server Version: version.Info{Major:"1", Minor:"11", GitVersion:"v1.11.2", GitCommit:"bb9ffb1654d4a729bb4cec18ff088eacc153c239", GitTreeState:"clean", BuildDate:"2018-08-07T23:08:19Z", GoVersion:"go1.10.3", Compiler:"gc", Platform:"linux/arm64"}
$:kubectl get nodes
NAME         STATUS    ROLES     AGE       VERSION
k8s-master   Ready     master    1d        v1.11.2
k8s-node1    Ready     &lt;none&gt;    1d        v1.11.2
[root@k8s-master ~]# kubectl run kubernetes-bootcamp --image=jocatalin/kubernetes-bootcamp
deployment.apps/kubernetes-bootcamp created
[root@k8s-master ~]# kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   1         1         1            0           3s
[root@k8s-master ~]# kubectl get pods
NAME                                   READY     STATUS             RESTARTS   AGE
kubernetes-bootcamp-589d48ddb4-qkn5s   0/1       ImagePullBackOff   0          54s
[root@k8s-master ~]# kubectl get pods -o wide
NAME                                   READY     STATUS             RESTARTS   AGE       IP          NODE        NOMINATED NODE
kubernetes-bootcamp-589d48ddb4-qkn5s   0/1       ImagePullBackOff   0          1m        10.2.1.12   k8s-node1   &lt;none&gt;

</code></pre> 
<p>[root@k8s-master ~]# journalctl -f         #查看k8s的运行状态</p> 
<p> </p> 
<p>参考：</p> 
<p><a href="https://segmentfault.com/a/1190000015787725" rel="nofollow">https://segmentfault.com/a/1190000015787725</a></p> 
<p><a href="http://blog.51cto.com/douya/1945382" rel="nofollow">http://blog.51cto.com/douya/1945382</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2c4d1db4704973c6ff6f262ab7f0eaef/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">分屏状态下分出的屏幕提示超出显示范围</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4c7ba238f55feba340775c3814155723/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Android 界面加载卡顿严重，Skipped 56 frames! The application may be doing too much work on its main thread</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>