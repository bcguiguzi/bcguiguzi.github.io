<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【零基础学爬虫】第五章：scrapy框架的使用（一） - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【零基础学爬虫】第五章：scrapy框架的使用（一）" />
<meta property="og:description" content="目录
一、安装scrapy
二、创建scrapy工程
三、执行工程
一、安装scrapy ①我使用anaconda安装，步骤如下：
注意：如果手动pip安装，需要安装很多依赖包，所以用anaconda很方便
twisted：为scrapy提供异步下载相关操作
pywin32：①捕获窗口 ②模拟鼠标键盘动作 ③自动获取某路径下文件列表 ④PIL截屏功能
conda install scrapy ②输入scrapy，检测安装成功！
二、创建scrapy工程 1.点击pycharm左下角的terminal，在终端中输入如下命令：
scrapy startproject firsttest（工程名） 2.创建爬虫文件
创建成功后，目录下就会出现创建的工程文件。
—spiders文件夹叫爬虫文件夹，或爬虫目录，在spiders的子目录中要创建一个爬虫文件。相关操作如下：
首先要进入该工程中： cd firsttest 然后输入如下命令：（爬虫文件名字不能和工程名相同，后面的url可以临时写一个，在文件中还可以改） scrapy genspider spiderName www.xxx.com —settings.py是我们当前工程的配置文件。（经常使用）
3.分析爬虫文件（first.py）
点开文件，有一个类，类名就是创建的文件名&#43;Spider，该类的父类是Spider，Spider属于scrapy中的一个类，它是scrapy中所有爬虫类的父类。
文件中有三个属性和一个方法，解释见代码：
import scrapy class FirstSpider(scrapy.Spider): # 爬虫文件的名称：就是爬虫源文件的一个唯一标识 name = &#39;first&#39; # 允许的域名：用来限定start_urls列表中，哪些url可以进行请求发送。但是通常不会用它 #allowed_domains = [&#39;www.baidu.com&#39;] # 起始的url列表：该列表中存放的url会被scrapy自动进行请求的发送，可以放多个url start_urls = [&#39;http://www.baidu.com/&#39;,&#39;http://www.sogou.com/&#39;] # 用作于数据解析：response参数表示的就是请求成功后对应的响应对象。parse调用的次数由start_urls列表中元素的个数决定。 def parse(self, response): print(response) # 之后可以执行代码，不要直接run，要在终端中执行。见下面的命令。 三、执行工程 1.编写完代码后，在终端中输入如下命令，即可进行数据的爬取：
scrapy crawl spiderName 2.输出的是日志信息，如果运行中出现了错误，也会显示出来，但是输出中没有我们想要的response，原因如下：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/5408706172f9a21d7d99aad3b044c227/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-21T18:49:31+08:00" />
<meta property="article:modified_time" content="2021-02-21T18:49:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【零基础学爬虫】第五章：scrapy框架的使用（一）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85scrapy-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85scrapy" rel="nofollow">一、安装scrapy</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%88%9B%E5%BB%BAscrapy%E5%B7%A5%E7%A8%8B-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81%E5%88%9B%E5%BB%BAscrapy%E5%B7%A5%E7%A8%8B" rel="nofollow">二、创建scrapy工程</a></p> 
<p id="%E4%B8%89%E3%80%81%E6%89%A7%E8%A1%8C%E5%B7%A5%E7%A8%8B-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81%E6%89%A7%E8%A1%8C%E5%B7%A5%E7%A8%8B" rel="nofollow">三、执行工程</a></p> 
<hr id="hr-toc"> 
<h3>一、安装scrapy</h3> 
<p><strong>①我使用anaconda安装，步骤如下：</strong></p> 
<p><span style="color:#f33b45;"><strong>注意：如果手动pip安装，需要安装很多依赖包，所以用anaconda很方便</strong></span></p> 
<p>   <strong>     twisted：为scrapy提供异步下载相关操作</strong></p> 
<p>       <strong> pywin32：①捕获窗口 ②模拟鼠标键盘动作 ③自动获取某路径下文件列表 ④PIL截屏功能</strong></p> 
<pre><code>conda install scrapy
</code></pre> 
<p><img alt="" height="97" src="https://images2.imgbox.com/f2/12/9aIiblaB_o.png" width="515"></p> 
<p><strong>②输入scrapy，检测安装成功！</strong></p> 
<p><img alt="" height="579" src="https://images2.imgbox.com/dc/ba/zbyIvOVs_o.png" width="730"></p> 
<h3 id="%E4%BA%8C%E3%80%81%E5%88%9B%E5%BB%BAscrapy%E5%B7%A5%E7%A8%8B">二、创建scrapy工程</h3> 
<p><strong>1.点击pycharm左下角的terminal，在终端中输入如下命令：</strong></p> 
<pre><code class="language-python">scrapy startproject firsttest（工程名）</code></pre> 
<p><img alt="" height="318" src="https://images2.imgbox.com/97/5d/loqRG2Hw_o.png" width="1039"></p> 
<p><strong>2.创建爬虫文件</strong></p> 
<p>创建成功后，目录下就会出现创建的工程文件。</p> 
<p><img alt="" height="257" src="https://images2.imgbox.com/43/70/XoMfmFms_o.png" width="369"></p> 
<p><span style="color:#f33b45;"><strong>    —spiders文件夹叫爬虫文件夹，或爬虫目录，在spiders的子目录中要创建一个爬虫文件。相关操作如下：</strong></span></p> 
<pre><code>首先要进入该工程中：
cd firsttest
然后输入如下命令：（爬虫文件名字不能和工程名相同，后面的url可以临时写一个，在文件中还可以改）
scrapy genspider spiderName www.xxx.com</code></pre> 
<p><span style="color:#f33b45;"><strong>   </strong></span><img alt="" height="148" src="https://images2.imgbox.com/e8/f5/wsLAVhC1_o.png" width="727"></p> 
<p><span style="color:#f33b45;"><strong>    —settings.py是我们当前工程的配置文件。（经常使用）</strong></span></p> 
<p><strong>3.分析爬虫文件（first.py）</strong></p> 
<p>点开文件，有一个类，类名就是创建的文件名+Spider，该类的父类是Spider，Spider属于scrapy中的一个类，它是scrapy中所有爬虫类的父类。</p> 
<h3 id="%E2%80%8B"><img alt="" height="224" src="https://images2.imgbox.com/19/9c/u2xop2qt_o.png" width="339"></h3> 
<p>文件中有三个属性和一个方法，解释见代码：</p> 
<pre><code>import scrapy


class FirstSpider(scrapy.Spider):
    # 爬虫文件的名称：就是爬虫源文件的一个唯一标识
    name = 'first'
    # 允许的域名：用来限定start_urls列表中，哪些url可以进行请求发送。但是通常不会用它
    #allowed_domains = ['www.baidu.com']
    # 起始的url列表：该列表中存放的url会被scrapy自动进行请求的发送，可以放多个url
    start_urls = ['http://www.baidu.com/','http://www.sogou.com/']

    # 用作于数据解析：response参数表示的就是请求成功后对应的响应对象。parse调用的次数由start_urls列表中元素的个数决定。
    def parse(self, response):
        print(response)

    # 之后可以执行代码，不要直接run，要在终端中执行。见下面的命令。</code></pre> 
<h3 id="%E4%B8%89%E3%80%81%E6%89%A7%E8%A1%8C%E5%B7%A5%E7%A8%8B"><strong>三、执行工程</strong></h3> 
<p><strong>1.编写完代码后，在终端中输入如下命令，即可进行数据的爬取：</strong></p> 
<pre><code>scrapy crawl spiderName</code></pre> 
<p><img alt="" height="656" src="https://images2.imgbox.com/35/6f/QRIPjOKx_o.png" width="1200"></p> 
<p> <img alt="" height="663" src="https://images2.imgbox.com/b5/fd/Sc6JH58X_o.png" width="1200"></p> 
<p><strong>2.输出的是日志信息，如果运行中出现了错误，也会显示出来，但是输出中没有我们想要的response，原因如下：</strong></p> 
<pre><code> INFO: Overridden settings:
{'BOT_NAME': 'firsttest',
 'NEWSPIDER_MODULE': 'firsttest.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['firsttest.spiders']}
</code></pre> 
<p><span style="color:#f33b45;"> 要在settings.py文件中修改 'ROBOTSTXT_OBEY': True——&gt; 'ROBOTSTXT_OBEY': Flase</span></p> 
<p><strong>3.如果不想输出日志，可以使用如下命令，但是不能报错：</strong></p> 
<pre><code>scrapy crawl first --nolog</code></pre> 
<p><strong>4.因此，可以在配置文件中添加如下命令：</strong></p> 
<pre><code># 显示指定类型的日志信息
LOG_LEVEL = 'ERROR'</code></pre> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/37bda9ae9c9a6774524a0f14af3d650b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python语言中最基本的概念_python基本概念-关键要素</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/813737bb6e4e2ebcaf95bee6e597b8b5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【零基础学爬虫】第四章：selenium模块使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>