<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>爬虫实战：从外地天气到美食推荐，探索干饭人的世界 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="爬虫实战：从外地天气到美食推荐，探索干饭人的世界" />
<meta property="og:description" content="今天是第二堂课，我们将继续学习爬虫技术。在上一节课中，我们已经学会了如何爬取干饭教程。正如鲁迅所说（我没说过），当地吃完饭就去外地吃，这启发了我去爬取城市天气信息，并顺便了解当地美食。这个想法永远是干饭人的灵魂所在。
今天我们的目标是学习如何爬取城市天气信息，因为要计划去哪里玩耍，首先得了解天气情况。虽然我们的手机已经装有许多免费天气软件，但是也不妨碍我们学习。
在我们开始学习爬虫技术之前，首先需要找到一个容易爬取数据的天气网站。并不要求特定网站，只要易于爬取的网站即可。毕竟我们目前并不需要爬取特定网站来抢票或抢购商品，我们的主要目的是学习爬虫技术。
天气爬虫 在进行爬虫操作时，如果不确定一个网站是否易于爬取，可以先尝试输入该网站的首页地址，查看能否成功解析出HTML网页。如果解析出来的页面与实际浏览的页面一致，那么说明该网站可能没有设置反爬虫机制；反之，如果解析出来的页面与实际不同，那么该网站很可能设置了反爬虫措施。在学习阶段，建议选择较为容易爬取的网站进行练习，避免过早挑战难度过大的网站。
好的，废话不多说，我们现在就开始抓取该网站上的所有城市信息。
城市列表 天气信息肯定与城市相关，因此几乎每个天气网站都会有城市列表。让我们先来抓取这些城市列表并保存起来，以备后续使用。以下是相应的代码：
# 导入urllib库的urlopen函数 from urllib.request import urlopen,Request # 导入BeautifulSoup from bs4 import BeautifulSoup as bf headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0&#39;} req = Request(&#34;https://www.tianqi.com/chinacity.html&#34;,headers=headers) # 发出请求，获取html # 获取的html内容是字节，将其转化为字符串 html = urlopen(req) html_text = bytes.decode(html.read()) obj = bf(html_text,&#39;html.parser&#39;) # 使用find_all函数获取所有图片的信息 province_tags = obj.find_all(&#39;h2&#39;) for province_tag in province_tags: province_name = province_tag.text.strip() cities = [] print(province_name) next_sibling = province_tag." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/92bdd43db1f86d97e27ea88e4325cfdc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-14T09:34:24+08:00" />
<meta property="article:modified_time" content="2024-03-14T09:34:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">爬虫实战：从外地天气到美食推荐，探索干饭人的世界</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>今天是第二堂课，我们将继续学习爬虫技术。在上一节课中，我们已经学会了如何爬取干饭教程。正如鲁迅所说（我没说过），当地吃完饭就去外地吃，这启发了我去爬取城市天气信息，并顺便了解当地美食。这个想法永远是干饭人的灵魂所在。</p> 
<p>今天我们的目标是学习如何爬取城市天气信息，因为要计划去哪里玩耍，首先得了解天气情况。虽然我们的手机已经装有许多免费天气软件，但是也不妨碍我们学习。</p> 
<p>在我们开始学习爬虫技术之前，首先需要找到一个容易爬取数据的天气网站。并不要求特定网站，只要易于爬取的网站即可。毕竟我们目前并不需要爬取特定网站来抢票或抢购商品，我们的主要目的是学习爬虫技术。</p> 
<h2><a id="_8"></a>天气爬虫</h2> 
<p>在进行爬虫操作时，如果不确定一个网站是否易于爬取，可以先尝试输入该网站的首页地址，查看能否成功解析出HTML网页。如果解析出来的页面与实际浏览的页面一致，那么说明该网站可能没有设置反爬虫机制；反之，如果解析出来的页面与实际不同，那么该网站很可能设置了反爬虫措施。在学习阶段，建议选择较为容易爬取的网站进行练习，避免过早挑战难度过大的网站。</p> 
<p>好的，废话不多说，我们现在就开始抓取该网站上的所有城市信息。</p> 
<h3><a id="_15"></a>城市列表</h3> 
<p>天气信息肯定与城市相关，因此几乎每个天气网站都会有城市列表。让我们先来抓取这些城市列表并保存起来，以备后续使用。以下是相应的代码：</p> 
<pre><code># 导入urllib库的urlopen函数
from urllib.request import urlopen,Request
# 导入BeautifulSoup
from bs4 import BeautifulSoup as bf

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'}
req = Request("https://www.tianqi.com/chinacity.html",headers=headers)
# 发出请求，获取html
# 获取的html内容是字节，将其转化为字符串
html = urlopen(req)
html_text = bytes.decode(html.read())
obj = bf(html_text,'html.parser')
# 使用find_all函数获取所有图片的信息
province_tags = obj.find_all('h2')
for province_tag in province_tags:
    province_name = province_tag.text.strip()
    cities = []
    print(province_name)
    next_sibling = province_tag.find_next_sibling()
    city_tags = next_sibling.find_all('a')
    for city_tag in city_tags:
        city_name = city_tag.text.strip()
        cities.append(city_name)
        print(city_name)

</code></pre> 
<p>在上述操作中，主要的步骤是从城市地址页面中获取信息，对其进行解析以获取省份和城市之间的对应关系。目前仅仅进行了简单的打印输出。</p> 
<h3><a id="_50"></a>城市天气</h3> 
<p>在获取城市信息之后，接下来的步骤是根据城市信息获取天气信息。在这里，我们仅考虑直辖市的天气情况，而省份的天气信息获取相比直辖市多了一步省份的跳转。我们暂时不进行省份天气信息的演示。现在，让我们一起来看一下代码：</p> 
<pre><code># 导入urllib库的urlopen函数
from urllib.request import urlopen,Request
# 导入BeautifulSoup
from bs4 import BeautifulSoup as bf

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'}
req = Request(f"https://www.tianqi.com/beijing/",headers=headers)
# 发出请求，获取html
# 获取的html内容是字节，将其转化为字符串
html = urlopen(req)
html_text = bytes.decode(html.read())
obj = bf(html_text,'html.parser')
city_tags = obj.find_all('div',class_='mainWeather')
for city_tag in city_tags:
    a_tags = city_tag.find_all('a', class_=lambda value: value != 'd15')
    for a_tag in a_tags:
        title = a_tag.get('title')
        print(title)
foods = obj.find_all('ul',class_='paihang_good_food')
for food in foods:
    a_tags = food.find_all('a')
    for a_tag in a_tags:
        href = a_tag.get('href')
        print(href)
        title = a_tag.get('title')
        print(title)
weather_info = obj.find_all('dl', class_='weather_info')
for info in weather_info:
    city_name = info.find('h1').text
    date = info.find('dd', class_='week').text
    temperature = info.find('p', class_='now').text
    humidity = info.find('dd', class_='shidu').text
    air_quality = info.find('dd', class_='kongqi').h5.text
    print(f"地点:{city_name}")
    print(f"时间:{date}")
    print(f"当前温度:{temperature}")
    print(humidity)
    print(air_quality)   

</code></pre> 
<p>以上代码不仅仅把天气解析出来，而且将当前地址的天气和各个城区 的天气以及当地美食都解析了出来。当地美食因为链接是变动的，所以将链接和美食做了响应的映射关系保存。</p> 
<h3><a id="_99"></a>城市美食</h3> 
<p>在确定天气适宜的情况下，我们通常都会想了解当地有哪些特色美食，毕竟不能总是吃快餐，特色美食才是我们吃货的灵魂所在。</p> 
<p>以下是一个示例代码：</p> 
<pre><code># 导入urllib库的urlopen函数
from urllib.request import urlopen,Request
# 导入BeautifulSoup
from bs4 import BeautifulSoup as bf

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'}
req = Request(f"https://www.tianqi.com/meishi/1737.html",headers=headers)
# 发出请求，获取html
# 获取的html内容是字节，将其转化为字符串
html = urlopen(req)
html_text = bytes.decode(html.read())
obj = bf(html_text,'html.parser')
span_tag = obj.find('span', class_='traffic')
text_content = ''.join(span_tag.stripped_strings)
print(text_content)

</code></pre> 
<p>在这里，我主要解析了当前美食推荐的原因。实际上，链接应该与之前解析的天气信息相关联，但为了演示方便，我在示例代码中使用了固定值。</p> 
<h2><a id="_127"></a>包装一下</h2> 
<p>将以上内容单独制作成小案例确实是一种有效的方式，但将其整合成一个简单的小应用则更具实用性，因为这样可以实现更灵活的交互。让我们一起来看一下最终的代码：</p> 
<pre><code># 导入urllib库的urlopen函数
from urllib.request import urlopen,Request
import urllib,string
# 导入BeautifulSoup
from bs4 import BeautifulSoup as bf
from random import choice,sample
from colorama import init
from os import system
from termcolor import colored
from readchar import  readkey
from xpinyin import Pinyin

p = Pinyin()

city_province_mapping = []

province_sub_weather = []

good_foods = []
FGS = ['green', 'yellow', 'blue', 'cyan', 'magenta', 'red']

def clear():
    system("CLS")

def get_city_province_mapping():
    print(colored('开始搜索城市',choice(FGS)))
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'}
    req = Request("https://www.tianqi.com/chinacity.html",headers=headers)
    # 发出请求，获取html
    # 获取的html内容是字节，将其转化为字符串
    html = urlopen(req)
    html_text = bytes.decode(html.read())
    obj = bf(html_text,'html.parser')
    # 使用find_all函数获取所有图片的信息
    province_tags = obj.find_all('h2')
    for province_tag in province_tags:
        province_name = province_tag.text.strip()
        cities = []

        next_sibling = province_tag.find_next_sibling()
        city_tags = next_sibling.find_all('a')
        for city_tag in city_tags:
            city_name = city_tag.text.strip()
            cities.append(city_name)

        city_province_mapping.append((province_name,cities))

def get_province_weather(province):
    print(colored(f'已选择：{province}',choice(FGS)))
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'}
    req = Request(f"https://www.tianqi.com/{province}/",headers=headers)
    # 发出请求，获取html
    # 获取的html内容是字节，将其转化为字符串
    html = urlopen(req)
    html_text = bytes.decode(html.read())
    obj = bf(html_text,'html.parser')
    city_tags = obj.find_all('div',class_='mainWeather')
    # city_tags = obj.find_all('ul',class_='raweather760')
    province_sub_weather.clear()
    print(colored('解析主要城市中',choice(FGS)))
    for city_tag in city_tags:
        a_tags = city_tag.find_all('a', class_=lambda value: value != 'd15')
        for a_tag in a_tags:
            title = a_tag.get('title')
            province_sub_weather.append(title)
    foods = obj.find_all('ul',class_='paihang_good_food')
    print(colored('解析热搜美食中',choice(FGS)))
    for food in foods:
        a_tags = food.find_all('a')
        for a_tag in a_tags:
            href = a_tag.get('href')
            title = a_tag.get('title')
            good_foods.append((href, title))
    weather_info = obj.find_all('dl', class_='weather_info')
    print(colored('解析完毕',choice(FGS)))
    for info in weather_info:
        city_name = info.find('h1').text
        date = info.find('dd', class_='week').text
        temperature = info.find('p', class_='now').text
        humidity = info.find('dd', class_='shidu').text
        air_quality = info.find('dd', class_='kongqi').h5.text

        print(colored(f"地点:{city_name}",choice(FGS)))
        print(colored(f"时间:{date}",choice(FGS)))
        print(colored(f"当前温度:{temperature}",choice(FGS)))
        print(colored(humidity,choice(FGS)))
        print(colored(air_quality,choice(FGS)))   
def search_food(link):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'}
    req = Request(f"https://www.tianqi.com{link}",headers=headers)
    # 发出请求，获取html
    # 获取的html内容是字节，将其转化为字符串
    html = urlopen(req)
    html_text = bytes.decode(html.read())
    obj = bf(html_text,'html.parser')
    span_tag = obj.find('span', class_='traffic')
    text_content = ''.join(span_tag.stripped_strings)
    print(colored(text_content,choice(FGS)))

def print_menu():
    for i in range(0, 4, 3):
        names = [f'{i + j}:{city_province_mapping[i + j][0]}' for j in range(3) if i + j &lt; 4]
        print(colored('\t\t'.join(names),choice(FGS)))

def print_food():
    if not good_foods:
        print(colored('请选择城市，才可查看',choice(FGS)))
        return
    for i in range(0, len(good_foods), 3):
        names = [f'{i + j}:{good_foods[i + j][1]}' for j in range(3) if i + j &lt; len(good_foods)]
        print(colored('\t\t'.join(names),choice(FGS)))

def print_hot(weather):
    if not weather:
        print(colored('请选择城市，才可查看',choice(FGS)))
        return
    for i in range(0,len(weather), 3):
        names = [f'{i + j}:{weather[i + j]}' for j in range(3) if i + j &lt; len(weather)]
        print(colored('\t\t'.join(names),choice(FGS)))

get_city_province_mapping()

# get_province_weather('beijing')
# search_food(good_foods[1][0])
init() ## 命令行输出彩色文字
print(colored('已搜索完毕！',choice(FGS)))
print(colored('m:返回首页',choice(FGS)))
print(colored('h:查看当前城区天气',choice(FGS)))
print(colored('f:查看当地美食',choice(FGS)))
print(colored('q:退出天气',choice(FGS)))
my_key = ['q','m','c','h','f']
while True:
    while True:
        move = readkey()
        if move in my_key:
            break
    if move == 'q': ## 键盘‘Q’是退出
        break 
    if move == 'c': ## 键盘‘C’是清空控制台
        clear()
    if move == 'h':  
        print_hot(province_sub_weather)
    if move == 'f':  
        print_food()
        num = int(input('请输入美食编号：=====&gt;'))
        if num &lt;= len(good_foods):
            search_food(good_foods[num][0])
    if move == 'm':
        print_menu()
        num = int(input('请输入城市编号：=====&gt;'))
        if num &lt;= len(city_province_mapping):
            pinyin_without_tone = p.get_pinyin(city_province_mapping[num][0],'')
            get_province_weather(pinyin_without_tone)

</code></pre> 
<p>按照我的习惯，我通常喜欢在控制台中进行打印输出，这样可以避免不必要的UI依赖。虽然整个过程并不算太复杂，但解析数据确实需要花费一些时间。尽管如此，还是成功完成了天气信息的爬取任务。</p> 
<h2><a id="_291"></a>总结</h2> 
<p>在今天的学习中，所涉及的知识点基本延续了上一次的内容，并没有太多新的拓展。主要是对网页进行解析，提取信息并保存，最后根据这些信息来动态改变链接地址，最终完成了一个简单的与用户交互的演示项目。我希望你也能跟着动手实践，尽管这个过程可能会有些痛苦，不过虽然并没有给你的技术水平带来实质性提升，但至少可以拓展你的技术广度。</p> 
<center> 
 <b>---------------------------END--------------------------- </b> 
</center> 
<h3><a id="_304"></a>题外话</h3> 
<p>感谢你能看到最后，给大家准备了一些福利！</p> 
<p><font face="幼圆" size="4" color="red">感兴趣的小伙伴，赠送全套Python学习资料，包含面试题、简历资料等具体看下方。<br> </font></p> 
<img src="https://images2.imgbox.com/0d/7c/7A3rdXAj_o.png"> 
<p>👉<font color="red">CSDN大礼包🎁：</font><a href="https://blog.csdn.net/weixin_68789096/article/details/132275547?spm=1001.2014.3001.5502">全网最全《Python学习资料》免费赠送🆓！</a><font color="#66cc66">（安全链接，放心点击）</font></p> 
<p><strong>一、Python所有方向的学习路线</strong></p> 
<p>Python所有方向的技术点做的整理，形成各个领域的知识点汇总，它的用处就在于，你可以按照下面的知识点去找对应的学习资源，保证自己学得较为全面。</p> 
<p><img src="https://images2.imgbox.com/c1/d7/1TqhQpVB_o.png" alt="img"></p> 
<p><strong>二、Python兼职渠道推荐</strong>*</p> 
<p>学的同时助你创收，每天花1-2小时兼职，轻松稿定生活费.<br> <img src="https://images2.imgbox.com/be/23/MvI3ySzJ_o.png" alt="在这里插入图片描述"></p> 
<p><strong>三、最新Python学习笔记</strong></p> 
<p>当我学到一定基础，有自己的理解能力的时候，会去阅读一些前辈整理的书籍或者手写的笔记资料，这些笔记详细记载了他们对一些技术点的理解，这些理解是比较独到，可以学到不一样的思路。</p> 
<p><img src="https://images2.imgbox.com/22/e9/R1Q6oKMj_o.png" alt="img"></p> 
<p><strong>四、实战案例</strong></p> 
<p>纸上得来终觉浅，要学会跟着视频一起敲，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。</p> 
<p><img src="https://images2.imgbox.com/32/a4/40NR7Stv_o.png" alt="img"></p> 
<img src="https://images2.imgbox.com/63/1b/F0Ignnkc_o.png"> 
<p>👉<font color="red">CSDN大礼包🎁：</font><a href="https://blog.csdn.net/weixin_68789096/article/details/132275547?spm=1001.2014.3001.5502">全网最全《Python学习资料》免费赠送🆓！</a><font color="#66cc66">（安全链接，放心点击）</font></p> 
<p>若有侵权，请联系删除</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/806297c251f03cddeaa3b2298e918fbf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">生产实践｜腾讯欧拉平台数据血缘架构</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c759a400b19a070d617d979b686807be/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">大米自动化生产线的运作原理与科技创新</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>