<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>如何避免Selenium爬虫被网站识破 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="如何避免Selenium爬虫被网站识破" />
<meta property="og:description" content="在对一些需要进行登陆操作的网站爬取时，通常都会使用到Selenium。但是Selenium爬虫在爬取数据时也是会被网站检测到，这是因为Selenium模拟了浏览器行为，而相对于真实用户的浏览器，Selenium模拟无法识别JavaScript代码和CSS文件。此外，网站也可能通过检测请求头、IP地址、Cookie等信息来判断是否是爬虫。
接下来我们就详细的来了解下这些原因是怎么产生的。
1、请求头信息：一般情况下Selenium提供的请求头信息与正常的浏览器请求头略有不同，所以需要自定义User-Agent字段让请求头更像正常的浏览器。
2、IP封禁或限制访问：如果频繁使用同一个IP地址进行数据爬取，服务器可能会将该IP地址视为恶意IP并加入黑名单。
3、Cookie验证：部分网站可能会在登录后，在后续的每个请求中都要求携带相关的Cookie信息。
4、页面加载速度：如果程序访问频率过高，页面加载时间却显示异常迅速，网站越是往后就越容易加强反爬虫措施了。
以下就是针对上述问题而提供的解决方式：
1、更换User-Agent：可以在每个请求中使用不同的User-Agent字段，避免与其他请求相同IP和头部参数给网站接口留下“爬虫”的印象。
2、设置合理的间隔时间：通过设置适当间隔（如访问网站后1-5秒内不再访问），以模拟真实用户的行为，减少被检测到的概率。
3、使用代理IP：代理IP可以隐藏你的真实IP地址，但是如果时间较长或请求次数太频繁，代理IP也有被风控的危险，所以尽量使用更多，质量更好的IP池。
以下是一段使用代理IP的Selenium爬虫示例代码，可以在每次请求时更换一个随机代理IP，这种就是隧道转发模式的代理，需要注意的是，要使用合法、高质量的代理IP服务提供商比如亿牛云代理，以避免安全和质量问题。
from selenium import webdriver import string import zipfile # 代理服务器(产品官网 www.16yun.cn) proxyHost = &#34;t.16yun.cn&#34; proxyPort = &#34;3111&#34; # 代理验证信息 proxyUser = &#34;username&#34; proxyPass = &#34;password&#34; def create_proxy_auth_extension(proxy_host, proxy_port, proxy_username, proxy_password, scheme=&#39;http&#39;, plugin_path=None): if plugin_path is None: plugin_path = r&#39;/tmp/{}_{}@t.16yun.zip&#39;.format(proxy_username, proxy_password) manifest_json = &#34;&#34;&#34; { &#34;version&#34;: &#34;1.0.0&#34;, &#34;manifest_version&#34;: 2, &#34;name&#34;: &#34;16YUN Proxy&#34;, &#34;permissions&#34;: [ &#34;proxy&#34;, &#34;tabs&#34;, &#34;unlimitedStorage&#34;, &#34;storage&#34;, &#34;&lt;all_urls&gt;&#34;, &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/a3d53354182864fb8c0f63e370e9ab70/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-31T16:12:23+08:00" />
<meta property="article:modified_time" content="2023-05-31T16:12:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">如何避免Selenium爬虫被网站识破</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>在对一些需要进行登陆操作的网站爬取时，通常都会使用到Selenium。但是Selenium爬虫在爬取数据时也是会被网站检测到，这是因为Selenium模拟了浏览器行为，而相对于真实用户的浏览器，Selenium模拟无法识别JavaScript代码和CSS文件。此外，网站也可能通过检测请求头、IP地址、Cookie等信息来判断是否是爬虫。<br> 接下来我们就详细的来了解下这些原因是怎么产生的。<br> 1、请求头信息：一般情况下Selenium提供的请求头信息与正常的浏览器请求头略有不同，所以需要自定义User-Agent字段让请求头更像正常的浏览器。<br> 2、IP封禁或限制访问：如果频繁使用同一个IP地址进行数据爬取，服务器可能会将该IP地址视为恶意IP并加入黑名单。<br> 3、Cookie验证：部分网站可能会在登录后，在后续的每个请求中都要求携带相关的Cookie信息。<br> 4、页面加载速度：如果程序访问频率过高，页面加载时间却显示异常迅速，网站越是往后就越容易加强反爬虫措施了。<br> 以下就是针对上述问题而提供的解决方式：<br> 1、更换User-Agent：可以在每个请求中使用不同的User-Agent字段，避免与其他请求相同IP和头部参数给网站接口留下“爬虫”的印象。<br> 2、设置合理的间隔时间：通过设置适当间隔（如访问网站后1-5秒内不再访问），以模拟真实用户的行为，减少被检测到的概率。<br> 3、使用代理IP：代理IP可以隐藏你的真实IP地址，但是如果时间较长或请求次数太频繁，代理IP也有被风控的危险，所以尽量使用更多，质量更好的IP池。<br> 以下是一段使用代理IP的Selenium爬虫示例代码，可以在每次请求时更换一个随机代理IP，这种就是隧道转发模式的代理，需要注意的是，要使用合法、高质量的代理IP服务提供商比如亿牛云代理，以避免安全和质量问题。</p> 
<pre><code>    from selenium import webdriver
    import string
    import zipfile

    # 代理服务器(产品官网 www.16yun.cn)
    proxyHost = "t.16yun.cn"
    proxyPort = "3111"

    # 代理验证信息
    proxyUser = "username"
    proxyPass = "password"


    def create_proxy_auth_extension(proxy_host, proxy_port,
                                    proxy_username, proxy_password,
                                    scheme='http', plugin_path=None):
        if plugin_path is None:
            plugin_path = r'/tmp/{}_{}@t.16yun.zip'.format(proxy_username, proxy_password)

        manifest_json = """
        {
            "version": "1.0.0",
            "manifest_version": 2,
            "name": "16YUN Proxy",
            "permissions": [
                "proxy",
                "tabs",
                "unlimitedStorage",
                "storage",
                "&lt;all_urls&gt;",
                "webRequest",
                "webRequestBlocking"
            ],
            "background": {
                "scripts": ["background.js"]
            },
            "minimum_chrome_version":"22.0.0"
        }
        """

        background_js = string.Template(
            """
            var config = {
                mode: "fixed_servers",
                rules: {
                    singleProxy: {
                        scheme: "${scheme}",
                        host: "${host}",
                        port: parseInt(${port})
                    },
                    bypassList: ["localhost"]
                }
              };

            chrome.proxy.settings.set({value: config, scope: "regular"}, function() {});

            function callbackFn(details) {
                return {
                    authCredentials: {
                        username: "${username}",
                        password: "${password}"
                    }
                };
            }

            chrome.webRequest.onAuthRequired.addListener(
                callbackFn,
                {urls: ["&lt;all_urls&gt;"]},
                ['blocking']
            );
            """
        ).substitute(
            host=proxy_host,
            port=proxy_port,
            username=proxy_username,
            password=proxy_password,
            scheme=scheme,
        )
        print(background_js)

        with zipfile.ZipFile(plugin_path, 'w') as zp:
            zp.writestr("manifest.json", manifest_json)
            zp.writestr("background.js", background_js)

        return plugin_path


    proxy_auth_plugin_path = create_proxy_auth_extension(
        proxy_host=proxyHost,
        proxy_port=proxyPort,
        proxy_username=proxyUser,
        proxy_password=proxyPass)

    option = webdriver.ChromeOptions()

    option.add_argument("--start-maximized")

    # 如报错 chrome-extensions
    # option.add_argument("--disable-extensions")

    option.add_extension(proxy_auth_plugin_path)

    # 关闭webdriver的一些标志
    # option.add_experimental_option('excludeSwitches', ['enable-automation'])

    driver = webdriver.Chrome(
        chrome_options=option,
        executable_path="./chromdriver"
    )

    # 修改webdriver get属性
    # script = '''
    # Object.defineProperty(navigator, 'webdriver', {
    # get: () =&gt; undefined
    # })
    # '''
    # driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {"source": script})


    driver.get("https://httpbin.org/ip")

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d3891fa2799db20a4964fab8d399ab74/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android开发基础</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5e744423b92fb9d1148a3aa71288e995/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【毕业设计】基于springboot &#43; vue微信小程序商城</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>