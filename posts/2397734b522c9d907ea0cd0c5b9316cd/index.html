<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>笔记｜李沐-动手学习机器学习｜现代卷积神经网络（视频24-30） - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="笔记｜李沐-动手学习机器学习｜现代卷积神经网络（视频24-30）" />
<meta property="og:description" content="笔记｜李沐-动手学习机器学习｜现代卷积神经网络（视频24-30） AlexNet (视频24）主要改进：AlexNet架构AlexNet复杂度d2l代码btw VGG主要改进：VGG块从lenet到VGG 网络中的网络NiN主要改进：NiN块架构代码 含并行连接的网络GoogLeNet（视频27）回顾主要改进：Inception块GoogLeNet 批量归一化（视频28）批量规范层的从零实现批量规范层的框架实现课程QA搬运 ResNet残差块（Residual Block）ResNet块Residual到底是怎么处理梯度消失，训练到1000层的？ Kaggle竞赛 AlexNet (视频24） 随着数据量增大，可以用更深的网络去挖掘数据里更加复杂的信息。
AlexNet赢得了ImageNet数据集比赛的优胜。而其实AlexNet可以理解为一个更加复杂更大的LeNet。
主要改进： 丢弃法ReLuMaxPooling 计算机视觉方法论的改变（CV主要关心如何进行特征提取）
不再那么关注人工特征提取，而是端到端地学习
AlexNet架构 第一个卷积层
第二个卷积层以及第3、4、5个卷积层（比LeNet多了3个卷积层）
两个隐藏层&#43;一个输出层
AlexNet的两个隐藏层显然大很多
更多细节（这些细节有助于更好地训练网络）
激活函数由Sigmoid变为ReLU两个Dense（4096）之后加入了丢弃层数据增强（卷积对于位置（光亮）敏感，所以在输入数据中输入更多的变种） AlexNet复杂度 AlexNet可学习参数的个数上多了10倍左右
d2l代码 d2l用的数据集是灰度图像数据集Fashion-MNIST，所以代码中的
通道量是1而非3；resize = 224来模拟ImageNet数据集对图片像素大小； 李沐老师的：
训练速度比lenet慢了。4190.6 examples/sec on cuda
btw 从代码上看alexnet和lenet没什么区别，但就是这些，跨度了20年。绝大部分的cnn论文想要证明自己的模型效果好，还是会使用ImageNet数据集。 VGG 前面AlexNet的最大问题是，他虽然是胖版的LeNet，但是很不规则，比如当我们想要进一步加深加大网络结构，不知道具体应该如何设计。所以需要整个框架要有更好的设计思路，这就是VGG在干的事情。
主要改进：VGG块 核心思想：使用可重复的块来配置深度神经网络
用大量3*3的卷积层形成一个块，然后多个块堆叠产生最后的网络。
使用VGG块的数量不同，得到VGG-11(包含8个卷积层，3个全连接层)VGG-16、VGG-19…
圆点的大小越大，占用的内存越多
从lenet到VGG 网络中的网络NiN （现在用的不多，但是有提出很多重要的概念）
前面的网络，在最后都使用了很大的全连接层，a)会占用很大的空间，b)占用很大的计算带宽，c)还容易带来过拟合（这一层就把所有参数给学习掉了）。
主要改进：NiN块 这两个1x1的卷积层，其实是相当于全连接层。唯一的作用就是对通道进行了一下融合。
架构 代码 在这里还看不太出来1x1卷积有什么好处。但是在googlenet上体现的更明显。
含并行连接的网络GoogLeNet（视频27） 回顾 主要改进：Inception块 Input和Output高宽不变，通道数叠加
蓝色的块才是真的用来提取特征信息的
inception块不仅增加了多样性（大量不同设置的卷积层），而且参数变少了
GoogLeNet 批量归一化（视频28） 之前存在的问题：顶部变化快，底部变化慢。但是底部变了，顶部也要变，从而导致收敛变慢。
回顾：均值和方差在每一层都会变化
批量归一化的思想：固定小批量的均值和方差，然后再做额外的调整
批量规范层的从零实现 批量规范层的框架实现 为了更好理解如何[应用BatchNorm]，下面我们将其应用(于LeNet模型)（ :numref:sec_lenet）。 回想一下，批量规范化是在卷积层或全连接层之后、相应的激活函数之前应用的。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/2397734b522c9d907ea0cd0c5b9316cd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-23T15:16:27+08:00" />
<meta property="article:modified_time" content="2022-10-23T15:16:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">笔记｜李沐-动手学习机器学习｜现代卷积神经网络（视频24-30）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>笔记｜李沐-动手学习机器学习｜现代卷积神经网络（视频24-30）</h4> 
 <ul><li><a href="#AlexNet_24_2" rel="nofollow">AlexNet (视频24）</a></li><li><ul><li><a href="#_6" rel="nofollow">主要改进：</a></li><li><a href="#AlexNet_14" rel="nofollow">AlexNet架构</a></li><li><a href="#AlexNet_30" rel="nofollow">AlexNet复杂度</a></li><li><a href="#d2l_37" rel="nofollow">d2l代码</a></li><li><a href="#btw_47" rel="nofollow">btw</a></li></ul> 
  </li><li><a href="#VGG_53" rel="nofollow">VGG</a></li><li><ul><li><a href="#VGG_56" rel="nofollow">主要改进：VGG块</a></li><li><a href="#lenetVGG_65" rel="nofollow">从lenet到VGG</a></li></ul> 
  </li><li><a href="#NiN_68" rel="nofollow">网络中的网络NiN</a></li><li><ul><li><a href="#NiN_72" rel="nofollow">主要改进：NiN块</a></li><li><a href="#_76" rel="nofollow">架构</a></li><li><a href="#_80" rel="nofollow">代码</a></li></ul> 
  </li><li><a href="#GoogLeNet27_84" rel="nofollow">含并行连接的网络GoogLeNet（视频27）</a></li><li><ul><li><a href="#_86" rel="nofollow">回顾</a></li><li><a href="#Inception_88" rel="nofollow">主要改进：Inception块</a></li><li><a href="#GoogLeNet_95" rel="nofollow">GoogLeNet</a></li></ul> 
  </li><li><a href="#28_97" rel="nofollow">批量归一化（视频28）</a></li><li><ul><li><a href="#_103" rel="nofollow">批量规范层的从零实现</a></li><li><a href="#_106" rel="nofollow">批量规范层的框架实现</a></li><li><a href="#QA_111" rel="nofollow">课程QA搬运</a></li></ul> 
  </li><li><a href="#ResNet_130" rel="nofollow">ResNet</a></li><li><ul><li><a href="#Residual_Block_135" rel="nofollow">残差块（Residual Block）</a></li><li><a href="#ResNet_138" rel="nofollow">ResNet块</a></li><li><a href="#Residual1000_140" rel="nofollow">Residual到底是怎么处理梯度消失，训练到1000层的？</a></li></ul> 
  </li><li><a href="#Kaggle_144" rel="nofollow">Kaggle竞赛</a></li></ul> 
</div> 
<p></p> 
<h2><a id="AlexNet_24_2"></a>AlexNet (视频24）</h2> 
<p>随着数据量增大，可以用更深的网络去挖掘数据里更加复杂的信息。<br> AlexNet赢得了ImageNet数据集比赛的优胜。而其实AlexNet可以理解为一个更加复杂更大的LeNet。</p> 
<h3><a id="_6"></a>主要改进：</h3> 
<ul><li>丢弃法</li><li>ReLu</li><li>MaxPooling</li></ul> 
<p><strong>计算机视觉方法论的改变（CV主要关心如何进行特征提取）</strong><br> 不再那么关注人工特征提取，而是端到端地学习<br> <img src="https://images2.imgbox.com/9b/be/FoMUWdbZ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="AlexNet_14"></a>AlexNet架构</h3> 
<p>第一个卷积层<br> <img src="https://images2.imgbox.com/7d/51/DGYWPBEc_o.png" alt="在这里插入图片描述"><br> 第二个卷积层以及第3、4、5个卷积层（比LeNet多了3个卷积层）<br> <img src="https://images2.imgbox.com/ee/e7/ckfY7CWF_o.png" alt="在这里插入图片描述"><br> 两个隐藏层+一个输出层<br> AlexNet的两个隐藏层显然大很多<br> <img src="https://images2.imgbox.com/74/9b/CIvy9ojb_o.png" alt="在这里插入图片描述"></p> 
<p>更多细节（这些细节有助于更好地训练网络）</p> 
<ul><li>激活函数由Sigmoid变为ReLU</li><li>两个Dense（4096）之后加入了丢弃层</li><li>数据增强（卷积对于位置（光亮）敏感，所以在输入数据中输入更多的变种）</li></ul> 
<p><img src="https://images2.imgbox.com/2a/f6/SjYnnmgD_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="AlexNet_30"></a>AlexNet复杂度</h3> 
<p><img src="https://images2.imgbox.com/11/43/99cTfzQu_o.png" alt="在这里插入图片描述"></p> 
<p>AlexNet可学习参数的个数上多了10倍左右</p> 
<h3><a id="d2l_37"></a>d2l代码</h3> 
<p>d2l用的数据集是灰度图像数据集Fashion-MNIST，所以代码中的</p> 
<ul><li>通道量是1而非3；</li><li>resize = 224来模拟ImageNet数据集对图片像素大小；</li></ul> 
<p>李沐老师的：<br> <img src="https://images2.imgbox.com/18/2a/RaCIJlbS_o.png" alt="在这里插入图片描述"><br> 训练速度比lenet慢了。4190.6 examples/sec on cuda</p> 
<h3><a id="btw_47"></a>btw</h3> 
<ul><li>从代码上看alexnet和lenet没什么区别，但就是这些，跨度了20年。</li><li>绝大部分的cnn论文想要证明自己的模型效果好，还是会使用ImageNet数据集。</li></ul> 
<h2><a id="VGG_53"></a>VGG</h2> 
<p>前面AlexNet的最大问题是，他虽然是胖版的LeNet，但是很不规则，比如当我们想要进一步加深加大网络结构，不知道具体应该如何设计。所以需要整个框架要有更好的设计思路，这就是VGG在干的事情。</p> 
<h3><a id="VGG_56"></a>主要改进：VGG块</h3> 
<p><strong>核心思想：使用可重复的块来配置深度神经网络</strong><br> 用大量3*3的卷积层形成一个块，然后多个块堆叠产生最后的网络。<img src="https://images2.imgbox.com/b8/23/bZqzr3z4_o.png" alt="在这里插入图片描述"><br> 使用VGG块的数量不同，得到VGG-11(包含8个卷积层，3个全连接层)VGG-16、VGG-19…</p> 
<p><img src="https://images2.imgbox.com/a3/87/juZu3SKS_o.png" alt="在这里插入图片描述"></p> 
<p>圆点的大小越大，占用的内存越多</p> 
<h3><a id="lenetVGG_65"></a>从lenet到VGG</h3> 
<p><img src="https://images2.imgbox.com/d6/70/QM7Er494_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="NiN_68"></a>网络中的网络NiN</h2> 
<p>（现在用的不多，但是有提出很多重要的概念）<br> 前面的网络，在最后都使用了很大的全连接层，a)会占用很大的空间，b)占用很大的计算带宽，c)还容易带来过拟合（这一层就把所有参数给学习掉了）。</p> 
<h3><a id="NiN_72"></a>主要改进：NiN块</h3> 
<p><img src="https://images2.imgbox.com/75/b3/sM0N2Sqx_o.png" alt="在这里插入图片描述"><br> 这两个1x1的卷积层，其实是相当于全连接层。唯一的作用就是对通道进行了一下融合。</p> 
<h3><a id="_76"></a>架构</h3> 
<p><img src="https://images2.imgbox.com/7b/a6/3j04YP0k_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/49/31/lzMfCCQ5_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_80"></a>代码</h3> 
<p>在这里还看不太出来1x1卷积有什么好处。但是在googlenet上体现的更明显。</p> 
<h2><a id="GoogLeNet27_84"></a>含并行连接的网络GoogLeNet（视频27）</h2> 
<p><img src="https://images2.imgbox.com/eb/d0/7uoLyz8i_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_86"></a>回顾</h3> 
<h3><a id="Inception_88"></a>主要改进：Inception块</h3> 
<p>Input和Output高宽不变，通道数叠加<br> 蓝色的块才是真的用来提取特征信息的</p> 
<p>inception块不仅增加了多样性（大量不同设置的卷积层），而且参数变少了</p> 
<h3><a id="GoogLeNet_95"></a>GoogLeNet</h3> 
<h2><a id="28_97"></a>批量归一化（视频28）</h2> 
<p>之前存在的问题：顶部变化快，底部变化慢。但是底部变了，顶部也要变，从而导致收敛变慢。<br> 回顾：均值和方差在每一层都会变化<br> 批量归一化的思想：固定小批量的均值和方差，然后再做额外的调整</p> 
<h3><a id="_103"></a>批量规范层的从零实现</h3> 
<p><img src="https://images2.imgbox.com/de/9e/c9C9Zxh7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/81/55/nFrAFDdE_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_106"></a>批量规范层的框架实现</h3> 
<p>为了更好理解如何[应用BatchNorm]，下面我们将其应用(于LeNet模型)（ :numref:sec_lenet）。 回想一下，批量规范化是在卷积层或全连接层之后、相应的激活函数之前应用的。</p> 
<p><img src="https://images2.imgbox.com/e5/32/HPTKJQ2k_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="QA_111"></a>课程QA搬运</h3> 
<p>理解“作用在通道上”<br> 问题讨论<br> 1、 批量归一化和之前讲到Xavier时到Normalization有什么区别呢？<br> 之前的normalization可以说是在网络初始的地方对网络的宽度等进行限制，但是不会影响深层的网络。而批量归一化就是能够通过在中间层网络的前面/后面添加一个批量归一化层，来让整个网络都保持更强的稳定性，从而加快收敛的速度。</p> 
<p>2、批量归一化和权重衰弱<br> 批量归一化不会影响前一层的权重。</p> 
<p>3、 BN为什能让收敛时间变短？<br> 实际上BN会使得每层的梯度变大一点，并且差值不大，这样可以选择更大的学习率，从而权重的更新变快，收敛时间就会变短。</p> 
<p>4、 <img src="https://images2.imgbox.com/5d/de/AfMXWXeA_o.png" alt="在这里插入图片描述"><br> 都挺重要，都不能忽略。做实验室肯定都会去看一看，但是遍历比较对于一般人来说成本太高，没有严格衡量的必要。开放框架的时候什么的肯定是需要严格对比，而且是需要投入大量的时间和算力去进行的。<br> epoch、batch size和 learning rate三者相关<br> 根据内存调整batch size<br> 然后调lr<br> 最后epoch（一开始可以设大点，然后如果发现收敛了可以停掉，下次就知道epoch大概要设置为多少轮了）</p> 
<h2><a id="ResNet_130"></a>ResNet</h2> 
<p>如果说卷积神经网络里只能学一个，就学他。<br> 思想来源：<br> 核心思想：让你在加深层的时候模型永远不会变差</p> 
<h3><a id="Residual_Block_135"></a>残差块（Residual Block）</h3> 
<p><img src="https://images2.imgbox.com/f4/0f/i0r8FT92_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="ResNet_138"></a>ResNet块</h3> 
<h3><a id="Residual1000_140"></a>Residual到底是怎么处理梯度消失，训练到1000层的？</h3> 
<p><img src="https://images2.imgbox.com/0e/42/8ELZ7ZcE_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="Kaggle_144"></a>Kaggle竞赛</h2> 
<p>这个题目，下周安排一下<br> https://www.kaggle.com/c/classify-leaves</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/369d1740104cbd40902090a359adf095/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">STM32使用外部中断控制led灯亮灭</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8bdaedeb2e00e1f8a0a014907e0c5093/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Web3.0是什么？带你解析Web3.0</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>