<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>中文文本分类（基于Pytorch） - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="中文文本分类（基于Pytorch）" />
<meta property="og:description" content="中文文本分类，TextCNN，TextRNN，FastText，TextRCNN，BiLSTM_Attention, DPCNN, Transformer, 基于pytorch
介绍 我从THUCNews中抽取了20万条新闻标题，文本长度在20到30之间。一共10个类别，每类2万条。
以字为单位输入模型，使用了预训练词向量：搜狗新闻 Word&#43;Character 300d。
类别：财经、房产、股票、教育、科技、社会、时政、体育、游戏、娱乐。
数据集数据量训练集18万验证集1万测试集1万 更换自己的数据集 如果用字，按照我数据集的格式来格式化你的数据。如果用词，提前分好词，词之间用空格隔开，python run.py --model TextCNN --word True使用预训练词向量：utils.py的main函数可以提取词表对应的预训练词向量。 效果 模型acc备注TextCNN91.22%Kim 2014 经典的CNN文本分类TextRNN91.12%BiLSTMTextRNN_Att90.90%BiLSTM&#43;AttentionTextRCNN91.54%BiLSTM&#43;池化FastText92.23%bow&#43;bigram&#43;trigram， 效果出奇的好DPCNN91.25%深层金字塔CNNTransformer89.91%效果较差bert94.83%bert &#43; fcERNIE94.61%比bert略差(说好的中文碾压bert呢) 使用说明 # 训练并测试： # TextCNN python run.py --model TextCNN # TextRNN python run.py --model TextRNN # TextRNN_Att python run.py --model TextRNN_Att # TextRCNN python run.py --model TextRCNN # FastText, embedding层是随机初始化的 python run.py --model FastText --embedding random # DPCNN python run.py --model DPCNN # Transformer python run." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/01544ef42ea85a06113151a6b5c6776b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-14T09:51:27+08:00" />
<meta property="article:modified_time" content="2022-09-14T09:51:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">中文文本分类（基于Pytorch）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>中文文本分类，TextCNN，TextRNN，FastText，TextRCNN，BiLSTM_Attention, DPCNN, Transformer, 基于pytorch</p> 
<h3>介绍</h3> 
<p>我从<a href="https://link.zhihu.com/?target=http%3A//thuctc.thunlp.org/" rel="nofollow" title="THUCNews">THUCNews</a>中抽取了20万条新闻标题，文本长度在20到30之间。一共10个类别，每类2万条。</p> 
<p>以字为单位输入模型，使用了预训练词向量：<a href="https://link.zhihu.com/?target=https%3A//github.com/Embedding/Chinese-Word-Vectors" rel="nofollow" title="搜狗新闻 Word+Character 300d">搜狗新闻 Word+Character 300d</a>。</p> 
<p>类别：财经、房产、股票、教育、科技、社会、时政、体育、游戏、娱乐。</p> 
<table><thead><tr><th>数据集</th><th>数据量</th></tr></thead><tbody><tr><td>训练集</td><td>18万</td></tr><tr><td>验证集</td><td>1万</td></tr><tr><td>测试集</td><td>1万</td></tr></tbody></table> 
<h4>更换自己的数据集</h4> 
<ul><li>如果用字，按照我数据集的格式来格式化你的数据。</li><li>如果用词，提前分好词，词之间用空格隔开，<code>python run.py --model TextCNN --word True</code></li><li>使用预训练词向量：utils.py的main函数可以提取词表对应的预训练词向量。</li></ul> 
<h3>效果</h3> 
<table><thead><tr><th>模型</th><th>acc</th><th>备注</th></tr></thead><tbody><tr><td>TextCNN</td><td>91.22%</td><td>Kim 2014 经典的CNN文本分类</td></tr><tr><td>TextRNN</td><td>91.12%</td><td>BiLSTM</td></tr><tr><td>TextRNN_Att</td><td>90.90%</td><td>BiLSTM+Attention</td></tr><tr><td>TextRCNN</td><td>91.54%</td><td>BiLSTM+池化</td></tr><tr><td>FastText</td><td>92.23%</td><td>bow+bigram+trigram， 效果出奇的好</td></tr><tr><td>DPCNN</td><td>91.25%</td><td>深层金字塔CNN</td></tr><tr><td>Transformer</td><td>89.91%</td><td>效果较差</td></tr><tr><td>bert</td><td>94.83%</td><td>bert + fc</td></tr><tr><td>ERNIE</td><td>94.61%</td><td>比bert略差(说好的中文碾压bert呢)</td></tr></tbody></table> 
<p> </p> 
<h3>使用说明</h3> 
<pre><code># 训练并测试：
# TextCNN
python run.py --model TextCNN

# TextRNN
python run.py --model TextRNN

# TextRNN_Att
python run.py --model TextRNN_Att

# TextRCNN
python run.py --model TextRCNN

# FastText, embedding层是随机初始化的
python run.py --model FastText --embedding random 

# DPCNN
python run.py --model DPCNN

# Transformer
python run.py --model Transformer
</code></pre> 
<h4>参数</h4> 
<p>模型都在models目录下，超参定义和模型定义在同一文件中。</p> 
<p>models下文件</p> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/models/DPCNN.py" title="DPCNN.py">DPCNN.py</a></p> 
<pre><code class="hljs"># coding: UTF-8
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'DPCNN'
        self.train_path = dataset + '/data/train.txt'                                # 训练集
        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集
        self.test_path = dataset + '/data/test.txt'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 20                                            # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度
        self.num_filters = 250                                          # 卷积核数量(channels数)


'''Deep Pyramid Convolutional Neural Networks for Text Categorization'''


class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        if config.embedding_pretrained is not None:
            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)
        else:
            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)
        self.conv_region = nn.Conv2d(1, config.num_filters, (3, config.embed), stride=1)
        self.conv = nn.Conv2d(config.num_filters, config.num_filters, (3, 1), stride=1)
        self.max_pool = nn.MaxPool2d(kernel_size=(3, 1), stride=2)
        self.padding1 = nn.ZeroPad2d((0, 0, 1, 1))  # top bottom
        self.padding2 = nn.ZeroPad2d((0, 0, 0, 1))  # bottom
        self.relu = nn.ReLU()
        self.fc = nn.Linear(config.num_filters, config.num_classes)

    def forward(self, x):
        x = x[0]
        x = self.embedding(x)
        x = x.unsqueeze(1)  # [batch_size, 250, seq_len, 1]
        x = self.conv_region(x)  # [batch_size, 250, seq_len-3+1, 1]

        x = self.padding1(x)  # [batch_size, 250, seq_len, 1]
        x = self.relu(x)
        x = self.conv(x)  # [batch_size, 250, seq_len-3+1, 1]
        x = self.padding1(x)  # [batch_size, 250, seq_len, 1]
        x = self.relu(x)
        x = self.conv(x)  # [batch_size, 250, seq_len-3+1, 1]
        while x.size()[2] &gt; 2:
            x = self._block(x)
        x = x.squeeze()  # [batch_size, num_filters(250)]
        x = self.fc(x)
        return x

    def _block(self, x):
        x = self.padding2(x)
        px = self.max_pool(x)

        x = self.padding1(px)
        x = F.relu(x)
        x = self.conv(x)

        x = self.padding1(x)
        x = F.relu(x)
        x = self.conv(x)

        # Short Cut
        x = x + px
        return </code></pre> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/models/FastText.py" title="FastText.py">FastText.py</a></p> 
<pre><code class="hljs"># coding: UTF-8
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'FastText'
        self.train_path = dataset + '/data/train.txt'                                # 训练集
        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集
        self.test_path = dataset + '/data/test.txt'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 20                                            # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度
        self.hidden_size = 256                                          # 隐藏层大小
        self.n_gram_vocab = 250499                                      # ngram 词表大小


'''Bag of Tricks for Efficient Text Classification'''


class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        if config.embedding_pretrained is not None:
            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)
        else:
            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)
        self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed)
        self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed)
        self.dropout = nn.Dropout(config.dropout)
        self.fc1 = nn.Linear(config.embed * 3, config.hidden_size)
        # self.dropout2 = nn.Dropout(config.dropout)
        self.fc2 = nn.Linear(config.hidden_size, config.num_classes)

    def forward(self, x):

        out_word = self.embedding(x[0])
        out_bigram = self.embedding_ngram2(x[2])
        out_trigram = self.embedding_ngram3(x[3])
        out = torch.cat((out_word, out_bigram, out_trigram), -1)

        out = out.mean(dim=1)
        out = self.dropout(out)
        out = self.fc1(out)
        out = F.relu(out)
        out = self.fc2(out)
        return </code></pre> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/models/TextCNN.py" title="TextCNN.py">TextCNN.py</a></p> 
<pre><code class="hljs"># coding: UTF-8
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'TextCNN'
        self.train_path = dataset + '/data/train.txt'                                # 训练集
        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集
        self.test_path = dataset + '/data/test.txt'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 20                                            # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度
        self.filter_sizes = (2, 3, 4)                                   # 卷积核尺寸
        self.num_filters = 256                                          # 卷积核数量(channels数)


'''Convolutional Neural Networks for Sentence Classification'''


class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        if config.embedding_pretrained is not None:
            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)
        else:
            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)
        self.convs = nn.ModuleList(
            [nn.Conv2d(1, config.num_filters, (k, config.embed)) for k in config.filter_sizes])
        self.dropout = nn.Dropout(config.dropout)
        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)

    def conv_and_pool(self, x, conv):
        x = F.relu(conv(x)).squeeze(3)
        x = F.max_pool1d(x, x.size(2)).squeeze(2)
        return x

    def forward(self, x):
        out = self.embedding(x[0])
        out = out.unsqueeze(1)
        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)
        out = self.dropout(out)
        out = self.fc(out)
        return out</code></pre> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/models/TextRCNN.py" title="TextRCNN.py">TextRCNN.py</a></p> 
<pre><code class="hljs"># coding: UTF-8
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'TextRCNN'
        self.train_path = dataset + '/data/train.txt'                                # 训练集
        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集
        self.test_path = dataset + '/data/test.txt'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 1.0                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 10                                            # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一
        self.hidden_size = 256                                          # lstm隐藏层
        self.num_layers = 1                                             # lstm层数


'''Recurrent Convolutional Neural Networks for Text Classification'''


class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        if config.embedding_pretrained is not None:
            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)
        else:
            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)
        self.lstm = nn.LSTM(config.embed, config.hidden_size, config.num_layers,
                            bidirectional=True, batch_first=True, dropout=config.dropout)
        self.maxpool = nn.MaxPool1d(config.pad_size)
        self.fc = nn.Linear(config.hidden_size * 2 + config.embed, config.num_classes)

    def forward(self, x):
        x, _ = x
        embed = self.embedding(x)  # [batch_size, seq_len, embeding]=[64, 32, 64]
        out, _ = self.lstm(embed)
        out = torch.cat((embed, out), 2)
        out = F.relu(out)
        out = out.permute(0, 2, 1)
        out = self.maxpool(out).squeeze()
        out = self.fc(out)
        return out</code></pre> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/models/TextRNN.py" title="TextRNN.py">TextRNN.py</a></p> 
<pre><code class="hljs"># coding: UTF-8
import torch
import torch.nn as nn
import numpy as np


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'TextRNN'
        self.train_path = dataset + '/data/train.txt'                                # 训练集
        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集
        self.test_path = dataset + '/data/test.txt'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 10                                            # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一
        self.hidden_size = 128                                          # lstm隐藏层
        self.num_layers = 2                                             # lstm层数


'''Recurrent Neural Network for Text Classification with Multi-Task Learning'''


class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        if config.embedding_pretrained is not None:
            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)
        else:
            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)
        self.lstm = nn.LSTM(config.embed, config.hidden_size, config.num_layers,
                            bidirectional=True, batch_first=True, dropout=config.dropout)
        self.fc = nn.Linear(config.hidden_size * 2, config.num_classes)

    def forward(self, x):
        x, _ = x
        out = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 32, 300]
        out, _ = self.lstm(out)
        out = self.fc(out[:, -1, :])  # 句子最后时刻的 hidden state
        return out

    '''变长RNN，效果差不多，甚至还低了点...'''
    # def forward(self, x):
    #     x, seq_len = x
    #     out = self.embedding(x)
    #     _, idx_sort = torch.sort(seq_len, dim=0, descending=True)  # 长度从长到短排序（index）
    #     _, idx_unsort = torch.sort(idx_sort)  # 排序后，原序列的 index
    #     out = torch.index_select(out, 0, idx_sort)
    #     seq_len = list(seq_len[idx_sort])
    #     out = nn.utils.rnn.pack_padded_sequence(out, seq_len, batch_first=True)
    #     # [batche_size, seq_len, num_directions * hidden_size]
    #     out, (hn, _) = self.lstm(out)
    #     out = torch.cat((hn[2], hn[3]), -1)
    #     # out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)
    #     out = out.index_select(0, idx_unsort)
    #     out = self.fc(out)
    #     return out</code></pre> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/models/TextRNN_Att.py" title="TextRNN_Att.py">TextRNN_Att.py</a></p> 
<pre><code class="hljs"># coding: UTF-8
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'TextRNN_Att'
        self.train_path = dataset + '/data/train.txt'                                # 训练集
        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集
        self.test_path = dataset + '/data/test.txt'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 10                                            # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 1e-3                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度, 若使用了预训练词向量，则维度统一
        self.hidden_size = 128                                          # lstm隐藏层
        self.num_layers = 2                                             # lstm层数
        self.hidden_size2 = 64


'''Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification'''


class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        if config.embedding_pretrained is not None:
            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)
        else:
            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)
        self.lstm = nn.LSTM(config.embed, config.hidden_size, config.num_layers,
                            bidirectional=True, batch_first=True, dropout=config.dropout)
        self.tanh1 = nn.Tanh()
        # self.u = nn.Parameter(torch.Tensor(config.hidden_size * 2, config.hidden_size * 2))
        self.w = nn.Parameter(torch.zeros(config.hidden_size * 2))
        self.tanh2 = nn.Tanh()
        self.fc1 = nn.Linear(config.hidden_size * 2, config.hidden_size2)
        self.fc = nn.Linear(config.hidden_size2, config.num_classes)

    def forward(self, x):
        x, _ = x
        emb = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 32, 300]
        H, _ = self.lstm(emb)  # [batch_size, seq_len, hidden_size * num_direction]=[128, 32, 256]

        M = self.tanh1(H)  # [128, 32, 256]
        # M = torch.tanh(torch.matmul(H, self.u))
        alpha = F.softmax(torch.matmul(M, self.w), dim=1).unsqueeze(-1)  # [128, 32, 1]
        out = H * alpha  # [128, 32, 256]
        out = torch.sum(out, 1)  # [128, 256]
        out = F.relu(out)
        out = self.fc1(out)
        out = self.fc(out)  # [128, 64]
        return </code></pre> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/models/Transformer.py" title="Transformer.py">Transformer.py</a></p> 
<pre><code class="hljs">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy


class Config(object):

    """配置参数"""
    def __init__(self, dataset, embedding):
        self.model_name = 'Transformer'
        self.train_path = dataset + '/data/train.txt'                                # 训练集
        self.dev_path = dataset + '/data/dev.txt'                                    # 验证集
        self.test_path = dataset + '/data/test.txt'                                  # 测试集
        self.class_list = [x.strip() for x in open(
            dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单
        self.vocab_path = dataset + '/data/vocab.pkl'                                # 词表
        self.save_path = dataset + '/saved_dict/' + self.model_name + '.ckpt'        # 模型训练结果
        self.log_path = dataset + '/log/' + self.model_name
        self.embedding_pretrained = torch.tensor(
            np.load(dataset + '/data/' + embedding)["embeddings"].astype('float32'))\
            if embedding != 'random' else None                                       # 预训练词向量
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备

        self.dropout = 0.5                                              # 随机失活
        self.require_improvement = 2000                                 # 若超过1000batch效果还没提升，则提前结束训练
        self.num_classes = len(self.class_list)                         # 类别数
        self.n_vocab = 0                                                # 词表大小，在运行时赋值
        self.num_epochs = 20                                            # epoch数
        self.batch_size = 128                                           # mini-batch大小
        self.pad_size = 32                                              # 每句话处理成的长度(短填长切)
        self.learning_rate = 5e-4                                       # 学习率
        self.embed = self.embedding_pretrained.size(1)\
            if self.embedding_pretrained is not None else 300           # 字向量维度
        self.dim_model = 300
        self.hidden = 1024
        self.last_hidden = 512
        self.num_head = 5
        self.num_encoder = 2


'''Attention Is All You Need'''


class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        if config.embedding_pretrained is not None:
            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)
        else:
            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)

        self.postion_embedding = Positional_Encoding(config.embed, config.pad_size, config.dropout, config.device)
        self.encoder = Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)
        self.encoders = nn.ModuleList([
            copy.deepcopy(self.encoder)
            # Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)
            for _ in range(config.num_encoder)])

        self.fc1 = nn.Linear(config.pad_size * config.dim_model, config.num_classes)
        # self.fc2 = nn.Linear(config.last_hidden, config.num_classes)
        # self.fc1 = nn.Linear(config.dim_model, config.num_classes)

    def forward(self, x):
        out = self.embedding(x[0])
        out = self.postion_embedding(out)
        for encoder in self.encoders:
            out = encoder(out)
        out = out.view(out.size(0), -1)
        # out = torch.mean(out, 1)
        out = self.fc1(out)
        return out


class Encoder(nn.Module):
    def __init__(self, dim_model, num_head, hidden, dropout):
        super(Encoder, self).__init__()
        self.attention = Multi_Head_Attention(dim_model, num_head, dropout)
        self.feed_forward = Position_wise_Feed_Forward(dim_model, hidden, dropout)

    def forward(self, x):
        out = self.attention(x)
        out = self.feed_forward(out)
        return out


class Positional_Encoding(nn.Module):
    def __init__(self, embed, pad_size, dropout, device):
        super(Positional_Encoding, self).__init__()
        self.device = device
        self.pe = torch.tensor([[pos / (10000.0 ** (i // 2 * 2.0 / embed)) for i in range(embed)] for pos in range(pad_size)])
        self.pe[:, 0::2] = np.sin(self.pe[:, 0::2])
        self.pe[:, 1::2] = np.cos(self.pe[:, 1::2])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = x + nn.Parameter(self.pe, requires_grad=False).to(self.device)
        out = self.dropout(out)
        return out


class Scaled_Dot_Product_Attention(nn.Module):
    '''Scaled Dot-Product Attention '''
    def __init__(self):
        super(Scaled_Dot_Product_Attention, self).__init__()

    def forward(self, Q, K, V, scale=None):
        '''
        Args:
            Q: [batch_size, len_Q, dim_Q]
            K: [batch_size, len_K, dim_K]
            V: [batch_size, len_V, dim_V]
            scale: 缩放因子 论文为根号dim_K
        Return:
            self-attention后的张量，以及attention张量
        '''
        attention = torch.matmul(Q, K.permute(0, 2, 1))
        if scale:
            attention = attention * scale
        # if mask:  # TODO change this
        #     attention = attention.masked_fill_(mask == 0, -1e9)
        attention = F.softmax(attention, dim=-1)
        context = torch.matmul(attention, V)
        return context


class Multi_Head_Attention(nn.Module):
    def __init__(self, dim_model, num_head, dropout=0.0):
        super(Multi_Head_Attention, self).__init__()
        self.num_head = num_head
        assert dim_model % num_head == 0
        self.dim_head = dim_model // self.num_head
        self.fc_Q = nn.Linear(dim_model, num_head * self.dim_head)
        self.fc_K = nn.Linear(dim_model, num_head * self.dim_head)
        self.fc_V = nn.Linear(dim_model, num_head * self.dim_head)
        self.attention = Scaled_Dot_Product_Attention()
        self.fc = nn.Linear(num_head * self.dim_head, dim_model)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(dim_model)

    def forward(self, x):
        batch_size = x.size(0)
        Q = self.fc_Q(x)
        K = self.fc_K(x)
        V = self.fc_V(x)
        Q = Q.view(batch_size * self.num_head, -1, self.dim_head)
        K = K.view(batch_size * self.num_head, -1, self.dim_head)
        V = V.view(batch_size * self.num_head, -1, self.dim_head)
        # if mask:  # TODO
        #     mask = mask.repeat(self.num_head, 1, 1)  # TODO change this
        scale = K.size(-1) ** -0.5  # 缩放因子
        context = self.attention(Q, K, V, scale)

        context = context.view(batch_size, -1, self.dim_head * self.num_head)
        out = self.fc(context)
        out = self.dropout(out)
        out = out + x  # 残差连接
        out = self.layer_norm(out)
        return out


class Position_wise_Feed_Forward(nn.Module):
    def __init__(self, dim_model, hidden, dropout=0.0):
        super(Position_wise_Feed_Forward, self).__init__()
        self.fc1 = nn.Linear(dim_model, hidden)
        self.fc2 = nn.Linear(hidden, dim_model)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(dim_model)

    def forward(self, x):
        out = self.fc1(x)
        out = F.relu(out)
        out = self.fc2(out)
        out = self.dropout(out)
        out = out + x  # 残差连接
        out = self.layer_norm(out)
        return out</code></pre> 
<h3><strong>在于models同级目录下有</strong></h3> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/tree/master/THUCNews" title="THUCNews">THUCNews</a></p> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/run.py" title="run.py">run.py</a>   <a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/train_eval.py" title="train_eval.py">train_eval.py</a></p> 
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/utils.py" title="utils.py">utils.py</a>   <a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/utils_fasttext.py" title="utils_fasttext.py">utils_fasttext.py</a></p> 
<h3>模型介绍</h3> 
<h3>1.TextCNN</h3> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/2c/76/uJIhrdw1_o.png"></p> 
<p>TextCNN整体结构</p> 
<p>数据处理：所有句子padding成一个长度：seq_len</p> 
<p>1.模型输入：<br><strong>[batch_size, seq_len]</strong></p> 
<p>2.经过embedding层：加载预训练词向量或者随机初始化, 词向量维度为embed_size：<br><strong>[batch_size, seq_len, embed_size]</strong></p> 
<p>3.卷积层：NLP中卷积核宽度与embed-size相同，相当于一维卷积。<br> 3个尺寸的卷积核：(2, 3, 4)，每个尺寸的卷积核有100个。卷积后得到三个特征图：<strong>[batch_size, 100, seq_len-1]</strong><br><strong>[batch_size, 100,</strong> <strong>seq_len-2]</strong><br><strong>[batch_size, 100,</strong> <strong>seq_len-3]</strong></p> 
<p>4.池化层：对三个特征图做最大池化<br><strong>[batch_size, 100]</strong><br><strong>[batch_size, 100]</strong><br><strong>[batch_size, 100]</strong></p> 
<p>5.拼接：<br><strong>[batch_size, 300]</strong></p> 
<p>6.全连接：num_class是预测的类别数<br><strong>[batch_size, num_class]</strong></p> 
<p>7.预测：softmax归一化，将num_class个数中最大的数对应的类作为最终预测<br><strong>[batch_size, 1]</strong></p> 
<p><strong>分析：</strong><br> 卷积操作相当于提取了句中的2-gram，3-gram，4-gram信息，多个卷积是为了提取多种特征，最大池化将提取到最重要的信息保留。</p> 
<h3>2.TextRNN</h3> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ca/ea/BQ8yZeAw_o.png"></p> 
<p>图片是单向RNN，看个意思就行</p> 
<p>1.模型输入： <strong>[batch_size, seq_len]</strong> </p> 
<p>2.经过embedding层：加载预训练词向量或者随机初始化, 词向量维度为embed_size： <strong>[batch_size, seq_len, embed_size]</strong></p> 
<p>3.双向LSTM：隐层大小为hidden_size，得到所有时刻的隐层状态(前向隐层和后向隐层拼接)<br><strong>[batch_size, seq_len, hidden_size * 2]</strong></p> 
<p>4.拿出最后时刻的隐层值：<br><strong>[batch_size, hidden_size * 2]</strong></p> 
<p>5.全连接：num_class是预测的类别数<br><strong>[batch_size, num_class]</strong></p> 
<p>6.预测：softmax归一化，将num_class个数中最大的数对应的类作为最终预测<br><strong>[batch_size, 1]</strong></p> 
<p><strong>分析：</strong><br> LSTM能更好的捕捉长距离语义关系，但是由于其递归结构，不能并行计算，速度慢。</p> 
<h3>3.TextRNN+Attention</h3> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/df/6f/0AT38c3O_o.jpg"></p> 
<p>BiLSTM+Attention</p> 
<p>1.模型输入： <strong>[batch_size, seq_len]</strong></p> 
<p>2.经过embedding层：加载预训练词向量或者随机初始化, 词向量维度为embed_size： <strong>[batch_size, seq_len, embed_size]</strong></p> 
<p>3.双向LSTM：隐层大小为hidden_size，得到所有时刻的隐层状态(前向隐层和后向隐层拼接) <strong>[batch_size, seq_len, hidden_size * 2]</strong></p> 
<p>4.初始化一个可学习的权重矩阵w<br><strong>w=[hidden_size * 2, 1]</strong></p> 
<p>5.对LSTM的输出进行非线性激活后与w进行矩阵相乘，并经行softmax归一化，得到每时刻的分值：<br><strong>[batch_size, seq_len, 1]</strong></p> 
<p>6.将LSTM的每一时刻的隐层状态乘对应的分值后求和，得到加权平均后的终极隐层值<br><strong>[batch_size, hidden_size * 2]</strong></p> 
<p>7.对终极隐层值进行非线性激活后送入两个连续的全连接层<br><strong>[batch_size, num_class]</strong></p> 
<p>8.预测：softmax归一化，将num_class个数中最大的数对应的类作为最终预测<br><strong>[batch_size, 1]</strong></p> 
<p><strong>分析：</strong><br> 其中4~6步是attention机制计算过程，其实就是对lstm每刻的隐层进行加权平均。比如句长为4，首先算出4个时刻的归一化分值：[0.1, 0.3, 0.4, 0.2]，然后<br><br> 终极h终极=0.1h1+0.3h2+0.4h3+0.2h4</p> 
<h3>4.TextRCNN</h3> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/bc/2b/X2mJBv4l_o.jpg"></p> 
<p>left context是前向RNN的隐层值，right context是后向RNN的隐层值。</p> 
<p>1.模型输入： <strong>[batch_size, seq_len]</strong></p> 
<p>2.经过embedding层：加载预训练词向量或者随机初始化, 词向量维度为embed_size： <strong>[batch_size, seq_len, embed_size]</strong></p> 
<p>3.双向LSTM：隐层大小为hidden_size，得到所有时刻的隐层状态(前向隐层和后向隐层拼接) <strong>[batch_size, seq_len, hidden_size * 2]</strong></p> 
<p>4.将embedding层与LSTM输出拼接，并进行非线性激活：<br><strong>[batch_size, seq_len, hidden_size * 2 + embed_size]</strong></p> 
<p>5.池化层：seq_len个特征中取最大的<br><strong>[batch_size, hidden_size * 2 + embed_size]</strong></p> 
<p>6.全连接后softmax<br><strong>[batch_size, num_class] ==&gt; [batch_size, 1]</strong></p> 
<p><strong>分析：</strong><br> 双向LSTM每一时刻的隐层值(前向+后向)都可以表示当前词的前向和后向语义信息，将隐藏值与embedding值拼接来表示一个词；然后用最大池化层来筛选出有用的特征信息。emm...就做了一个池化，就能称之为RCNN...<br> 需要注意的是，我的实现和论文中略有不同，论文中其实用的不是我们平时见的RNN，其实从图中能看出来，看公式的区别：</p> 
<p>传统RNN：</p> 
<p>正向反向ht正向=f(Wht−1+Uxt)ht反向=f(Wht+1+Uxt)</p> 
<h3>正向反向ht正向=f(Wht−1+Uxt−1)ht反向=f(Wht+1+Uxt+1)</h3> 
<p></p> 
<h3><strong>5.FastText</strong></h3> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/95/a3/mcso9oge_o.png"></p> 
<p>简单粗暴的FastText</p> 
<p>0.用哈希算法将2-gram、3-gram信息分别映射到两张表内。</p> 
<p>1.模型输入： <strong>[batch_size, seq_len]</strong></p> 
<p>2.embedding层：随机初始化, 词向量维度为embed_size，2-gram和3-gram同理：<br> word： <strong>[batch_size, seq_len, embed_size]</strong><br> 2-gram：<strong>[batch_size, seq_len, embed_size]</strong><br> 3-gram：<strong>[batch_size, seq_len, embed_size]</strong></p> 
<p>3.拼接embedding层：<br><strong>[batch_size, seq_len, embed_size * 3]</strong></p> 
<p>4.求所有seq_len个词的均值<br><strong>[batch_size, embed_size * 3]</strong></p> 
<p>5.全连接+非线性激活：隐层大小hidden_size<br><strong>[batch_size, hidden_size]</strong></p> 
<p>6.全连接+softmax归一化：<br><strong>[batch_size, num_class]==&gt;[batch_size, 1]</strong></p> 
<p><strong>分析：</strong><br> 不加N-Gram信息，就是词袋模型，准确率89.59%，加上2-gram和3-gram后准确率92.23%。N-Gram的哈希映射算法见<strong><a href="https://link.zhihu.com/?target=https%3A//github.com/649453932/Chinese-Text-Classification-Pytorch/blob/master/utils_fasttext.py" rel="nofollow" title="utils_fasttext.py">utils_fasttext.py</a></strong>中注释------gram-------处。<br> N-Gram的词表我设的25W，相对于前面几个模型，这个模型稍慢一点，FastText被我搞成SlowText了。。？但是效果不错呀。。哈哈</p> 
<p>有人私信问我这个N-Gram，我大体讲一下。对于N-Gram，我们设定一个词表，词表大小自己定，当然越大效果越好，但是你得结合实际情况，对于2-Gram，5000个字(字表大小)的两两组合有多少种你算算，3-Gram的组合有多少种，组合太多了，词表设大效果是好了，但是机器它受不了啊。<br> 所以N-Gram词表大小的设定是要适中的，那么适中的词表就放不下所有N-Gram了，不同的N-Gram用哈希算法可能会映射到词表同一位置，这确实是个弊端，但是问题不大：5000个字不可能每两个字都两两组合出现，很多两个字是永远不会组成2-Gram的，所以真正出现的N-Gram不会特别多，映射到同一词表位置的N-Gram也就不多了。<br> N-Gram词表大小我定的25w，比较大了，比小词表训练慢了很多，效果也提升了。这么形容N-Gram词表大小对效果的影响吧：一分价钱1分货，十分价钱1.1分货。</p> 
<p></p> 
<h3>6.DPCNN</h3> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/9e/f5/bXggILog_o.jpg"></p> 
<p> </p> 
<p>1.模型输入： <strong>[batch_size, seq_len]</strong></p> 
<p>2.经过embedding层：加载预训练词向量或者随机初始化, 词向量维度为embed_size： <strong>[batch_size, seq_len, embed_size]</strong></p> 
<p>3.进行卷积，250个尺寸为3的卷积核，论文中称这层为region embedding。<br><strong>[batch_size, 250, seq_len - 3 + 1]</strong></p> 
<p>4.接<strong>两层</strong>卷积(+relu)，每层都是250个尺寸为3的卷积核，(等长卷积，先padding再卷积，保证卷积前后的序列长度不变)<br><strong>[batch_size, 250, seq_len - 3 + 1]</strong></p> 
<p>5.接下来进行上图中小框中的操作。<br> I. 进行 大小为3，步长为2的最大池化，将序列长度压缩为原来的二分之一。（进行采样）<br> II. 接两层等长卷积(+relu)，每层都是250个尺寸为3的卷积核。<br> III. I的结果加上II的结果。（残差连接）<br> 重复以上操作，直至序列长度等于1。<br><strong>[batch_size, 250, 1]</strong></p> 
<p>6.全连接+softmax归一化：<br><strong>[batch_size, num_class]==&gt;[batch_size, 1]</strong></p> 
<p><strong>分析：</strong><br> TextCNN的过程类似于提取N-Gram信息，而且只有一层，难以捕捉长距离特征。<br> 反观DPCNN，可以看出来它的region embedding就是一个去掉池化层的TextCNN，再将卷积层叠加。</p> 
<p>每层序列长度都减半(如上图所示)，可以这么理解：相当于在N-Gram上再做N-Gram。越往后的层，每个位置融合的信息越多，最后一层提取的就是整个序列的语义信息。</p> 
<h3>Transformer</h3> 
<h3 id="h_338817680_1">1.Transformer 整体结构</h3> 
<p>首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e2/ce/DjinAcSP_o.jpg"></p> 
<p>Transformer 的整体结构，左图Encoder和右图Decoder</p> 
<p>可以看到 <strong>Transformer 由 Encoder 和 Decoder 两个部分组成</strong>，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：</p> 
<p><strong>第一步：</strong>获取输入句子的每一个单词的表示向量 <strong>X</strong>，<strong>X</strong>由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e2/11/8kVA257Y_o.jpg"></p> 
<p>Transformer 的输入表示</p> 
<p><strong>第二步：</strong>将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 <strong>x</strong>) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 <strong>C</strong>，如下图。单词向量矩阵用 Xn×d 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/23/ae/mmJX1iLA_o.jpg"></p> 
<p>Transformer Encoder 编码句子信息</p> 
<p><strong>第三步</strong>：将 Encoder 输出的编码信息矩阵 <strong>C</strong>传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 <strong>Mask (掩盖)</strong> 操作遮盖住 i+1 之后的单词。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2a/14/xjy9F7Nv_o.jpg"></p> 
<p>Transofrmer Decoder 预测</p> 
<p>上图 Decoder 接收了 Encoder 的编码矩阵<strong> C</strong>，然后首先输入一个翻译开始符 "&lt;Begin&gt;"，预测第一个单词 "I"；然后输入翻译开始符 "&lt;Begin&gt;" 和单词 "I"，预测单词 "have"，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。</p> 
<h3 id="h_338817680_2">2. Transformer 的输入</h3> 
<p>Transformer 中单词的输入表示 <strong>x</strong>由<strong>单词 Embedding</strong> 和<strong>位置 Embedding</strong> （Positional Encoding）相加得到。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/5d/40/d9dDc2jG_o.jpg"></p> 
<p>Transformer 的输入表示</p> 
<h4 id="h_338817680_3">2.1 单词 Embedding</h4> 
<p>单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。</p> 
<h4 id="h_338817680_4">2.2 位置 Embedding</h4> 
<p>Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。<strong>因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。</strong>所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。</p> 
<p>位置 Embedding 用 <strong>PE</strong>表示，<strong>PE</strong> 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/48/b5/dvoote3r_o.png"></p> 
<p>其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处：</p> 
<ul><li>使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。</li><li>可以让模型容易地计算出相对位置，对于固定长度的间距 k，<strong>PE(pos+k)</strong> 可以用 <strong>PE(pos)</strong> 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。</li></ul> 
<p>将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 <strong>x</strong>，<strong>x </strong>就是 Transformer 的输入。</p> 
<h3 id="h_338817680_5">3. Self-Attention（自注意力机制）</h3> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/c0/a9/dBtzTjAn_o.jpg"></p> 
<p>Transformer Encoder 和 Decoder</p> 
<p>上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为<strong> Multi-Head Attention</strong>，是由多个 <strong>Self-Attention</strong>组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</p> 
<p>因为 <strong>Self-Attention</strong>是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。</p> 
<h4 id="h_338817680_6">3.1 Self-Attention 结构</h4> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/4a/bb/QaBywzvw_o.jpg"></p> 
<p>Self-Attention 结构</p> 
<p>上图是 Self-Attention 的结构，在计算的时候需要用到矩阵<strong>Q(查询),K(键值),V(值)</strong>。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而<strong>Q,K,V</strong>正是通过 Self-Attention 的输入进行线性变换得到的。</p> 
<h4 id="h_338817680_7">3.2 Q, K, V 的计算</h4> 
<p>Self-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵<strong>WQ,WK,WV</strong>计算得到<strong>Q,K,V</strong>。计算如下图所示，<strong>注意 X, Q, K, V 的每一行都表示一个单词。</strong></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e6/3e/oY3ThYYu_o.jpg"></p> 
<p>Q, K, V 的计算</p> 
<h4 id="h_338817680_8">3.3 Self-Attention 的输出</h4> 
<p>得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ce/69/HCcniHC0_o.jpg"></p> 
<p>Self-Attention 的输出</p> 
<p>公式中计算矩阵<strong>Q</strong>和<strong>K</strong>每一行向量的内积，为了防止内积过大，因此除以 dk 的平方根。<strong>Q</strong>乘以<strong>K</strong>的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为<strong>Q</strong>乘以 KT ，1234 表示的是句子中的单词。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/3c/40/uVnfjCNy_o.png"></p> 
<p>Q乘以K的转置的计算</p> 
<p>得到QKT 之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/4e/8e/1jo6r287_o.png"></p> 
<p>对矩阵的每一行进行 Softmax</p> 
<p>得到 Softmax 矩阵之后可以和<strong>V</strong>相乘，得到最终的输出<strong>Z</strong>。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/4c/07/xNpi8u9K_o.png"></p> 
<p>Self-Attention 输出</p> 
<p>上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出 Z1 等于所有单词 i 的值 Vi 根据 attention 系数的比例加在一起得到，如下图所示：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/d4/e8/OsCXMaDD_o.jpg"></p> 
<p>Zi 的计算方法</p> 
<h4 id="h_338817680_9">3.4 Multi-Head Attention</h4> 
<p>在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/6d/58/dSbcIZJe_o.jpg"></p> 
<p>Multi-Head Attention</p> 
<p>从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入<strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<strong>Z</strong>。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵<strong>Z</strong>。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/83/d3/u6c6PCQr_o.jpg"></p> 
<p>多个 Self-Attention</p> 
<p>得到 8 个输出矩阵 Z1 到 Z8 之后，Multi-Head Attention 将它们拼接在一起 <strong>(Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出<strong>Z</strong>。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/40/3c/1mhwaRJA_o.jpg"></p> 
<p>Multi-Head Attention 的输出</p> 
<p>可以看到 Multi-Head Attention 输出的矩阵<strong>Z</strong>与其输入的矩阵<strong>X</strong>的维度是一样的。</p> 
<h3 id="h_338817680_10">4. Encoder 结构</h3> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/7f/dd/xzoSvBmh_o.jpg"></p> 
<p>Transformer Encoder block</p> 
<p>上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention,<strong> Add &amp; Norm, Feed Forward, Add &amp; Norm </strong>组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p> 
<h4 id="h_338817680_11">4.1 Add &amp; Norm</h4> 
<p>Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/c7/ca/B86ZVfmL_o.png"></p> 
<p>Add &amp;amp;amp;amp;amp;amp; Norm 公式</p> 
<p>其中 <strong>X</strong>表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(<strong>X</strong>) 和 FeedForward(<strong>X</strong>) 表示输出 (输出与输入 <strong>X </strong>维度是一样的，所以可以相加)。</p> 
<p><strong>Add</strong>指 <strong>X</strong>+MultiHeadAttention(<strong>X</strong>)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/c6/5c/kmhfO0XZ_o.png"></p> 
<p>残差连接</p> 
<p><strong>Norm</strong>指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。</p> 
<h4 id="h_338817680_12">4.2 Feed Forward</h4> 
<p>Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ac/bc/q5aEmwN1_o.png"></p> 
<p>Feed Forward</p> 
<p><strong>X</strong>是输入，Feed Forward 最终得到的输出矩阵的维度与<strong>X</strong>一致。</p> 
<h4 id="h_338817680_13">4.3 组成 Encoder</h4> 
<p>通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵 X(n×d) ，并输出一个矩阵 O(n×d) 。通过多个 Encoder block 叠加就可以组成 Encoder。</p> 
<p>第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是<strong>编码信息矩阵 C</strong>，这一矩阵后续会用到 Decoder 中。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/cb/48/bR26IP1d_o.jpg"></p> 
<p>Encoder 编码句子信息</p> 
<h3 id="h_338817680_14">5. Decoder 结构</h3> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/28/b4/bBmZXnww_o.jpg"></p> 
<p>Transformer Decoder block</p> 
<p>上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：</p> 
<ul><li>包含两个 Multi-Head Attention 层。</li><li>第一个 Multi-Head Attention 层采用了 Masked 操作。</li><li>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个 Decoder block 的输出计算。</li><li>最后有一个 Softmax 层计算下一个翻译单词的概率。</li></ul> 
<h4 id="h_338817680_15">5.1 第一个 Multi-Head Attention</h4> 
<p>Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 "我有一只猫" 翻译成 "I have a cat" 为例，了解一下 Masked 操作。</p> 
<p>下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 "&lt;Begin&gt;" 预测出第一个单词为 "I"，然后根据输入 "&lt;Begin&gt; I" 预测下一个单词 "have"。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ee/8d/upU7mlUz_o.png"></p> 
<p>Decoder 预测</p> 
<p>Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (&lt;Begin&gt; I have a cat) 和对应输出 (I have a cat &lt;end&gt;) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，<strong>注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 "&lt;Begin&gt; I have a cat &lt;end&gt;"。</strong></p> 
<p><strong>第一步：</strong>是 Decoder 的输入矩阵和 <strong>Mask </strong>矩阵，输入矩阵包含 "&lt;Begin&gt; I have a cat" (0, 1, 2, 3, 4) 五个单词的表示向量，<strong>Mask </strong>是一个 5×5 的矩阵。在 <strong>Mask </strong>可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2b/70/tD29vYRA_o.png"></p> 
<p>输入矩阵与 Mask 矩阵</p> 
<p><strong>第二步：</strong>接下来的操作和之前的 Self-Attention 一样，通过输入矩阵<strong>X</strong>计算得到<strong>Q,K,V</strong>矩阵。然后计算<strong>Q</strong>和 KT 的乘积 QKT 。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f9/cc/LMWDxM86_o.jpg"></p> 
<p>Q乘以K的转置</p> 
<p><strong>第三步：</strong>在得到 QKT 之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用<strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/fe/8a/s6uzQ6ny_o.jpg"></p> 
<p>Softmax 之前 Mask</p> 
<p>得到 <strong>Mask</strong> QKT 之后在 <strong>Mask </strong>QKT上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p> 
<p><strong>第四步：</strong>使用 <strong>Mask </strong>QKT与矩阵<strong> V</strong>相乘，得到输出 <strong>Z</strong>，则单词 1 的输出向量 Z1 是只包含单词 1 信息的。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/44/ed/46aWIbpW_o.png"></p> 
<p>Mask 之后的输出</p> 
<p><strong>第五步：</strong>通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 Zi ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出Zi 然后计算得到第一个 Multi-Head Attention 的输出<strong>Z</strong>，<strong>Z</strong>与输入<strong>X</strong>维度一样。</p> 
<h4 id="h_338817680_16">5.2 第二个 Multi-Head Attention</h4> 
<p>Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 <strong>K, V</strong>矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 <strong>Encoder 的编码信息矩阵 C </strong>计算的。</p> 
<p>根据 Encoder 的输出 <strong>C</strong>计算得到 <strong>K, V</strong>，根据上一个 Decoder block 的输出<strong> Z</strong> 计算 <strong>Q</strong> (如果是第一个 Decoder block 则使用输入矩阵 <strong>X</strong> 进行计算)，后续的计算方法与之前描述的一致。</p> 
<p>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 <strong>Mask</strong>)。</p> 
<h4 id="h_338817680_17">5.3 Softmax 预测输出单词</h4> 
<p>Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/93/6c/UwrCAo1B_o.jpg"></p> 
<p>Decoder Softmax 之前的 Z</p> 
<p>Softmax 根据输出矩阵的每一行预测下一个单词：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/44/cd/6apHqi0f_o.png"></p> 
<p>Decoder Softmax 预测</p> 
<p>这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。</p> 
<h3 id="h_338817680_18">6. Transformer 总结</h3> 
<ul><li>Transformer 与 RNN 不同，可以比较好地并行训练。</li><li>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</li><li>Transformer 的重点是 Self-Attention 结构，其中用到的 <strong>Q, K, V</strong>矩阵通过输出进行线性变换得到。</li><li>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</li></ul> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7b51b02235bb41e8e3581669ef3bc00b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Axure RP9 安装教程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6257131cdff38b98385adf1997f4d4b5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">免费调用快递鸟物流跟踪轨迹订阅接口技术文档</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>