<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【零基础学爬虫】第五章：scrapy数据解析实战（二） - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【零基础学爬虫】第五章：scrapy数据解析实战（二）" />
<meta property="og:description" content="一、项目准备 1.创建工程 scrapy startproject qiubaiPro 2.创建爬虫文件 需求：爬取糗事百科中“段子”栏中的数据：https://www.qiushibaike.com/text/，解析作者名称&#43;段子内容。
cd qiubaiPro scrapy genspider qiubai https://www.qiushibaike.com/text/ 3.修改配置文件 ROBOTSTXT_OBEY = False # 显示指定类型的日志信息 LOG_LEVEL = &#39;ERROR&#39; # 修改UA伪装 USER_AGENT = &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36&#39; 二、程序编写 1.编写程序 import scrapy class QiubaiSpider(scrapy.Spider): name = &#39;qiubai&#39; # allowed_domains = [&#39;www.xxx.com&#39;] start_urls = [&#39;https://www.qiushibaike.com/text/&#39;] def parse(self, response): # 解析作者的名称&#43;段子内容 # 这里的xpath和etree.xpath不是同一个方法，但是用法几乎一样 div_list = response.xpath(&#39;//div[@class=&#34;col1 old-style-col1&#34;]/div&#39;) for div in div_list: # xpath返回的是列表，但是列表元素一定是selector类型的对象 # extract 可以将selector对象中data参数存储的字符串提取出来 author = div." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/f7d9e920bb03ab6c4b1c20fc166f86af/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-03-04T11:14:14+08:00" />
<meta property="article:modified_time" content="2021-03-04T11:14:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【零基础学爬虫】第五章：scrapy数据解析实战（二）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>一、项目准备</h3> 
<h4>1.创建工程</h4> 
<pre><code>scrapy startproject qiubaiPro</code></pre> 
<h4>2.创建爬虫文件</h4> 
<p><span style="color:#f33b45;"><strong>需求：爬取糗事百科中“段子”栏中的数据：</strong></span><a href="https://www.qiushibaike.com/text/" rel="nofollow">https://www.qiushibaike.com/text/</a><span style="color:#f33b45;"><strong>，解析作者名称+段子内容。</strong></span></p> 
<pre><code>cd qiubaiPro

scrapy genspider qiubai https://www.qiushibaike.com/text/</code></pre> 
<h4>3.修改配置文件</h4> 
<pre><code>ROBOTSTXT_OBEY = False

# 显示指定类型的日志信息
LOG_LEVEL = 'ERROR'

# 修改UA伪装
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'</code></pre> 
<h3>二、程序编写</h3> 
<h4>1.编写程序</h4> 
<pre><code class="language-python">import scrapy


class QiubaiSpider(scrapy.Spider):
    name = 'qiubai'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://www.qiushibaike.com/text/']

    def parse(self, response):
        # 解析作者的名称+段子内容
        # 这里的xpath和etree.xpath不是同一个方法，但是用法几乎一样
        div_list = response.xpath('//div[@class="col1 old-style-col1"]/div')
        for div in div_list:
            # xpath返回的是列表，但是列表元素一定是selector类型的对象
            # extract 可以将selector对象中data参数存储的字符串提取出来
            author = div.xpath('./div[1]/a[2]/h2/text()')[0].extract()
            # 或者用extract_first，作用是将列表中第0个列表元素对应的selector进行extract操作。
            # 列表中只有一个列表元素的时候，可以用extract_first
            # author = div.xpath('./div[1]/a[2]/h2/text()').extract_first()
            # 列表调用了extract后，表示将列表中每一个selector对象中data对应的字符串提取了出来
            content = div.xpath('./a[1]/div/span//text()').extract()   # 因为文本中有&lt;br&gt;标签，要用//
            # 将列表转换为字符串
            content = ''.join(content)
            print(author, content)
            break</code></pre> 
<p><span style="color:#f33b45;"><strong>注意：scrapy中的xpath返回的列表中是一个Selector对象，如需转换成字符串，应该用extract把Selector中data对应的字符串取出来。</strong></span></p> 
<h4>2.执行程序</h4> 
<pre><code>scrapy crawl qiubai</code></pre> 
<p><img alt="" height="154" src="https://images2.imgbox.com/b3/0e/dsuUzpJt_o.png" width="1200"></p> 
<h3><strong>三、持久化存储</strong></h3> 
<h4><strong>1.基于终端指令存储</strong></h4> 
<p><strong>①在上面代码的基础上，把数据存在一个字典中。</strong></p> 
<pre><code class="language-python">import scrapy


class QiubaiSpider(scrapy.Spider):
    name = 'qiubai'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://www.qiushibaike.com/text/']

# 基于终端的存储

    def parse(self, response):
        # 解析作者的名称+段子内容
        # 这里的xpath和etree.xpath不是同一个方法，但是用法几乎一样
        div_list = response.xpath('//div[@class="col1 old-style-col1"]/div')
        all_data = []
        for div in div_list:
            # xpath返回的是列表，但是列表元素一定是selector类型的对象
            # extract 可以将selector对象中data参数存储的字符串提取出来
            author = div.xpath('./div[1]/a[2]/h2/text()')[0].extract()
            # author = div.xpath('./div[1]/a[2]/h2/text()').extract_first()
            # 列表调用了extract后，表示将列表中每一个selector对象中data对应的字符串提取了出来
            content = div.xpath('./a[1]/div/span//text()').extract()   # 因为文本中有&lt;br&gt;标签，要用//
            # 将列表转换为字符串
            content = ''.join(content)
            dic = {
                'author' : author,
                'content': content
            }
            all_data.append(dic)
        return all_data</code></pre> 
<p><strong>②在终端输入命令进行存储</strong></p> 
<pre><code>scrapy crawl qiubai -o ./qiubai.csv</code></pre> 
<p><strong>③注意事项</strong></p> 
<p><span style="color:#f33b45;"><strong>    - 要求：只可以将parse方法的返回值存储到本地的文本文件中 </strong></span></p> 
<p><span style="color:#f33b45;"><strong>    - 注意：持久化存储对应的文本文件的类型只可以为：'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle </strong></span></p> 
<p><span style="color:#f33b45;"><strong>    - 指令：scrapy crawl xxx -o filePath </strong></span></p> 
<p><span style="color:#f33b45;"><strong>    - 好处：简介高效便捷 </strong></span></p> 
<p><span style="color:#f33b45;"><strong>    - 缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）</strong></span></p> 
<h4>2.基于管道存储</h4> 
<p><strong>①编码流程：</strong></p> 
<p>  <span style="color:#f33b45;"><strong>  - 数据解析 </strong></span></p> 
<p><span style="color:#f33b45;"><strong>    - 在item类中定义相关的属性 （修改items.py）</strong></span></p> 
<p><span style="color:#f33b45;"><strong>    - 将解析的数据封装存储到item类型的对象 </strong></span></p> 
<p><span style="color:#f33b45;"><strong>    - 将item类型的对象提交给管道进行持久化存储的操作</strong></span></p> 
<p><span style="color:#f33b45;"><strong>    - 在管道类的process_item中要将其接受到的item对象中存储的数据进行持久化存储操作 </strong></span></p> 
<p><span style="color:#f33b45;"><strong>    - 在配置文件中开启管道</strong></span></p> 
<p><strong>②修改items.py文件</strong></p> 
<pre><code class="language-python"># Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class QiubaiproItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    author = scrapy.Field()
    content = scrapy.Field()
    pass
</code></pre> 
<p><strong>③修改pipelines.py文件</strong></p> 
<pre><code># Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


class QiubaiproPipeline:
    fp = None
    # 重写父类的一个方法，该方法只在开始爬虫的时候被调用一次
    def open_spider(self,spider):
        print("开始爬虫.......")
        self.fp = open('./qiubai.txt','w',encoding='utf-8')
    # 专门用来处理item类型对象的
    # 该方法可以接收爬虫文件提交过来的item对象
    # 该方法每接收到1个item就会被调用一次
    def process_item(self, item, spider):
        author = item['author']
        content = item['content']
        self.fp.write(author+':'+content+'\n')

        return item
    def close_spider(self,spider):
        print("结束爬虫！！")
        self.fp.close()
</code></pre> 
<p><strong>④修改settings.py文件</strong></p> 
<pre><code># 开启管道，把文件中的注释删掉
ITEM_PIPELINES = {
   'qiubaiPro.pipelines.QiubaiproPipeline': 300,
}
# 300表示的是优先级，数值越小优先级越高</code></pre> 
<p><strong>⑤完整主程序</strong></p> 
<pre><code>import scrapy
from qiubaiPro.items import QiubaiproItem

class QiubaiSpider(scrapy.Spider):
    name = 'qiubai'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://www.qiushibaike.com/text/']


# 基于管道的存储

    def parse(self, response):
        # 解析作者的名称+段子内容
        # 这里的xpath和etree.xpath不是同一个方法，但是用法几乎一样
        div_list = response.xpath('//div[@class="col1 old-style-col1"]/div')
        all_data = []
        for div in div_list:
            # xpath返回的是列表，但是列表元素一定是selector类型的对象
            # extract 可以将selector对象中data参数存储的字符串提取出来
            author = div.xpath('./div[1]/a[2]/h2/text()')[0].extract()
            # author = div.xpath('./div[1]/a[2]/h2/text()').extract_first()
            # 列表调用了extract后，表示将列表中每一个selector对象中data对应的字符串提取了出来
            content = div.xpath('./a[1]/div/span//text()').extract()   # 因为文本中有&lt;br&gt;标签，要用//
            # 将列表转换为字符串
            content = ''.join(content)

            # 将解析的数据封装到item类型的对象中
            item = QiubaiproItem()
            item['author'] = author
            item['content'] = content

            # 将item提交给了管道
            yield item</code></pre> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3b7ea8d19e319e75c24aa8f1bbbdc012/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">iw命令 linux 没有_linux下ifconfig、iwconfig、iwlist命令详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9e73d0a32ed243e65c903cc1fb089baf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">实现sqlite datediff日期时间相减（日期差）的方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>