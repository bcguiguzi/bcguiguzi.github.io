<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>WSDM&#39;23 | 基于实体对齐的文图检索优化算法 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="WSDM&#39;23 | 基于实体对齐的文图检索优化算法" />
<meta property="og:description" content="每天给你送来NLP技术干货！
来自：知识工场
图文检索是一项具有挑战性的跨模态任务，引起了人们的广泛关注。传统方法无法打破不同模态之间的障碍。视觉语言预训练模型（VLP）基于海量图文数据，大大提高了图文检索性能。尽管如此，基于VLP模型的方法仍然容易产生无法在跨模态数据上对齐实体的检索结果。并且在预训练阶段解决该问题的方法不仅昂贵，且欠缺应用性。
因此，我们提出了一种轻量级的实用方法AGREE，基于VLP模型，仅在微调和重排序阶段对齐图像-文本实体，以优化图文检索策略的性能。我们利用外部知识和工具构建额外的细粒度图像-文本对，并微调训练中通过对比学习和实体级mask建模来强调跨模式的实体对齐。我们同时提出了两种重排序策略，其中一种是专为零样本场景设计的。在多个中文和英文数据集上，及多个VLP模型的大量实验表明，我们的方法在几乎所有的设置下都能获得最先进的结果。目前该工作已被WSDM-2023主会接收。
一、研究背景 图文检索任务可以输入文本作为查询内容以进行图像检索，也可以输入图像作为查询内容以进行文本检索。在VLP模型执行下游图文检索任务时，希望在输入特定文本时，模型能够准确预测出该特定文本对应的图像文本样本对中的图像；同时，希望在输入特定图像时，模型能够准确预测出该特定图像对应的图像文本对中的文本。
如图1所示，左侧“查询”列和中间“真实数据”列所包括的图像-文本对，是用于进行VLP模型预训练时所使用的样本对。但图1所示的三个例子中，都发生了由于实体没有对齐而导致的错误预测。
图 1 在COCO-CN数据集微调后3个不匹配的图文检索案例
在第一个例子中，文本查询中的“菠萝”没有出现在预测图像中。同样，在第二个例子中，模型只关注“蔬菜”和“盘子”的匹配，而忽略了查询中的另一个重要实体“紫菜包饭”。另外，对盘子的数量也有误判。在第三个例子中，预测文本不包含在查询图像中可以明显观察到的“苹果”和“蛋糕”。
为了改善VLP的图文检索性能，最近的一些工作，如ROSITA，ERNIE-ViL分别从图像和文本中构建场景图，然后将它们对齐，其他一些工作利用外部知识，如OSCAR外部目标检测器或多语言数据集UC2，以改进细粒度跨模态匹配。尽管他们同样取得了成功，然而VLP模型的预训练成本极高，并且难以获取的预训练数据集也使得现有方法的实用性不高。
因此，考虑到VLP模型极高的预训练成本，本文提出了一种在无需重训VLP模型的情况下，仅仅通过微调，甚至在零样本场景下通过重排序而实现图像-文本实体的细粒度对齐，由此优化图文检索对策性能。具体地，可以在微调中通过对比学习和实体级掩模建模强调跨模式的实体对齐，并可以通过外部知识的引入进一步提升性能。可以通过重排序策略进一步改善图文检索结果。
二、方法部分 我们通过图2的流程图来展示AGREE，包含微调和重排序两部分内容。
图2 AGREE的总体框架
框架中的具体步骤包括：
Step1：我们从文本中识别出文本实体，从图像中识别出视觉实体，然后通过预训练的VLP模型的编码器将它们与原始文本和图像一起进行编码。
Step2：在微调阶段，我们设计了三个不同的模块来学习跨模态实体之间的对齐:
a. 视觉实体-图像对齐(VEA)：通过整理Visual Genome的数据作为多模态知识库（MMKB），从中获取额外的视觉实体-图像对，通过对比学习和图像实体级区域掩模建模来学习视觉实体标签与其对应的细粒度图像之间的对齐。 b. 文本实体-图像对齐(TEA)：通过每个文本中包含文本实体及其可视化属性（如颜色和数字）来构建句子，之后通过对比学习和文本实体掩模建模来学习句子与其对应的图像之间的对齐。 c. 文本-图像实体对齐(TIA)：强调了跨模态实体对齐的重要性，通过随机掩模建模图像或文本中的实体，让模型对跨模态对齐实体的缺失更加敏感。
Step3：基于结果的重排序阶段，该阶段设计了两个不同的模块来细化top-𝑘排名结果: a. 文本-图像双向重排序(TBR)：采用top-𝑘(𝑘=20)的检索结果进行反向图像-文本检索，其反向检索的结果排名随后被用于重新计算排名，进行重新排序。 b. 文本实体指导的重新排序(EGR)：专门为零样本场景下设计的重排序结果，它采用top-𝑘检索结果来计算来自图像和文本的实体之间的相似性，然后考虑相似性来细化排序结果。
其中，AGREE的微调框架的架构如图3所示：
图3 AGREE微调框架的架构
视觉实体编码器的输入从Visual Genome中选择的实体图像。文本实体编码器的输入包含来自使用实体链接的文本的实体，以及来自 Visual Genome 的标签实体，具体的实体内容取决于不同的对齐任务。图片右上角的框指示了每个模块的视觉和文本输入。
如框架所示，基于跨模态实体对齐的方法计算了全局相似度和实体相似度，并之后对其进行融合，作为统一的训练目标。实体相似度是指基于VEA、TEA和TIA三个模块的跨模态实体比对，强调图像和文本之间的相似性。其中，VEA将从外部多模态知识库中获得的对应图像作为实体标签的输入，通过VEM和MVC两个子模块输出视觉图像与其标签之间的相似性。TEA由TEE和MEC两个子模块组成，接收以文本实体和图像为输入的文本，输出文本实体与图像之间的相似度。TIA也接受带有实体的原始图像和文本，但学习计算图像和文本实体之间的相似距离。
AGREE具体模块的技术介绍：
PART 01
微调阶段
1）视觉实体-图像对齐（Visual Entity-Image Alignment，VEA）模块
与严重依赖对象检测模型用于细粒度交互的现有VLP模型不同，AGREE简单地将检测到的标签用作媒介并重建一个MMKB， 用于视觉知识以与其视觉图像对齐。我们选择Visual Genome来作为外部增强的知识库，并设计简单的启发式规则过滤图像来建立该object-image库。在微调过程中，为批次内𝑁幅图像所带有的𝑘个实体收集视觉标签集 ，并从MMKB中找出与实体的关联图像。
VEA遵循图文对比学习的范式，通过两个任务来学习每个视觉实体的实体-图像对齐，分别为VEM（视觉实体匹配Visual Entity Matching）和MVC（掩码视觉对象一致性对齐Masking Visual Object Consitency Alignment）。 用于优化每个标签文本对其关联图像的匹配，使用提示“a photo contains {entity}”的方法，优化视觉对象的标签文本 与其图像 的匹配； 包括用于表征带有掩码实体的标签文本的embedding与其关联图像的embedding的相似度与不带有掩码实体的标签文本的embedding与其关联图像的embedding的相似度差异的损失函数。我们利用实体标签提示与原始图像以及标签提示与带有掩码实体的图像之间计算的相似度分数的差异，最小化式（4）中视觉对象一致性学习的边际排序损失。 用于表示带掩码实体区域的图像和文本之间的相似性。 期望原始图像和对象标签的得分更高，以更多地强调那些缺失的视觉实体。
(4)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/611e1049a4a33f6a5f28a3731059cbb2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-02T18:28:29+08:00" />
<meta property="article:modified_time" content="2022-12-02T18:28:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">WSDM&#39;23 | 基于实体对齐的文图检索优化算法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;">每天给你送来NLP技术干货！</p> 
 <hr> 
 <p style="text-align:justify;">来自：知识工场</p> 
 <p style="text-align:justify;">图文检索是一项具有挑战性的跨模态任务，引起了人们的广泛关注。传统方法无法打破不同模态之间的障碍。视觉语言预训练模型（VLP）基于海量图文数据，大大提高了图文检索性能。尽管如此，基于VLP模型的方法仍然容易产生无法在跨模态数据上对齐实体的检索结果。并且在预训练阶段解决该问题的方法不仅昂贵，且欠缺应用性。</p> 
 <p style="text-align:justify;">因此，我们提出了一种轻量级的实用方法AGREE，基于VLP模型，仅在微调和重排序阶段对齐图像-文本实体，以优化图文检索策略的性能。我们利用外部知识和工具构建额外的细粒度图像-文本对，并微调训练中通过对比学习和实体级mask建模来强调跨模式的实体对齐。我们同时提出了两种重排序策略，其中一种是专为零样本场景设计的。在多个中文和英文数据集上，及多个VLP模型的大量实验表明，我们的方法在几乎所有的设置下都能获得最先进的结果。<strong>目前该工作已被WSDM-2023主会接收</strong>。<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ea/a4/OFS61Zoe_o.jpg" alt="eeb7e19bb119c6126027493d160f4118.jpeg"></p> 
 <h4>一、研究背景</h4> 
 <p style="text-align:justify;">图文检索任务可以输入文本作为查询内容以进行图像检索，也可以输入图像作为查询内容以进行文本检索。在VLP模型执行下游图文检索任务时，希望在输入特定文本时，模型能够准确预测出该特定文本对应的图像文本样本对中的图像；同时，希望在输入特定图像时，模型能够准确预测出该特定图像对应的图像文本对中的文本。</p> 
 <p style="text-align:justify;">如图1所示，左侧“查询”列和中间“真实数据”列所包括的图像-文本对，是用于进行VLP模型预训练时所使用的样本对。但图1所示的三个例子中，都发生了由于实体没有对齐而导致的错误预测。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/63/e7/YzDCjiyQ_o.png" alt="e2ccb4b218611bd8d9173f9843fa0c59.png"></p> 
 <p style="text-align:center;">图 1   在COCO-CN数据集微调后3个不匹配的图文检索案例</p> 
 <p style="text-align:justify;">在第一个例子中，文本查询中的“菠萝”没有出现在预测图像中。同样，在第二个例子中，模型只关注“蔬菜”和“盘子”的匹配，而忽略了查询中的另一个重要实体“紫菜包饭”。另外，对盘子的数量也有误判。在第三个例子中，预测文本不包含在查询图像中可以明显观察到的“苹果”和“蛋糕”。<br></p> 
 <p style="text-align:justify;">为了改善VLP的图文检索性能，最近的一些工作，如ROSITA，ERNIE-ViL分别从图像和文本中构建场景图，然后将它们对齐，其他一些工作利用外部知识，如OSCAR外部目标检测器或多语言数据集UC2，以改进细粒度跨模态匹配。尽管他们同样取得了成功，然而VLP模型的预训练成本极高，并且难以获取的预训练数据集也使得现有方法的实用性不高。</p> 
 <p style="text-align:justify;">因此，考虑到VLP模型极高的预训练成本，本文提出了一种在无需重训VLP模型的情况下，仅仅通过微调，甚至在零样本场景下通过重排序而实现图像-文本实体的细粒度对齐，由此优化图文检索对策性能。具体地，可以在微调中通过对比学习和实体级掩模建模强调跨模式的实体对齐，并可以通过外部知识的引入进一步提升性能。可以通过重排序策略进一步改善图文检索结果。</p> 
 <h4>二、方法部分</h4> 
 <p style="text-align:justify;">我们通过图2的流程图来展示AGREE，包含微调和重排序两部分内容。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5a/19/YWkUGWl8_o.png" alt="6e9ec223447de60aea42d216f1f0ed52.png"></p> 
 <p style="text-align:center;">图2 AGREE的总体框架</p> 
 <p style="text-align:justify;">框架中的具体步骤包括：</p> 
 <p style="text-align:justify;"><strong>Step1</strong>：我们从文本中识别出文本实体，从图像中识别出视觉实体，然后通过预训练的VLP模型的编码器将它们与原始文本和图像一起进行编码。</p> 
 <p style="text-align:justify;"><strong>Step2</strong>：在微调阶段，我们设计了三个不同的模块来学习跨模态实体之间的对齐:</p> 
 <p style="text-align:justify;"><strong>a.  </strong>视觉实体-图像对齐(VEA)：通过整理Visual Genome的数据作为多模态知识库（MMKB），从中获取额外的视觉实体-图像对，通过对比学习和图像实体级区域掩模建模来学习视觉实体标签与其对应的细粒度图像之间的对齐。   </p> 
 <p style="text-align:justify;"><strong>b.  </strong>文本实体-图像对齐(TEA)：通过每个文本中包含文本实体及其可视化属性（如颜色和数字）来构建句子，之后通过对比学习和文本实体掩模建模来学习句子与其对应的图像之间的对齐。     </p> 
 <p style="text-align:justify;"><strong>c.  </strong>文本-图像实体对齐(TIA)：强调了跨模态实体对齐的重要性，通过随机掩模建模图像或文本中的实体，让模型对跨模态对齐实体的缺失更加敏感。</p> 
 <p style="text-align:justify;"><strong>Step3</strong>：基于结果的重排序阶段，该阶段设计了两个不同的模块来细化top-𝑘排名结果: </p> 
 <p style="text-align:justify;"><strong>a.  </strong>文本-图像双向重排序(TBR)：采用top-𝑘(𝑘=20)的检索结果进行反向图像-文本检索，其反向检索的结果排名随后被用于重新计算排名，进行重新排序。   </p> 
 <p style="text-align:justify;"><strong>b.  </strong>文本实体指导的重新排序(EGR)：专门为零样本场景下设计的重排序结果，它采用top-𝑘检索结果来计算来自图像和文本的实体之间的相似性，然后考虑相似性来细化排序结果。</p> 
 <p style="text-align:justify;">其中，AGREE的微调框架的架构如图3所示：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d0/ae/lBPea9X9_o.png" alt="8c218ebf458f0996bd3f0f1ea14ca9dd.png"></p> 
 <p style="text-align:center;">图3 AGREE微调框架的架构</p> 
 <p style="text-align:justify;">视觉实体编码器的输入从Visual Genome中选择的实体图像。文本实体编码器的输入包含来自使用实体链接的文本的实体，以及来自 Visual Genome 的标签实体，具体的实体内容取决于不同的对齐任务。图片右上角的框指示了每个模块的视觉和文本输入。<br></p> 
 <p style="text-align:justify;">如框架所示，基于跨模态实体对齐的方法计算了全局相似度和实体相似度，并之后对其进行融合，作为统一的训练目标。实体相似度是指基于VEA、TEA和TIA三个模块的跨模态实体比对，强调图像和文本之间的相似性。其中，VEA将从外部多模态知识库中获得的对应图像作为实体标签的输入，通过VEM和MVC两个子模块输出视觉图像与其标签之间的相似性。TEA由TEE和MEC两个子模块组成，接收以文本实体和图像为输入的文本，输出文本实体与图像之间的相似度。TIA也接受带有实体的原始图像和文本，但学习计算图像和文本实体之间的相似距离。</p> 
 <p style="text-align:justify;">AGREE具体模块的技术介绍：</p> 
 <p>PART 01</p> 
 <p>微调阶段</p> 
 <p style="text-align:justify;"><strong>1）视觉实体-图像对齐</strong>（Visual Entity-Image Alignment，VEA）模块<br></p> 
 <p style="text-align:justify;">与严重依赖对象检测模型用于细粒度交互的现有VLP模型不同，AGREE简单地将检测到的标签用作媒介并重建一个MMKB， 用于视觉知识以与其视觉图像对齐。我们选择Visual Genome来作为外部增强的知识库，并设计简单的启发式规则过滤图像来建立该object-image库。在微调过程中，为批次内𝑁幅图像所带有的𝑘个实体收集视觉标签集  ，并从MMKB中找出与实体的关联图像。</p> 
 <p style="text-align:justify;">VEA遵循图文对比学习的范式，通过两个任务来学习每个视觉实体的实体-图像对齐，分别为VEM（视觉实体匹配Visual Entity Matching）和MVC（掩码视觉对象一致性对齐Masking Visual Object Consitency Alignment）。  用于优化每个标签文本对其关联图像的匹配，使用提示“a photo contains {entity}”的方法，优化视觉对象的标签文本  与其图像  的匹配；  包括用于表征带有掩码实体的标签文本的embedding与其关联图像的embedding的相似度与不带有掩码实体的标签文本的embedding与其关联图像的embedding的相似度差异的损失函数。我们利用实体标签提示与原始图像以及标签提示与带有掩码实体的图像之间计算的相似度分数的差异，最小化式（4）中视觉对象一致性学习的边际排序损失。  用于表示带掩码实体区域的图像和文本之间的相似性。  期望原始图像和对象标签的得分更高，以更多地强调那些缺失的视觉实体。</p> 
 <p style="text-align:right;">         (4)</p> 
 <p style="text-align:justify;"><strong>2）文本实体-图像对齐</strong>（Textual Entity-Image Alignment，TEA）模块</p> 
 <p style="text-align:justify;">我们关注到，相比于简明的文本说明，图像总能包含更多的冗余信息，因此在此重新考虑视觉和文本信息的不对称性，并特别注意文本中的实体级信息以与相应的图像对齐。在TEA模块包含TEE（文本实体强调的对齐Textual Entity-Image Alignment）和MEC（掩码实体一致性对齐Mask Entity Consitency Alignment两个子模块。TEE首先强调文本中的实体级信息，通过优化表征图像embedding与实体prompt的embedding的相似度与同batch中其他图像的embedding与实体prompt的embedding的相似度差异的损失函数（对应于如下  中的  ）。MEC通过掩码文本实体token，进一步将图像与文本实体一致地对齐。这里我们并不像大多数模型那样给出准确的词汇表并对实体进行分类，而是采用一种更轻量的方式来学习关于文本实体的统一跨模态表示。我们重新计算原始图像  与带有掩码实体的文本  之间的相似度，期望图像与被破坏句子之间的相似度  小于原始文本与图像之间的相似度  <sub></sub>。如式（6）所示。</p> 
 <p style="text-align:right;">        (6)</p> 
 <p style="text-align:justify;"><strong>3）文本-图像实体对齐</strong>（Textual-Image Entity Alignment,  TIA）模块</p> 
 <p style="text-align:justify;">为了进一步弥合模态之间的差距并补偿异构信息之间的无序词汇表造成的对齐缺陷，可以利用预训练的视觉基础模型作为锚点来针对检测到的文本实体  识别图像中每个实体的区域。随后对图像中的基准实体进行掩码，以最大化原始图像与带掩码区域  图像之间的差异。</p> 
 <p style="text-align:right;">          (7)</p> 
 <p style="text-align:justify;">因为在 VEA 模块中，已经学习了视觉侧的实体-图像对齐。因此在 TIA 中，我们只关注文本中实体和图像的一致性。</p> 
 <p>PART 02</p> 
 <p>重排序阶段</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/83/1d/9XhV1JpG_o.png" alt="cf48089a24c5f9b2f4e914ac341ea3e1.png"><br></p> 
 <p style="text-align:center;">图4 表明图像和文本的不一致的例子：文本描述总是简洁而重要的，而图像通常包含丰富的实体</p> 
 <p style="text-align:justify;"><strong>1）文本图像双向重排序</strong>（Text-Image Bidirectional Re-ranking，TBR）模块</p> 
 <p style="text-align:justify;">如图4所示，丰富的视觉信息和简洁的文本知识之间的冗余不一致可能导致仅通过一种模态的部分信息做出错误的决策，对于没有细粒度交互的 VLP 模型来说尤是如此。因此，TBR 策略用于补偿图文不一致性，该策略通过反向检索将来自互补模态的互信息引入作为额外的监督信号，且仅依赖于跨模态样本本身。具体而言，可以将具有最高相似性的文本样本  视为图像  的互惠近邻互近邻（reciprocal neighbors），并从候选池中反向检索与每个文本最相似的图像。在这里，使用排名位置而不是相似度得分。然后图像  的前k个候选图像用新计算的位置重新排序为  以代表  。文本到图像的检索也是如此。这种简单但有效的自监督方式只是重新访问排名位置，不需要额外的数据，但在一定程度上保证了视觉和文本信息的对齐。</p> 
 <p style="text-align:justify;"><strong>2）实体指导的重排序</strong>（Textual Entity-Guided Re-ranking，EGR)模块</p> 
 <p style="text-align:justify;">在零样本场景下，AGREE将TEA模块策略转换为用于重排序的实体对齐分数，以进一步提高具有细粒度实体级交互的 VLP 模型的性能。按照相同的程序，一方面将提取的文本实体  转换为基于prompt的文本，并计算针对图像  的文本实体对齐分数计算  ，另一方面，将文本中的实体替换为 [MASK] 以获取文本实体一致性评分为  。实体引导的重排序分数  由  和  的组合计算得出。</p> 
 <h4>三、实验部分</h4> 
 <p><strong>01</strong></p> 
 <p>微调和零样本实验</p> 
 <p style="text-align:justify;">为了证明AGREE对跨模态检索任务的改进，我们使用中英文数据集（中文：COCO-CN，Flickr30k，MUGE；英文：Flickr30k）在中英文预训练的CLIP模型上分别进行了实验。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c5/91/JNWhNhbT_o.jpg" alt="0ab2d8bc164924dd8145217b71302305.jpeg"></p> 
 <p style="text-align:center;">表1 在Flickr30k数据集上的微调结果</p> 
 <p><img src="https://images2.imgbox.com/bb/e4/4R26KeiK_o.jpg" alt="4d7f671300c354d3959e7143cc6b4be0.jpeg"><br></p> 
 <p style="text-align:center;">表2 在COCO-CN数据集上的微调结果</p> 
 <p style="text-align:justify;"><strong>1）微调实验</strong>：如表1和表2所示，微调阶段的实验展示了AGREE在微调场景下中英文数据集上效果，与最先进的采用预训练的方法进行对比，取得了具有竞争力的结果。我们可以从结果中探索跨模态实体在跨模态检索中的贡献。与仅使用全局相似性对Wukong和CLIP等大规模VLP模型对比，AGREE在COCO-CN上表现出较大的改进，且显著高于FILIP中使用patch-token细粒度对比学习框架的VLP模型的微调结果，从而进一步证明了基于实体的策略的有效性。对于英文数据集，我们在Flickr30k上的结果也获得了很大的提升。可以发现我们的改进主要体现在R-1的结果上，无论中文还是英文数据集。在对大量数据进行预训练的VLP模型具有较强拟合能力的前提下，AGREE的改进主要在于优化重排序结果。将正确结果的排序移到更靠前的位置从而提高平均召回率。之后也将有进一步的实证分析来说明这一点。</p> 
 <p style="text-align:justify;"><strong>2）零样本实验</strong>：我们零样本场景下测试了AGREE中重新排序策略的有效性，并在表3中显示了两个模块（EGR或TBR）及其组合的实验结果。在不同数据集和图像编码器上的实验结果表明，图像到文本和文本到图像的MR均可提升3%，其中在R-1上的提升更为显著，平均提升约5%。对于只包含文本到图像检索任务的数据集MUGE，图像到文本的逆向检索也取得了明显的性能提升。而且，大多数的实验结果表明，文本到图像的增强效果比图像到文本的增强效果更加明显，这进一步印证了我们对于图文信息的丰富程度不一致的观察。通过图像到文本的逆向检索结果，AGREE对图像的描述更丰富、更具体，可以更好地辅助文本找到更合适的图像。</p> 
 <p><img src="https://images2.imgbox.com/ad/09/Wun6KtD1_o.jpg" alt="6367bf435c28c1f97f90263486ea6622.jpeg"></p> 
 <p style="text-align:center;">表3 在中文数据集上零样本场景的实验结果，预训练模型权重来自于Wukong</p> 
 <p><strong>02</strong></p> 
 <p>少样本场景下的实验</p> 
 <p style="text-align:justify;">我们验证了在少样本场景下AGREE的影响。如表4所示，我们将COCO-CN的训练集随机分为5%、15%、25%和50%，并在同样的测试集全集上进行实验。对于每种数据量，进行3个实验并报告它们的平均值。实验结果表明，在数据量较小时，AGREE微调（简化为FT）的性能提升显著（例如，当训练数据量为5%时，R-1的性能提升约1.6%）。在同时采用微调和重排序的优化策略后，较小数据量的MR得分可以达到甚至超过使用原始微调方法在较大数据量下的结果（例如，AGREE框架下25%的数据的结果约等于50%数据的基线方法）。我们认为这是一个令人兴奋的结果，说明AGREE提供了一种有效的微调方法。在微调过程中关注实体级信息的对齐，可以大大减少训练数据的依赖性。这样一来，我们甚至可以用更少的数据获得更好的结果。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1b/40/FndTwKAX_o.jpg" alt="0dea4ec69939bf5f4b39449a937c5ff7.jpeg"></p> 
 <p style="text-align:center;">表4 COCO-CN数据集在不同训练数据量下的少样本实验结果</p> 
 <p><strong>03</strong></p> 
 <p>实证分析</p> 
 <p style="text-align:justify;">在图5和图6中，我们通过COCO-CN数据集中text-to-image和image-to-text检索的几个例子，来更直观地说明了AGREE的有效性，揭示了对齐跨模态实体对提高图文检索性能的重要性（真实值在图中被用红线框起）。以text-to-image为例，采用AGREE得到的前5个图像显然包含更多与查询对应的实体，因此正确的样本排名更高。例如，对于文本query“pineapple, bananas and oranges in a glass plate”，原有baseline的top-1图像中不包含重要实体“pineapple”。而显然AGREE有助于重新建立实体之间的这种对应关系，解决了VLP模型中缺少实体级的细粒度交互问题。对于image-to-text第一行中的例子，AGREE框架使得图像与图像中具有实体的文本（包括“a plate of pizza”，“bottles”，“pots”）匹配得更准确。因此可以看到，AGREE更加关注图像和文本中多个实体的一致性，从而优化排序结果。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/8e/7b/k1BItCNg_o.png" alt="926086738b35e95fe5ca502a3e0d8d70.png"></p> 
 <p style="text-align:center;">图5 AGREE框架下text-to-image的Top-5检索结果对比示例</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/af/72/qohaFLnx_o.png" alt="e00c1f0523386d046c7e2bde410b0949.png"></p> 
 <p style="text-align:center;">图6 AGREE框架下image-to-text的Top-3检索结果对比示例</p> 
 <h4>四、总  结</h4> 
 <p style="text-align:justify;">我们提出了AGREE一种轻量级和适用的方法，在微调和重排序阶段对齐跨模态实体以进行图文。我们利用外部知识和工具构建额外的细粒度视觉-文本对，然后在微调中通过对比学习和实体级掩码建模强调跨模态实体对齐。提出了两种重排序策略，包括一种专门针对零样本场景设计的重排序策略。在多个中英文数据集上使用多个VLP模型进行了广泛的实验，结果表明，所提出方法在微调和零短场景的各种设置下取得了最先进的结果。在少样本场景下的实验也验证了AGREE可以显著降低数据依赖性。我们希望我们的工作可以启发未来在视觉和语言社区的研究。</p> 
 <p><img src="https://images2.imgbox.com/a8/e5/uu2sOWf4_o.gif" alt="0206eaa188a6d367986f7efe76746756.gif"></p> 
 <p>论文&amp;文稿作者</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/6e/c7/EiHNQH4L_o.png" alt="d3c4b8c54b06c166ea7ff726cb487227.png"></p> 
 <p style="text-align:right;">责任编辑：王文</p> 
 <p><img src="https://images2.imgbox.com/d3/0a/Izjn76tP_o.gif" alt="c4761baebcca9e5c504cba1de709f3cb.gif"></p> 
 <p><strong>📝论文解读投稿，让你的文章被更多不同背景、不同方向的人看到，不被石沉大海，或许还能增加不少引用的呦~ 投稿加下面微信备注“投稿”即可。</strong></p> 
 <p><strong>最近文章</strong></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">COLING'22 | SelfMix：针对带噪数据集的半监督学习方法</a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">ACMMM 2022 | 首个针对跨语言跨模态检索的噪声鲁棒研究工作</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">ACM MM 2022 Oral  | PRVR: 新的文本到视频跨模态检索子任务</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">统计机器学习方法 for NLP：基于CRF的词性标注</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">统计机器学习方法 for NLP：基于HMM的词性标注</a></p> 
 <hr> 
 <p><strong>点击这里进群—&gt;<a href="" rel="nofollow">加入NLP交流群</a></strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/166c80ec474cbf7ec3665c6a76d2b9c7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">npm run build打包后页面空白报错：Failed to load resource: net::ERR_FILE_NOT_FOUND</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0f1fb19643d9860704ac8dbbd0d276e4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">cJSON函数用法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>