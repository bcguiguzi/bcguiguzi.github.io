<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>YOLOv5模型部署TensorRT之 FP32、FP16、INT8推理 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="YOLOv5模型部署TensorRT之 FP32、FP16、INT8推理" />
<meta property="og:description" content="引言 YOLOv5最新版本的6.x已经支持直接导出engine文件并部署到TensorRT上了。
FP32推理TensorRT演示
FP32推理TensorRT演示 可能很多人不知道YOLOv5新版本6.x中已经支持一键导出Tensor支持engine文件，而且只需要一条命令行就可以完成：演示如下：
python export.py --weights yolov5s.pt --include onnx engine --device 0 其中onnx表示导出onnx格式的模型文件，支持部署到：
- OpenCV DNN - OpenVINO - TensorRT - ONNXRUNTIME 但是在TensorRT上推理想要速度快，必须转换为它自己的engine格式文件，参数engine就是这个作用。上面的命令行执行完成之后，就会得到onnx格式模型文件与engine格式模型文件。--device 0参数表示GPU 0，因为我只有一张卡！上述导出的FP32的engine文件。
使用tensorRT推理
YOLOv5 6.x中很简单，一条命令行搞定了，直接执行：
python detect.py --weights yolov5s.engine --view-img --source data/images/zidane.jpg FP16推理TensorRT演示 在上面的导出命令行中修改为如下
python export.py --weights yolov5s.onnx --include engine --half --device 0 其中就是把输入的权重文件改成onnx格式，然后再添加一个新的参 --half 表示导出半精度的engine文件。就这样直接执行该命令行就可以导出生成了，图示如下：
对比可以发现相比FP32大小的engine文件，FP16的engine文件比FP32的engine大小减少一半左右，整个文件只有17MB大小左右。
推理执行的命令跟FP32的相同，直接运行，显示结果如下：
对比发现FP32跟FP16版本相比，速度提升了但是精度几乎不受影响！
INT8量化与推理TensorRT演示 TensorRT的INT量化支持要稍微复杂那么一点点，最简单的就是训练后量化。只要完成Calibrator这个接口支持，我用的TensorRT版本是8.4.0.x的，它支持以下几种Calibrator：
不同的量化策略，得到的结果可能稍有差异，另外高版本上的INT8量化之后到低版本的TensorRT机器上可能无法运行，我就遇到过！所以建议不同平台要统一TensorRT版本之后，再量化部署会比较好。上面的Calibrator都必须完成四个方法，分别是：
#使用calibrator验证时候每次张数，跟显存有关系，最少1张 get_batch_size #获取每个批次的图像数据，组装成CUDA内存数据 get_batch #如果以前运行过保存过，可以直接读取量化，低碳给国家省电 read_calibration_cache #保存calibration文件，量化时候会用到 write_calibration_cache 这块对函数集成不懂建议参考TensorRT自带的例子：
TensorRT-8.4.0.6\samples\python\int8_caffe_mnist 几乎是可以直接用的！Copy过来改改就好了！
搞定了Calibrator之后，需要一个验证数据集，对YOLOv5来说，其默认coco128数据集就是一个很好的验证数据，在data文件夹下有一个coco128.yaml文件，最后一行就是就是数据集的下载URL，直接通过URL下载就好啦。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/86a6961d7c5ff9290c147dd89ea56cfb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-10T17:00:10+08:00" />
<meta property="article:modified_time" content="2022-10-10T17:00:10+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">YOLOv5模型部署TensorRT之 FP32、FP16、INT8推理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3><strong>引言</strong></h3> 
<p>YOLOv5最新版本的6.x已经支持直接导出engine文件并部署到TensorRT上了。</p> 
<p>FP32推理TensorRT演示</p> 
<h3><strong>FP32推理TensorRT演示</strong></h3> 
<p>可能很多人不知道YOLOv5新版本6.x中已经支持一键导出Tensor支持engine文件，而且只需要一条命令行就可以完成：演示如下：</p> 
<pre><code class="hljs">python export.py --weights yolov5s.pt --include onnx engine --device 0</code></pre> 
<p>其中onnx表示导出onnx格式的模型文件，支持部署到：</p> 
<pre><code class="hljs">- OpenCV DNN
- OpenVINO
- TensorRT
- ONNXRUNTIME</code></pre> 
<p> 但是在TensorRT上推理想要速度快，必须转换为它自己的engine格式文件，参数engine就是这个作用。上面的命令行执行完成之后，就会得到onnx格式模型文件与engine格式模型文件。--device 0参数表示GPU 0，因为我只有一张卡！上述导出的FP32的engine文件。</p> 
<p>使用tensorRT推理</p> 
<p>YOLOv5 6.x中很简单，一条命令行搞定了，直接执行：</p> 
<pre><code class="hljs">python detect.py --weights yolov5s.engine --view-img --source data/images/zidane.jpg</code></pre> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a5/ab/8NsjDs9u_o.jpg"></p> 
<h3> FP16推理TensorRT演示</h3> 
<p>在上面的导出命令行中修改为如下</p> 
<pre><code class="hljs">python export.py --weights yolov5s.onnx --include engine --half --device 0</code></pre> 
<p> 其中就是把输入的权重文件改成onnx格式，然后再添加一个新的参 --half 表示导出半精度的engine文件。就这样直接执行该命令行就可以导出生成了，图示如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/3e/3c/ZyYoFGpC_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e6/6b/HQCri1yJ_o.png"></p> 
<p>对比可以发现相比FP32大小的engine文件，FP16的engine文件比FP32的engine大小减少一半左右，整个文件只有17MB大小左右。</p> 
<p>推理执行的命令跟FP32的相同，直接运行，显示结果如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/da/72/iB2kelir_o.png"></p> 
<p> 对比发现FP32跟FP16版本相比，速度提升了但是精度几乎不受影响！</p> 
<h3>INT8量化与推理TensorRT演示</h3> 
<p>TensorRT的INT量化支持要稍微复杂那么一点点，最简单的就是训练后量化。只要完成Calibrator这个接口支持，我用的TensorRT版本是8.4.0.x的，它支持以下几种Calibrator：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a7/3d/whVoM1s1_o.png"></p> 
<p> 不同的量化策略，得到的结果可能稍有差异，另外高版本上的INT8量化之后到低版本的TensorRT机器上可能无法运行，我就遇到过！所以建议不同平台要统一TensorRT版本之后，再量化部署会比较好。上面的Calibrator都必须完成四个方法，分别是：</p> 
<pre><code class="hljs">#使用calibrator验证时候每次张数，跟显存有关系，最少1张
get_batch_size  
#获取每个批次的图像数据，组装成CUDA内存数据
get_batch  
#如果以前运行过保存过，可以直接读取量化，低碳给国家省电
read_calibration_cache  
#保存calibration文件，量化时候会用到
write_calibration_cache </code></pre> 
<p> 这块对函数集成不懂建议参考TensorRT自带的例子：</p> 
<pre><code class="hljs">TensorRT-8.4.0.6\samples\python\int8_caffe_mnist</code></pre> 
<p> 几乎是可以直接用的！Copy过来改改就好了！</p> 
<p>搞定了Calibrator之后，需要一个验证数据集，对YOLOv5来说，其默认coco128数据集就是一个很好的验证数据，在data文件夹下有一个coco128.yaml文件，最后一行就是就是数据集的下载URL，直接通过URL下载就好啦。</p> 
<p>完成自定义YOLOv5的Calibrator之后，就可以直接读取onnx模型文件，跟之前的官方转换脚本非常相似了，直接在上面改改，最重要的配置与生成量化的代码如下：</p> 
<pre><code class="hljs"># build trt engine
builder.max_batch_size = 1
config.max_workspace_size = 1 &lt;&lt; 30
config.set_flag(trt.BuilderFlag.INT8)
config.int8_calibrator = calibrator
print('Int8 mode enabled')
plan = builder.build_serialized_network(network, config)</code></pre> 
<p>主要就是设置config中的flag为INT8，然后直接运行，得到plan对象，反向序列化为engine文件，保存即可。最终得到的INT8量化engine文件的大小在9MB左右。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/fc/d6/x7KnKCkQ_o.png"></p> 
<p><strong>数据太少，只有128张， INT8量化之后的YOLOv5s模型推理结果并不尽如人意</strong>。但是我也懒得再去下载COCO数据集， COCO训练集一半数据作为验证完成的量化效果是非常好。</p> 
<p>这里，<strong>我基于YOLOv5s模型自定义数据集训练飞鸟跟无人机，对得到模型，直接用训练集270张数据做完INT8量化之后的推理效果如下</strong>：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/bf/da/QLrMQJkY_o.png"></p> 
<p>量化效果非常好，精度只有一点下降，但是速度比FP32的提升了1.5倍左右（3050Ti）。</p> 
<h3>已知问题与解决</h3> 
<p><strong>量化过程遇到这个错误</strong></p> 
<pre><code class="hljs">[09/22/2022-23:01:13] [TRT] [I]   Calibrated batch 127 in 0.30856 seconds.
[09/22/2022-23:01:16] [TRT] [E] 2: [quantization.cpp::nvinfer1::DynamicRange::DynamicRange::70] Error Code 2: Internal Error (Assertion min_ &lt;= max_ failed. )
[09/22/2022-23:01:16] [TRT] [E] 2: [builder.cpp::nvinfer1::builder::Builder::buildSerializedNetwork::619] Error Code 2: Internal Error (Assertion engine != nullptr failed. )
Failed to create the engine
Traceback (most recent call last):</code></pre> 
<p> <strong>解决方法</strong>，把Calibrator中getBtach方法里面的代码：</p> 
<pre><code class="hljs">img = np.ascontiguousarray(img, dtype=np.float32)</code></pre> 
<p></p> 
<p><strong>to</strong></p> 
<pre><code class="hljs">img = np.ascontiguousarray(img, dtype=np.float16)</code></pre> 
<p> 这样就可以避免量化失败。</p> 
<pre><code class="hljs">https://github.com/NVIDIA/TensorRT/issues/1634</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6ce83f311d9b783febb90795f2cee6c9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">音频深度学习变得简单1：最先进的技术</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1ae448932f9147921e6d5314867d8afd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Git基本操作之 修改提交信息 取消暂存文件 撤销文件修改</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>