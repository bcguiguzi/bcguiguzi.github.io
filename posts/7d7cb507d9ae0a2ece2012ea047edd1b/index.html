<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【自然语言处理五-注意力其他--多头注意力&amp;位置编码等】 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【自然语言处理五-注意力其他--多头注意力&amp;位置编码等】" />
<meta property="og:description" content="自然语言处理五-self attention的其他问题（多头注意力&amp;位置编码）等 多头注意力 multi-head attention多头注意力的意义是什么？多头注意力的运作流程 位置编码postion encoding 之前用四篇博客说明了注意力以及自注意力的运作流程，下面来介绍剩余点。
多头注意力、位置编码
多头注意力 multi-head attention 单头的注意力是这样的：
而多头的矩阵是这样的：
也就是说我们由输入生成的q k v矩阵是多组，上面的图就显示2头注意力的示意图。
多头注意力的意义是什么？ 事物之间的关系，往往不止一种，多头就意味着多种不同类型的相关性
多头注意力的运作流程 单头的注意力的过程是这样的：
而多头的注意力，计算注意力分数、softmax等操作都是相同的，只不过会有两次这样的操作，最终每一个输入对应的bi会生成多个，以两头注意力为例子：
第一次生成bi,1，第二次生成bi,2,下面是示例了生成bi,2的过程：
而注意力层最终的输出是将(bi,1,bi,2)又做了一次矩阵乘法
整体注意力层对外的输出的bi就整合了多头(bi,1,bi,2)的信息了。
位置编码postion encoding 前面讲的自注意力其实缺少了一部分，没有任何关于位置的信息。
但是在自然语言处理领域，位置信息有时候很重要，比如词性识别的时候，动词在开头的概率一般很小。因此self attention中又加入了位置信息编码：
postion encoding，具体的做法就是在输入加上一个位置信息向量ei，Q K V的信息中就包括了位置的信息，如下图：
这个ei的生成有多重方法：
1.transformer论文中 用sin和cos的函数
2.手动设置
3.其他。在其他的论文中还有很多种做法" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/7d7cb507d9ae0a2ece2012ea047edd1b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-02T15:22:11+08:00" />
<meta property="article:modified_time" content="2024-03-02T15:22:11+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【自然语言处理五-注意力其他--多头注意力&amp;位置编码等】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>自然语言处理五-self attention的其他问题（多头注意力&amp;位置编码）等</h4> 
 <ul><li><a href="#_multihead_attention_5" rel="nofollow">多头注意力 multi-head attention</a></li><li><ul><li><a href="#_12" rel="nofollow">多头注意力的意义是什么？</a></li><li><a href="#_14" rel="nofollow">多头注意力的运作流程</a></li></ul> 
  </li><li><a href="#postion_encoding_26" rel="nofollow">位置编码postion encoding</a></li></ul> 
</div> 
<p></p> 
<p>之前用四篇博客说明了注意力以及自注意力的运作流程，下面来介绍剩余点。<br> 多头注意力、位置编码</p> 
<h2><a id="_multihead_attention_5"></a>多头注意力 multi-head attention</h2> 
<p>单头的注意力是这样的：<br> <img src="https://images2.imgbox.com/1b/47/4jNOLfp8_o.png" alt="在这里插入图片描述"><br> 而多头的矩阵是这样的：<br> <img src="https://images2.imgbox.com/a6/1c/0NIqxxZd_o.png" alt="在这里插入图片描述"><br> 也就是说我们由输入生成的q k v矩阵是多组，上面的图就显示2头注意力的示意图。</p> 
<h3><a id="_12"></a>多头注意力的意义是什么？</h3> 
<p>事物之间的关系，往往不止一种，多头就意味着多种不同类型的相关性</p> 
<h3><a id="_14"></a>多头注意力的运作流程</h3> 
<p>单头的注意力的过程是这样的：<br> <img src="https://images2.imgbox.com/74/fb/0DUoJf7n_o.png" alt="在这里插入图片描述"><br> 而多头的注意力，计算注意力分数、softmax等操作都是相同的，只不过会有两次这样的操作，最终每一个输入对应的b<sup>i</sup>会生成多个，以两头注意力为例子：<br> 第一次生成b<sup>i,1</sup>，第二次生成b<sup>i,2</sup>,下面是示例了生成b<sup>i,2</sup>的过程：<br> <img src="https://images2.imgbox.com/d6/e9/8aOen9lB_o.png" alt="在这里插入图片描述"></p> 
<p>而注意力层最终的输出是将(b<sup>i,1</sup>,b<sup>i,2</sup>)又做了一次矩阵乘法<br> <img src="https://images2.imgbox.com/a3/58/mme3VSw9_o.png" alt="在这里插入图片描述"></p> 
<p><strong>整体注意力层对外的输出</strong>的b<sup>i</sup>就整合了多头(b<sup>i,1</sup>,b<sup>i,2</sup>)的信息了。</p> 
<h2><a id="postion_encoding_26"></a>位置编码postion encoding</h2> 
<p>前面讲的自注意力其实缺少了一部分，没有任何关于位置的信息。<br> 但是在自然语言处理领域，位置信息有时候很重要，比如词性识别的时候，动词在开头的概率一般很小。因此self attention中又加入了位置信息编码：<br> postion encoding，具体的做法就是在输入加上一个位置信息向量e<sup>i</sup>，Q K V的信息中就包括了位置的信息，如下图：<br> <img src="https://images2.imgbox.com/1d/0a/hpe1pREe_o.png" alt="在这里插入图片描述"><br> 这个e<sup>i</sup>的生成有多重方法：<br> 1.transformer论文中 用sin和cos的函数<br> <img src="https://images2.imgbox.com/af/74/0Del9qm6_o.png" alt="在这里插入图片描述"></p> 
<p>2.手动设置<br> 3.其他。在其他的论文中还有很多种做法</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d64cac382b558641c646a3bc39e7677a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PMOS 应用电路</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/16ca42258eefa959e177bbbdddf9db3d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">PYTHON&#43;EXCEL学习笔记1：glob查找</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>