<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【跟李沐学AI—机器学习】2 神经网络 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【跟李沐学AI—机器学习】2 神经网络" />
<meta property="og:description" content="文章目录 1 多层感知机（MLP Multilayer Perceptron）2 卷积神经网络（CNN Convolution Neural Net）3 循环神经网络（RNN Recurrent Neural Networks ） 1 多层感知机（MLP Multilayer Perceptron） dense layer/全连接层，y=wx&#43;b
线性回归就是一个全连接层，即单层感知机
多层感知机是一种前向结构的人工神经网络，包含输入层、输出层及多个隐藏层
由于多个全连接层叠加还是线性关系，所以要在其中加入非线性函数，即激活函数
2 卷积神经网络（CNN Convolution Neural Net） 由于MLP使用的是全连接层，每经过一层都会有几个数量级参数的增加，参数太多。
假设要在某图像中寻找一个戴白帽子的人，即使这个戴白帽子的人在图像中的位置变化了，但是这块区域的像素信息不变，即平移不变性。
同时这块区域和附近的像素信息具有相关性，即本地性。
卷积则利用了这两个性质。
卷积层可以通过滑动窗口对像素矩阵进行一块一块进行加权和计算，这个权重矩阵（大小为k*k）为数值一样的方阵，叫卷积核。
**池化层/汇聚层：**当搜寻的区域发生平移时，这片区域的像素最大值/平均值也会发生平移，只要抓住这个最大值/平均值就可以补偿平移带来的变化。
3 循环神经网络（RNN Recurrent Neural Networks ） 多用于语言模型，带有时序信息
假设有一句话是“hello world!”。
先输入“hello”，通过全连接层预测得到“world”，先不进行softmax操作，把这个得到的、带有之前所有信息的“world”作为输入，传给下一个全连接层，再对“world”进行softmax
进行下一个词语的预测时，输入就有①带有之前所有信息的“world”和②当前信息的“world”
其中这个带有之前所有信息的“world”为隐藏状态，不论之后还有多少层，这个“world”包含的信息不变，包含过去时间所有的信息
简化版RNN：
图中的RNN即全连接层
输入有 当前时刻的xt 和 上一时刻的输出ht-1
输出 yt 和 ht（简单版中它们是同一个东西），ht 留给下一时刻的RNN
输出后还会有激活函数
更复杂的RNN（LSTM、GRU）：
忽略当前的输入xt，比如空格、介词等不太重要的信息忽视以前的信息ht-1，比如一个段落开始了，或者前面的信息实在是太远了不需要了忽视的时间也需要另外一组权重来学习 更更复杂的RNN：
Bi-RNN
双向，输入有时刻t&#43;1和t-1的信息，有两个子层同时走，但起点相反、方向相反
Deep RNN
叠加很多RNN层" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/5bfe5c6b9f5a1ab545ed4be2b0cf1ae2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-11T13:09:49+08:00" />
<meta property="article:modified_time" content="2023-12-11T13:09:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【跟李沐学AI—机器学习】2 神经网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#1_MLP_Multilayer_Perceptron_1" rel="nofollow">1 多层感知机（MLP Multilayer Perceptron）</a></li><li><a href="#2_CNN_Convolution_Neural_Net_13" rel="nofollow">2 卷积神经网络（CNN Convolution Neural Net）</a></li><li><a href="#3_RNN_Recurrent_Neural_Networks__30" rel="nofollow">3 循环神经网络（RNN Recurrent Neural Networks ）</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="1_MLP_Multilayer_Perceptron_1"></a>1 多层感知机（MLP Multilayer Perceptron）</h3> 
<p>dense layer/全连接层，y=wx+b</p> 
<p>线性回归就是一个全连接层，即单层感知机</p> 
<p>多层感知机是一种前向结构的人工神经网络，包含输入层、输出层及多个隐藏层</p> 
<p>由于多个全连接层叠加还是线性关系，所以要在其中加入非线性函数，即激活函数</p> 
<p><img src="https://images2.imgbox.com/23/d7/ZVgIde4u_o.png" alt="在这里插入图片描述" width="700"></p> 
<h3><a id="2_CNN_Convolution_Neural_Net_13"></a>2 卷积神经网络（CNN Convolution Neural Net）</h3> 
<p>由于MLP使用的是全连接层，每经过一层都会有几个数量级参数的增加，参数太多。</p> 
<p>假设要在某图像中寻找一个戴白帽子的人，即使这个戴白帽子的人在图像中的位置变化了，但是这块区域的像素信息不变，即<strong>平移不变性。</strong></p> 
<p>同时这块区域和附近的像素信息具有相关性，即<strong>本地性。</strong></p> 
<p>卷积则利用了这两个性质。</p> 
<p>卷积层可以通过滑动窗口对像素矩阵进行一块一块进行加权和计算，这个权重矩阵（大小为k*k）为数值一样的方阵，叫卷积核。</p> 
<p>**池化层/汇聚层：**当搜寻的区域发生平移时，这片区域的像素最大值/平均值也会发生平移，只要抓住这个最大值/平均值就可以补偿平移带来的变化。</p> 
<p><img src="https://images2.imgbox.com/38/f1/dKVKWSej_o.png" alt="在这里插入图片描述" width="900"></p> 
<h3><a id="3_RNN_Recurrent_Neural_Networks__30"></a>3 循环神经网络（RNN Recurrent Neural Networks ）</h3> 
<p>多用于语言模型，带有时序信息</p> 
<p>假设有一句话是“hello world!”。</p> 
<p>先输入“hello”，通过全连接层预测得到“world”，先不进行softmax操作，把这个得到的、<strong>带有之前所有信息</strong>的“world”作为输入，传给下一个全连接层，再对“world”进行softmax</p> 
<p>进行下一个词语的预测时，输入就有①<strong>带有之前所有信息</strong>的“world”和②当前信息的“world”</p> 
<p>其中这个<strong>带有之前所有信息</strong>的“world”为隐藏状态，不论之后还有多少层，这个“world”包含的信息不变，包含过去时间所有的信息</p> 
<p><img src="https://images2.imgbox.com/84/ba/MrO0vQ4V_o.png" alt="在这里插入图片描述" width="350"></p> 
<p>简化版RNN：</p> 
<p>图中的RNN即全连接层</p> 
<p>输入有 当前时刻的xt 和 上一时刻的输出ht-1</p> 
<p>输出 yt 和 ht（简单版中它们是同一个东西），ht 留给下一时刻的RNN</p> 
<p>输出后还会有激活函数</p> 
<p><img src="https://images2.imgbox.com/86/a6/rFNVhCQg_o.png" alt="在这里插入图片描述" width="450"></p> 
<p>更复杂的RNN（LSTM、GRU）：</p> 
<ol><li>忽略当前的输入xt，比如空格、介词等不太重要的信息</li><li>忽视以前的信息ht-1，比如一个段落开始了，或者前面的信息实在是太远了不需要了</li><li>忽视的时间也需要另外一组权重来学习</li></ol> 
<p>更更复杂的RNN：</p> 
<ol><li> <p>Bi-RNN</p> <p>双向，输入有时刻t+1和t-1的信息，有两个子层同时走，但起点相反、方向相反</p> </li><li> <p>Deep RNN</p> <p>叠加很多RNN层</p> </li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e1ddad779708121664823dd946f1c035/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【NLP】RAG 应用中的调优策略</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/67d9d4ab3447a6306d03df9da7227623/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">计算机专业答辩技巧详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>