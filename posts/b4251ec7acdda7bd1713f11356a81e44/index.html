<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>神经网络-注意力机制 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="神经网络-注意力机制" />
<meta property="og:description" content="1 注意力简介 计算机视觉（computer vision）中的注意力机制（attention）主要是想让系统学会把注意力放在感兴趣的地方，具备注意力机制的神经网络能够自主学习注意力机制。近几年来，深度学习与视觉注意力机制结合的研究工作，大多数是集中于使用掩码(mask)来形成注意力机制。掩码的原理在于通过另一层新的权重，将图片数据中关键的特征标识出来，通过学习训练，让深度神经网络学到每一张新图片中需要关注的区域，也就形成了注意力。
1.1 引入注意力机制的原因 1、计算能力的限制：目前计算能力依然是限制神经网络发展的瓶颈，当输入的信息过多时，模型也会变得更复杂，通过引入注意力，可以减少处理的信息量，从而减小需要的计算资源。
2、优化算法的限制：虽然局部连接、权重共享以及池化 pooling 等优化操作可以让神经网络变得简单一些，有效缓解模型复杂度和表达能力之间的矛盾。但是，比如循环神经网络中的长序列输入，信息“记忆”能力并不高。
2 注意力机制的分类 1、聚焦式（Focus）注意力：是一种自上而下的有意识的注意力，“主动注意” 是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力。
2、显著性（Saliency-Based）注意力：是一种自下而上的无意识的注意力，“被动注意” 是基于显著性的注意力，是由外界刺激驱动的注意，不需要主动干预，也和任务无关。池化（Max Pooling） 和 门控（Gating） 可以近似地看作是自下而上的基于显著性的注意力机制。
3 注意力的形式 注意力有两种形式，一种是软性注意力(soft attention)，另一种则是硬性注意力(hard attention)。
软性注意力更关注区域或者通道，而且软性注意力是确定性的注意力，学习完成后直接可以通过网络生成，最关键的地方是软性注意力是可微的，这是一个非常重要的地方。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重。
硬性注意力与软性注意力不同点在于，首先硬性注意力是更加关注点，也就是图像中的每个点都有可能延伸出注意力，同时硬性注意力是一个随机的预测过程，更强调动态变化。当然，最关键是硬性注意力是一个不可微的注意力，训练过程往往是通过增强学习(reinforcement learning)来完成的。
注意力机制一般可以分为三步：一信息输入；二计算注意力分布；三根据计算的注意力分布来对输入信息进行处理。
令：
为输入向量 为N个输入样本 为查询向量或特征向量 为注意力变量，表示被选择信息的位置，比如 z = i 表示选择了第 i 个输入向量 3.1 软性注意力 软性注意力（Soft Attention）是指在选择信息的时候，不是从 N 个信息中只选择1个，而是计算 N 个输入信息的加权平均，再输入到神经网络中计算。
1、计算注意力分布 在给定 q 和 X 下，选择第 i 个输入向量的概率 为：
其中：
称为注意力分布
s(xi​,q)称为注意力打分函数
2、加权平均
注意力分布α i 可以解释为给定查询 q 时，第 i 个输入向量关注的程度，软性注意力选择机制是对它们进行汇总：
3.2 硬性注意力 硬性注意力（Hard Attention）是指只选择输入序列某一个位置上的信息。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/b4251ec7acdda7bd1713f11356a81e44/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-15T17:23:58+08:00" />
<meta property="article:modified_time" content="2022-06-15T17:23:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">神经网络-注意力机制</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1 注意力简介</h2> 
<p>计算机视觉（computer vision）中的<strong>注意力机制</strong>（attention）主要是想让系统学会把注意力放在感兴趣的地方，具备注意力机制的神经网络能够自主学习注意力机制。近几年来，深度学习与视觉注意力机制结合的研究工作，大多数是集中于使用掩码(mask)来形成注意力机制。掩码的原理在于通过另一层新的权重，将图片数据中关键的特征标识出来，通过学习训练，让深度神经网络学到每一张新图片中需要关注的区域，也就形成了注意力。</p> 
<h3>1.1 引入注意力机制的原因</h3> 
<p><strong>1、计算能力的限制</strong>：目前计算能力依然是限制神经网络发展的瓶颈，当输入的信息过多时，模型也会变得更复杂，通过引入注意力，可以减少处理的信息量，从而减小需要的计算资源。<br><strong>2、优化算法的限制</strong>：虽然局部连接、权重共享以及池化 pooling 等优化操作可以让神经网络变得简单一些，有效缓解模型复杂度和表达能力之间的矛盾。但是，比如循环神经网络中的长序列输入，信息“记忆”能力并不高。</p> 
<h2>2 注意力机制的分类</h2> 
<p><strong>1、聚焦式（Focus）注意力</strong>：是一种自上而下的有意识的注意力，“主动注意” 是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力。<br><strong>2、显著性（Saliency-Based）注意力</strong>：是一种自下而上的无意识的注意力，“被动注意” 是基于显著性的注意力，是由外界刺激驱动的注意，不需要主动干预，也和任务无关。池化（Max Pooling） 和 门控（Gating） 可以近似地看作是自下而上的基于显著性的注意力机制。</p> 
<h2>3 注意力的形式</h2> 
<p>注意力有两种形式，一种是<strong>软性注意力</strong>(soft attention)，另一种则是<strong>硬性注意力</strong>(hard attention)。</p> 
<p>软性注意力更关注区域或者通道，而且软性注意力是<strong>确定性</strong>的注意力，学习完成后直接可以通过网络生成，最关键的地方是<strong>软性注意力是可微的</strong>，这是一个非常重要的地方。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重。</p> 
<p>硬性注意力与软性注意力<strong>不同点</strong>在于，首先硬性注意力是更加关注点，也就是图像中的每个点都有可能延伸出注意力，同时硬性注意力是一个随机的预测过程，更强调动态变化。当然，最关键是硬性注意力是一个不可微的注意力，训练过程往往是通过增强学习(reinforcement learning)来完成的。</p> 
<p>注意力机制一般可以分为三步：<strong>一信息输入</strong>；<strong>二计算注意力分布</strong>；<strong>三根据计算的注意力分布来对输入信息进行处理</strong>。</p> 
<p>令：</p> 
<ol><li><img alt="x\in R^{d}" class="mathcode" src="https://images2.imgbox.com/e0/0c/GTCBzMTm_o.png"> 为输入向量</li><li><img alt="X = \left [ x_{1},x_{2},... ,x_{N} \right ]" class="mathcode" src="https://images2.imgbox.com/a0/c4/UCDCmspp_o.png">  为N个输入样本</li><li><img alt="q\in R^{k}" class="mathcode" src="https://images2.imgbox.com/43/a8/pL7IJvO4_o.png"> 为查询向量或特征向量</li><li><img alt="z\in \left [ 1,N \right ]" class="mathcode" src="https://images2.imgbox.com/5d/fa/zt9YHWns_o.png">  为注意力变量，表示被选择信息的位置，比如 z = i 表示选择了第 i 个输入向量</li></ol> 
<h3>3.1 软性注意力</h3> 
<p>软性注意力（Soft Attention）是指在选择信息的时候，不是从 N 个信息中只选择1个，而是计算 N 个输入信息的加权平均，再输入到神经网络中计算。</p> 
<p><strong>1、计算注意力分布 <img alt="\alpha _{i}" class="mathcode" src="https://images2.imgbox.com/5f/2f/CZNtq03N_o.png"></strong></p> 
<p>在给定 q 和 X  下，选择第 i  个输入向量的概率 <img alt="\alpha _{i}" class="mathcode" src="https://images2.imgbox.com/f8/86/KLXPPeE0_o.png">为：</p> 
<p><img alt="\alpha _{i} = p(z = i\parallel X , q)" class="mathcode" src="https://images2.imgbox.com/a0/92/YlKCzD1O_o.png"></p> 
<p><strong>      <img alt="= softmax(s(x_{i , q}))" class="mathcode" src="https://images2.imgbox.com/38/1e/xHHBIzKf_o.png"></strong></p> 
<p>       <img alt="= \frac{exp(s(x_{i} , q))}{\sum_{j=1}^{N} exp(s(x_{i} , q))}" class="mathcode" src="https://images2.imgbox.com/c1/8c/8vtDbLvY_o.png"></p> 
<p>其中：</p> 
<p> <img alt="\alpha _{i}" class="mathcode" src="https://images2.imgbox.com/1e/d1/3Dsa5oTq_o.png"> 称为注意力分布</p> 
<p>s(xi​,q)称为注意力打分函数</p> 
<p>2、加权平均</p> 
<p>注意力分布α i 可以解释为给定查询 q  时，第 i 个输入向量关注的程度，软性注意力选择机制是对它们进行汇总：<br><img alt="att(X , q) = \sum_{i=1}^{N}\alpha _{i}x_{i}" class="mathcode" src="https://images2.imgbox.com/0c/79/0jyPmf1u_o.png"></p> 
<p>                <img alt="= E_{z\sim p(z|X , p)}\left [ x \right ]" class="mathcode" src="https://images2.imgbox.com/55/fa/NRUI4aht_o.png"></p> 
<h3>3.2 硬性注意力</h3> 
<p>硬性注意力（Hard Attention）是指只选择输入序列某一个位置上的信息。</p> 
<p>硬性注意力有两种实现方式：</p> 
<p>（1）选取最高概率的输入信息</p> 
<p><img alt="att(X , q) = x_{i} , where" class="mathcode" src="https://images2.imgbox.com/f6/2b/CEh0iVOG_o.png">  <img alt="j = arg" class="mathcode" src="https://images2.imgbox.com/59/c2/jpAfTBG7_o.png">  <img alt="max_{i=1}^{N} \alpha _{i}" class="mathcode" src="https://images2.imgbox.com/c2/3a/MbGxjz96_o.png"></p> 
<p>（2）另一种硬性注意力是通过在注意力分布式上随机采样的方式实现</p> 
<p><strong>硬性注意力的缺点：</strong></p> 
<p>硬性注意力基于<strong>最大采样</strong>或者<strong>随机采样</strong>的方式选择信息，导致最终的损失函数与注意力分布之间的函数关系不可导，因此无法在反向传播时进行训练。</p> 
<p></p> 
<p>参考：</p> 
<p>原文链接：<a href="https://blog.csdn.net/coffee_cream/article/details/109095154" title="https://blog.csdn.net/coffee_cream/article/details/109095154">https://blog.csdn.net/coffee_cream/article/details/109095154</a></p> 
<p>原文链接：<a href="https://blog.csdn.net/weixin_39552472/article/details/110746151" title="https://blog.csdn.net/weixin_39552472/article/details/110746151">https://blog.csdn.net/weixin_39552472/article/details/110746151</a></p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8f86781e2293286e28c5f2d85918d55a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Retrofit2基础使用与解析：从使用步骤深入分析源码</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cec9d5e3be6814efa1c578e3f09da869/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MinGW-w64 - for 32 and 64 bit Windows</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>