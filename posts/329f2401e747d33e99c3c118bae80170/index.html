<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习各算法思想（极简版） - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习各算法思想（极简版）" />
<meta property="og:description" content="读到的一篇不错的文章，拿来和大家分享一下。 转自–头条公众号–极数蜗牛
（1）线性回归
回归最早是由高尔顿研究子女身高与父母身高遗传关系提出的，发现子女平均身高总是向中心回归而得名。其实“一分辛苦一分才”中就蕴含了线性回归算法思想，比较简单表现出才能与辛苦是正比关系。另外，经常听说“成功=3分才能&#43;6分机会&#43;1分贵人帮”，这是标准的线性回归方程，其中成功是因变量；才能、机会和贵人是自变量；而自变量前边的3、6和1是权重。
（2）K-聚类
中国有句古话是“物以类聚，人以群分”，其实已经蕴含了聚类算法的基本思想。比如说人，可以根据年龄分70后、80后、90后等；根据区域分北京、上海、广东等；根据性别分男和女。人可以根据不同特征属性进行划分，而在聚类算法中是根据不同方式来计算两个事物的距离，如欧氏距离、皮尔森相似度等。
假如根据年龄可以每10年份划分为70后、80后、90后等，也可以根据成长周期分童年、少年、青年、中年和老年。因此特征刻度大小决定了群体的范围，这反映在聚类算法中就是通过不同方法处理事物间距离，来确定事物属于哪个群体，如根据平均值、最大值等。
其中K值是表示划分群体的大小，如以区域为例，K=34，则划分为全国省份；K=7，则划分为东北，中原，华东，华北，华南，西北和西部等；K=2，则划分为南方和北方。
（3）K-邻近
中国还有句古话是“近朱者赤近墨者黑”，该句也蕴含了K-邻近算法的思想。比如判断一个人是否是有钱人，可以根据其最近联系的人群中，有钱人的比例来推测。这就需要解决两个问题，一是如何确定最近联系人，二是如何计算有钱人比例。这反映在K-邻近算法中就是首先确定不同事物样本的距离，然后确定K值的大小，根据K值内的有钱人占比，来预测未知用户的状态。
K值的大小将会直接决定预测结果，假如你有5个有钱人朋友，当K=8时，判定你为有钱人；但当K=12时，则判定你不是有钱人。因此在该算法中K的选择至关重要。
（4）朴素贝叶斯
“吃一亏长一智”反映了朴素贝叶斯算法思维，就是通过后验经验法，来对未知的预测。假如你经常买水果，发现10个青苹果里边8个都是酸的，而10个红苹果里有7个都是甜的，这种经验告诉以后再挑选苹果时，红苹果7/10是甜的，青苹果2/10是甜的，如果你喜欢甜的，那就选红苹果吧。
（5）决策树
在婚恋相亲时经常被问到“你有车吗？你有房吗？你有钱吗？”，这和决策树的思维过程极其相似。决策树是由树枝，树叶，节点组成的树型结构，其中每个节点就是一个问题或特征（如你有车吗？），每个树枝是问题的走向（如有），每个节点就是答案（相亲成功）。 （6）主成分分析
经常在网上看到两个字“干货”。那怎么定义“干货”，我觉得应该包括两方面：一是信息量大，二是没有废话。其实如何将“水货”制作成干货的过程，与主成分分析有异曲同工之妙。“干货”能够使原文到达“短小精悍”，而主成分分析能够实现数据集降维，即用较少维度表示原有样本含有的信息，两则都是通过其它语言或转变维度来表达原有信息。
“水货”变成“干货”就是将意思相近或相似的句子进行浓缩或提炼，也就是将“水货”里的的水分拧干；而主成分分析是根据样本集的协方差矩阵，通过线性变换将原数据映射到新的坐标系统，并将差异性较大特征值的保留，以到达降维目的。
（7）随机森林
“三个臭皮匠赛过诸葛亮”与随机森林算法内核类似。随机森林是是由一棵棵的决策树构成的，每决策树的形成都是随机的，它可以避免单一决策树过拟合和偏向的毛病。
再以相亲为例，对相亲对象要求，你可能看重“有房”“有车”“有钱”；你妈看重“有房”“孝顺”；你爸看重“事业”“顾家”“有车”等。其实你们每个人都是一个决策树，可根据自己判断标准决策出相亲对手是否“满意”，最后集合每个人的决策结果，来判断最后是否相亲成功。一个人相亲是决策树，全家人相亲就是随机森林。
（8）最大熵模型
“不要把鸡蛋放在一个篮子里”是最大熵模型比较朴素的说法，也反映了该算法的本质，就是对不确定的或未知的，尽量保持随机和均匀分布，能够将风险降到最低。其实在生活中大家应该都不自觉的应用了该模型。比如，去年P2P较火的时候，很多人被其高收益吸引，但由于P2P鱼龙混杂，又担心跑路；因此采取比较保险的举措，就是多投几家公司。
其实，熵是对无序状态的描述，而最大熵就是表示样本是均匀分布，可能性概率相同。
（9）AdaBoost
在学生时代，考试有个技巧就是构建自己的“错题本”，每次考试前都加强对“错题本”学习，通过不断强化“错题本”上题目，最终可能获得较高分数。其实这个学习过程与AdaBoost是算法逻辑是相同的。
假设每次考试作为一次模型训练，每道题目作为一个样本，分数作为预测准确率，而“错题本”就是预测错误的样本；当再次进行预测训练考试的时候，AdaBoost算法策略就是会对上次预测“错误的样本”加大权重，并以此不断迭代，通过多次训练，最后能够组合成一个较强的分类器（即考试高分）。
（10）关联规则
是否耳熟“我看你天赋异禀、骨骼惊奇，想来是百年难得一见的练武奇才”“贫道夜观天象，发现北斗星南移，天狼星耀青光，帝王星显现”等台词。其实这里边就蕴含了关联规则，通过经验积累发现骨骼与练武，北斗星与帝王等之间关联。
“用生辰八字来算命”虽然被成为伪科学，但偶尔能算准，这是这么回事？用关联规则算法就容易解释，首先理解两个概念支持度和置信度。
支持度是指A（某生辰八字）和B（某命运）同时发生的占比，如某生辰对应某命运的人数占总人数比值；置信度是指A发生后B发生的概率，如某生辰中当官的人数/某生辰总人数。如果置信度是100%，如果A发生，那么B一定发生。算命先生就将生辰和命运的置信度定为100%。
如果算命先生学过机器学习算法，就不会很肯定指出你将来一定当官，而是说你将来当官的支持度为20%，置信度为30%。
（11）逻辑回归
逻辑回归与线性回归都是广义线性模型，只所以在回归前加上“逻辑”，是因为他在线性回归的基础上穿上了一件马甲（转变函数）。比如人的成功公式： “成功=3分才能&#43;6分机会&#43;1分贵人帮”，但通过计算可能得出如1,5,99,200等各种数字，如果就想知道是否是成功人士，你就需要一个“成功评委”来对结果进行评价，最终输出量化指标，如成功率是80%，其中越接近100%，说明越成功。而这个“成功评委”就是一个转变函数。
（12）因子分析
中国有句古话是“三岁看老”和“性格决定命运”，这与因子分析的思维类似，就是将影响或决定事物的本质东西总结出来。网络上将有喝绿茶饮料，穿361运动鞋，周末在家打游戏，留着平头等特征行为的人，称为具有“屌丝”气质。因子分析过程也类似，就是对具有关联行为或相似事件，进行共性因子提取，将具有共同本质的特征行为归为一个因子。像土豪，即使有钱了，依然没有改变“土”的因子。
（13）人工神经网络
其实全国人民大会代表选举过程与人工神经网络运算流程是类似。首先由基层人民选取乡/县级人民代表，再有县级选举市级，由市级选举省级，最后产生全国人民代表。其实，在选举过程中，每个人/代表相当于人工神经网络的一个神经元，而县、市、省等行政级别，相当于人工神经网络的层级，最后选取的代表相当于输出结果。层级越多代表越复杂，深度学习就是多层神经网络。现在终于明白国家领导人考察农村，叫做深入基层。
（14）SVM（支持向量机）
以相亲为例，假如你根据学历，身高，年龄，相貌等指标，综合评估下来依然有较多候选人，你不知道这么办？这时候你闺蜜告诉你个方法：“就是看看他的父母和朋友情况”，这时候你豁然开朗，根据他的父母健康并是高官，朋友都年轻有为等指标，很快就确定了候选者。其实这个过程与SVM模型的思维逻辑相同的。SVM的核心思维就是将低维数据（一个人）映射到高维空间（多个人），从而实现数据可分（可选择）；你闺蜜的方法在SVM中就是核函数。这是一个拼“爹”的时代。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/329f2401e747d33e99c3c118bae80170/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-04-20T20:04:14+08:00" />
<meta property="article:modified_time" content="2017-04-20T20:04:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习各算法思想（极简版）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>读到的一篇不错的文章，拿来和大家分享一下。 <br> 转自–头条公众号–极数蜗牛</p> 
<p>（1）线性回归</p> 
<p>回归最早是由高尔顿研究子女身高与父母身高遗传关系提出的，发现子女平均身高总是向中心回归而得名。其实“一分辛苦一分才”中就蕴含了线性回归算法思想，比较简单表现出才能与辛苦是正比关系。另外，经常听说“成功=3分才能+6分机会+1分贵人帮”，这是<strong>标准的线性回归方程，其中成功是因变量；才能、机会和贵人是自变量；而自变量前边的3、6和1是权重</strong>。</p> 
<p>（2）K-聚类</p> 
<p>中国有句古话是“物以类聚，人以群分”，其实已经蕴含了聚类算法的基本思想。比如说人，可以根据年龄分70后、80后、90后等；根据区域分北京、上海、广东等；根据性别分男和女。人可以根据不同特征属性进行划分，而<strong>在聚类算法中是根据不同方式来计算两个事物的距离</strong>，如欧氏距离、皮尔森相似度等。</p> 
<p>假如根据年龄可以每10年份划分为70后、80后、90后等，也可以根据成长周期分童年、少年、青年、中年和老年。<strong>因此特征刻度大小决定了群体的范围，这反映在聚类算法中就是通过不同方法处理事物间距离，来确定事物属于哪个群体，如根据平均值、最大值等</strong>。</p> 
<p>其中<strong>K值是表示划分群体的大小</strong>，如以区域为例，K=34，则划分为全国省份；K=7，则划分为东北，中原，华东，华北，华南，西北和西部等；K=2，则划分为南方和北方。</p> 
<p>（3）K-邻近</p> 
<p>中国还有句古话是“近朱者赤近墨者黑”，该句也蕴含了K-邻近算法的思想。比如判断一个人是否是有钱人，可以根据其最近联系的人群中，有钱人的比例来推测。这就需要解决两个问题，一是如何确定最近联系人，二是如何计算有钱人比例。这反映在K-邻近算法中就是<strong>首先确定不同事物样本的距离，然后确定K值的大小，根据K值内的有钱人占比，来预测未知用户的状态。</strong></p> 
<p><strong>K值的大小将会直接决定预测结果，</strong>假如你有5个有钱人朋友，当K=8时，判定你为有钱人；但当K=12时，则判定你不是有钱人。因此<strong>在该算法中K的选择至关重要</strong>。</p> 
<p>（4）朴素贝叶斯</p> 
<p><strong>“吃一亏长一智”反映了朴素贝叶斯算法思维，就是通过后验经验法，来对未知的预测</strong>。假如你经常买水果，发现10个青苹果里边8个都是酸的，而10个红苹果里有7个都是甜的，这种经验告诉以后再挑选苹果时，红苹果7/10是甜的，青苹果2/10是甜的，如果你喜欢甜的，那就选红苹果吧。</p> 
<p>（5）决策树</p> 
<p>在婚恋相亲时经常被问到“你有车吗？你有房吗？你有钱吗？”，这和决策树的思维过程极其相似。<strong>决策树是由树枝，树叶，节点组成的树型结构，其中每个节点就是一个问题或特征（如你有车吗？），每个树枝是问题的走向（如有），每个节点就是答案（相亲成功）。</strong> <br> <img src="https://images2.imgbox.com/cd/ac/CNIA2ITz_o.jpg" alt="相亲决策树" title=""></p> 
<p>（6）主成分分析</p> 
<p>经常在网上看到两个字“干货”。那怎么定义“干货”，我觉得应该包括两方面：一是信息量大，二是没有废话。其实如何将“水货”制作成干货的过程，与主成分分析有异曲同工之妙。“干货”能够使原文到达“短小精悍”，而<strong>主成分分析能够实现数据集降维，即用较少维度表示原有样本含有的信息，两则都是通过其它语言或转变维度来表达原有信息。</strong></p> 
<p>“水货”变成“干货”就是将意思相近或相似的句子进行浓缩或提炼，也就是将“水货”里的的水分拧干；而<strong>主成分分析是根据样本集的协方差矩阵，通过线性变换将原数据映射到新的坐标系统，并将差异性较大特征值的保留，以到达降维目的。</strong></p> 
<p>（7）随机森林</p> 
<p><strong>“三个臭皮匠赛过诸葛亮”与随机森林算法内核类似。随机森林是是由一棵棵的决策树构成的，每决策树的形成都是随机的，它可以避免单一决策树过拟合和偏向的毛病。</strong></p> 
<p>再以相亲为例，对相亲对象要求，你可能看重“有房”“有车”“有钱”；你妈看重“有房”“孝顺”；你爸看重“事业”“顾家”“有车”等。其实你们每个人都是一个决策树，可根据自己判断标准决策出相亲对手是否“满意”，最后集合每个人的决策结果，来判断最后是否相亲成功。一个人相亲是决策树，全家人相亲就是随机森林。</p> 
<p><img src="https://images2.imgbox.com/71/73/Awzzzf7y_o.jpg" alt="相亲随机森林" title=""></p> 
<p>（8）最大熵模型</p> 
<p><strong>“不要把鸡蛋放在一个篮子里”是最大熵模型比较朴素的说法</strong>，也反映了该算法的本质，就是<strong>对不确定的或未知的，尽量保持随机和均匀分布，能够将风险降到最低。</strong>其实在生活中大家应该都不自觉的应用了该模型。比如，去年P2P较火的时候，很多人被其高收益吸引，但由于P2P鱼龙混杂，又担心跑路；因此采取比较保险的举措，就是多投几家公司。</p> 
<p>其实，<strong>熵是对无序状态的描述，而最大熵就是表示样本是均匀分布，可能性概率相同。</strong></p> 
<p>（9）AdaBoost</p> 
<p>在学生时代，考试有个技巧就是构建自己的“错题本”，每次考试前都加强对“错题本”学习，通过不断强化“错题本”上题目，最终可能获得较高分数。其实这个学习过程与AdaBoost是算法逻辑是相同的。</p> 
<p>假设每次考试作为一次模型训练，每道题目作为一个样本，分数作为预测准确率，而“错题本”就是预测错误的样本；当再次进行预测训练考试的时候，<strong>AdaBoost算法策略就是会对上次预测“错误的样本”加大权重，并以此不断迭代，通过多次训练，最后能够组合成一个较强的分类器（即考试高分）。</strong></p> 
<p>（10）关联规则</p> 
<p>是否耳熟“我看你天赋异禀、骨骼惊奇，想来是百年难得一见的练武奇才”“贫道夜观天象，发现北斗星南移，天狼星耀青光，帝王星显现”等台词。其实这里边就蕴含了关联规则，通过经验积累发现骨骼与练武，北斗星与帝王等之间关联。</p> 
<p>“用生辰八字来算命”虽然被成为伪科学，但偶尔能算准，这是这么回事？用关联规则算法就容易解释，首先理解两个概念支持度和置信度。</p> 
<p><strong>支持度是指A（某生辰八字）和B（某命运）同时发生的占比</strong>，如某生辰对应某命运的人数占总人数比值；<strong>置信度是指A发生后B发生的概率</strong>，如某生辰中当官的人数/某生辰总人数。如果置信度是100%，如果A发生，那么B一定发生。算命先生就将生辰和命运的置信度定为100%。</p> 
<p>如果算命先生学过机器学习算法，就不会很肯定指出你将来一定当官，而是说你将来当官的支持度为20%，置信度为30%。</p> 
<p>（11）逻辑回归</p> 
<p>逻辑回归与线性回归都是广义线性模型，只所以在回归前加上“逻辑”，是因为<strong>他在线性回归的基础上穿上了一件马甲（转变函数）</strong>。比如人的成功公式： “成功=3分才能+6分机会+1分贵人帮”，但通过计算可能得出如1,5,99,200等各种数字，如果就想知道是否是成功人士，你就需要一个“成功评委”来对结果进行评价，最终输出量化指标，如成功率是80%，其中越接近100%，说明越成功。<strong>而这个“成功评委”就是一个转变函数</strong>。</p> 
<p>（12）因子分析</p> 
<p>中国有句古话是“三岁看老”和“性格决定命运”，这与因子分析的思维类似，就是<strong>将影响或决定事物的本质东西总结出来</strong>。网络上将有喝绿茶饮料，穿361运动鞋，周末在家打游戏，留着平头等特征行为的人，称为具有“屌丝”气质。<strong>因子分析过程也类似，就是对具有关联行为或相似事件，进行共性因子提取，将具有共同本质的特征行为归为一个因子。</strong>像土豪，即使有钱了，依然没有改变“土”的因子。</p> 
<p>（13）人工神经网络</p> 
<p>其实<strong>全国人民大会代表选举过程与人工神经网络运算流程是类似</strong>。首先由基层人民选取乡/县级人民代表，再有县级选举市级，由市级选举省级，最后产生全国人民代表。其实，在选举过程中，<strong>每个人/代表相当于人工神经网络的一个神经元，而县、市、省等行政级别，相当于人工神经网络的层级，最后选取的代表相当于输出结果</strong>。层级越多代表越复杂，深度学习就是多层神经网络。现在终于明白国家领导人考察农村，叫做深入基层。</p> 
<p><img src="https://images2.imgbox.com/f4/12/JV0t6iqZ_o.jpg" alt="选举的人工神经网网络" title=""></p> 
<p>（14）SVM（支持向量机）</p> 
<p>以相亲为例，假如你根据学历，身高，年龄，相貌等指标，综合评估下来依然有较多候选人，你不知道这么办？这时候你闺蜜告诉你个方法：“就是看看他的父母和朋友情况”，这时候你豁然开朗，根据他的父母健康并是高官，朋友都年轻有为等指标，很快就确定了候选者。其实这个过程与SVM模型的思维逻辑相同的。<strong>SVM的核心思维就是将低维数据（一个人）映射到高维空间（多个人），从而实现数据可分（可选择）；你闺蜜的方法在SVM中就是核函数。这是一个拼“爹”的时代。</strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b8df6f7b9d50c9190519a74d7ac4df5d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android源码--Settings之Preference布局的详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d98c886e82e49af36cbd520ca44d5e32/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Vue2中的键盘事件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>