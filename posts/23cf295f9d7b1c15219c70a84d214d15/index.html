<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>理解AdamW - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="理解AdamW" />
<meta property="og:description" content="理解AdamW
我们先弄清楚什么是weight decay
其实是在损失函数求导后，放在正则项前面的系数，比如L2正则，我们看一下weight decay的位置
我 们 可 以 认 为 λ 就 是 w e i g h t d e c a y min ⁡ w L 2 ( w ) = min ⁡ w f ( w ) &#43; λ 2 n ∑ i = 1 n w i 2 L 2 ′ ( w ) = f ′ ( w ) &#43; λ n ∑ i = 1 n w i 我们可以认为\lambda就是weight\ decay\\ \min_wL_2(w)=\min_wf(w)&#43;\frac{\lambda}{2n}\sum_{i=1}^nw_i^2\\ L_2^{&#39;}(w)=f^{&#39;}(w)&#43;\frac{\lambda}{n}\sum_{i=1}^nw_i 我们可以认为λ就是weight decaywmin​L2​(w)=wmin​f(w)&#43;2nλ​i=1∑n​wi2​L2′​(w)=f′(w)&#43;nλ​i=1∑n​wi​" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/23cf295f9d7b1c15219c70a84d214d15/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-29T15:23:02+08:00" />
<meta property="article:modified_time" content="2021-11-29T15:23:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">理解AdamW</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <ol><li> <p>理解AdamW</p> 
  <ol><li> <p>我们先弄清楚什么是weight decay</p> </li><li> <p>其实是在损失函数求导后，放在正则项前面的系数，比如L2正则，我们看一下weight decay的位置</p> </li><li> <p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
          
           
            
            
              我 
             
            
              们 
             
            
              可 
             
            
              以 
             
            
              认 
             
            
              为 
             
            
              λ 
             
            
              就 
             
            
              是 
             
            
              w 
             
            
              e 
             
            
              i 
             
            
              g 
             
            
              h 
             
            
              t 
             
            
                
             
            
              d 
             
            
              e 
             
            
              c 
             
            
              a 
             
            
              y 
             
             
             
              
              
                min 
               
              
                ⁡ 
               
              
             
               w 
              
             
             
             
               L 
              
             
               2 
              
             
            
              ( 
             
            
              w 
             
            
              ) 
             
            
              = 
             
             
              
              
                min 
               
              
                ⁡ 
               
              
             
               w 
              
             
            
              f 
             
            
              ( 
             
            
              w 
             
            
              ) 
             
            
              + 
             
             
             
               λ 
              
              
              
                2 
               
              
                n 
               
              
             
             
             
               ∑ 
              
              
              
                i 
               
              
                = 
               
              
                1 
               
              
             
               n 
              
             
             
             
               w 
              
             
               i 
              
             
               2 
              
             
             
             
             
               L 
              
             
               2 
              
              
               
              
                ′ 
               
              
             
            
              ( 
             
            
              w 
             
            
              ) 
             
            
              = 
             
             
             
               f 
              
              
               
              
                ′ 
               
              
             
            
              ( 
             
            
              w 
             
            
              ) 
             
            
              + 
             
             
             
               λ 
              
             
               n 
              
             
             
             
               ∑ 
              
              
              
                i 
               
              
                = 
               
              
                1 
               
              
             
               n 
              
             
             
             
               w 
              
             
               i 
              
             
            
           
             我们可以认为\lambda就是weight\ decay\\ \min_wL_2(w)=\min_wf(w)+\frac{\lambda}{2n}\sum_{i=1}^nw_i^2\\ L_2^{'}(w)=f^{'}(w)+\frac{\lambda}{n}\sum_{i=1}^nw_i 
            
           
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord cjk_fallback">我</span><span class="mord cjk_fallback">们</span><span class="mord cjk_fallback">可</span><span class="mord cjk_fallback">以</span><span class="mord cjk_fallback">认</span><span class="mord cjk_fallback">为</span><span class="mord mathdefault">λ</span><span class="mord cjk_fallback">就</span><span class="mord cjk_fallback">是</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mord mathdefault">e</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="mord mathdefault">h</span><span class="mord mathdefault">t</span><span class="mspace"> </span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.166667em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height: 1.45em; vertical-align: -0.7em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.66786em;"><span class="" style="top: -2.1em; margin-left: 0em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02691em;">w</span></span></span><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class=""><span class="mop">min</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.7em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.45em; vertical-align: -0.7em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.66786em;"><span class="" style="top: -2.1em; margin-left: 0em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02691em;">w</span></span></span><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class=""><span class="mop">min</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.7em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 2.92907em; vertical-align: -1.27767em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.37144em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathdefault">n</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">λ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em;"><span class="" style="top: -1.87233em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05001em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.30001em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.864108em;"><span class="" style="top: -2.453em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height: 1.24248em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.99248em;"><span class="" style="top: -2.453em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class=""></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.827829em;"><span class="" style="top: -2.931em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.24248em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.99248em;"><span class="" style="top: -2.99248em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.57948em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class=""></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.827829em;"><span class="" style="top: -2.931em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 2.92907em; vertical-align: -1.27767em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.37144em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">n</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">λ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em;"><span class="" style="top: -1.87233em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05001em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.30001em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p> 
    <ol><li>AdamW是在Adam+L2正则化的基础上进行改进的算法。使用Adam优化带L2正则的损失并不有效。如果引入L2正则项，在计算梯度的时候会加上对正则项求梯度的结果。</li><li>那么如果本身比较大的一些权重对应的梯度也会比较大，由于Adam计算步骤中减去项会除以梯度平方的累积开根号，使得减去项偏小。按常理说，越大的权重应该惩罚越大，但是在Adam并不是这样。分子分母相互抵消掉了。</li><li>而权重衰减对所有的权重都采用相同的系数进行更新，越大的权重显然惩罚越大。</li><li>在常见的深度学习库中只提供了L2正则，并没有提供权重衰减的实现。</li><li><a href="https://openreview.net/pdf?id=rk6qdGgCZ" rel="nofollow">paper地址</a></li></ol> </li></ol> </li></ol> 
<p><img src="https://images2.imgbox.com/17/a7/3l9s3cJN_o.png" alt="在这里插入图片描述"></p> 
<p>Adam+L2 VS AdamW</p> 
<p>图片中红色是传统的Adam+L2 regularization的方式，绿色是Adam + weight decay的方式。可以看出两个方法的区别仅在于"系数乘以上一步参数值"（这一项实际上就是权重乘以L2项的导数，因为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          2 
         
        
       
      
        x^2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>的导数是本身x。）这一项的位置。</p> 
<p>再结合代码来看一下AdamW的具体实现。</p> 
<p>以下代码来自<a href="https://www.lizenghai.com/goto/?url=https://github.com/macanv/BERT-BiLSTM-CRF-NER/blob/master/bert_base/bert/optimization.py">https://github.com/macanv/BERT-BiLSTM-CRF-NER/blob/master/bert_base/bert/optimization.py</a>中的AdamWeightDecayOptimizer中的apply_gradients函数中，BERT中的优化器就是使用这个方法。</p> 
<p>在代码中也做了一些注释用于对应之前给出的Adam简化版公式，方便理解。可以看出update += self.weight_decay_rate * param这一句是Adam中没有的，也就是Adam中绿色的部分对应的代码，weightdecay这一步是是发生在Adam中需要被更新的参数update计算之后，并且在乘以学习率learning_rate之前，这和图片中的伪代码的计算顺序是完全一致的。总之一句话，如果使用了weightdecay就不必再使用L2正则化了。</p> 
<pre><code class="prism language-python">  <span class="token keyword">def</span> <span class="token function">apply_gradients</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grads_and_vars<span class="token punctuation">,</span> global_step<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""See base class."""</span>
    assignments <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>grad<span class="token punctuation">,</span> param<span class="token punctuation">)</span> <span class="token keyword">in</span> grads_and_vars<span class="token punctuation">:</span>
      <span class="token keyword">if</span> grad <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">or</span> param <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">continue</span>

      param_name <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_variable_name<span class="token punctuation">(</span>param<span class="token punctuation">.</span>name<span class="token punctuation">)</span>

      m <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span>
          name<span class="token operator">=</span>param_name <span class="token operator">+</span> <span class="token string">"/adam_m"</span><span class="token punctuation">,</span>
          shape<span class="token operator">=</span>param<span class="token punctuation">.</span>shape<span class="token punctuation">.</span>as_list<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
          trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
          initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span>
          name<span class="token operator">=</span>param_name <span class="token operator">+</span> <span class="token string">"/adam_v"</span><span class="token punctuation">,</span>
          shape<span class="token operator">=</span>param<span class="token punctuation">.</span>shape<span class="token punctuation">.</span>as_list<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
          trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
          initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

      <span class="token comment"># Standard Adam update.</span>
      next_m <span class="token operator">=</span> <span class="token punctuation">(</span>
          tf<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>self<span class="token punctuation">.</span>beta_1<span class="token punctuation">,</span> m<span class="token punctuation">)</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>beta_1<span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">)</span>
      next_v <span class="token operator">=</span> <span class="token punctuation">(</span>
          tf<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>self<span class="token punctuation">.</span>beta_2<span class="token punctuation">,</span> v<span class="token punctuation">)</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>beta_2<span class="token punctuation">,</span>
                                                    tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grad<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

      update <span class="token operator">=</span> next_m <span class="token operator">/</span> <span class="token punctuation">(</span>tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>next_v<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>epsilon<span class="token punctuation">)</span>

      <span class="token comment"># Just adding the square of the weights to the loss function is *not*</span>
      <span class="token comment"># the correct way of using L2 regularization/weight decay with Adam,</span>
      <span class="token comment"># since that will interact with the m and v parameters in strange ways.</span>
      <span class="token comment">#</span>
      <span class="token comment"># Instead we want ot decay the weights in a manner that doesn't interact</span>
      <span class="token comment"># with the m/v parameters. This is equivalent to adding the square</span>
      <span class="token comment"># of the weights to the loss with plain (non-momentum) SGD.</span>
      <span class="token keyword">if</span> self<span class="token punctuation">.</span>_do_use_weight_decay<span class="token punctuation">(</span>param_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
        update <span class="token operator">+=</span> self<span class="token punctuation">.</span>weight_decay_rate <span class="token operator">*</span> param

      update_with_lr <span class="token operator">=</span> self<span class="token punctuation">.</span>learning_rate <span class="token operator">*</span> update

      next_param <span class="token operator">=</span> param <span class="token operator">-</span> update_with_lr

      assignments<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>
          <span class="token punctuation">[</span>param<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>next_param<span class="token punctuation">)</span><span class="token punctuation">,</span>
           m<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>next_m<span class="token punctuation">)</span><span class="token punctuation">,</span>
           v<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>next_v<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token operator">*</span>assignments<span class="token punctuation">,</span> name<span class="token operator">=</span>name<span class="token punctuation">)</span>

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/598c827190e30f05c6ae6a2065ad8f7e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java中各数据类型的取值范围：</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/49aa5095af01aa91136901f35600d4f8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">常见HTML消息的错误代码的含义</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>