<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Ubuntu18.04&#43;Docker&#43;Hadoop&#43;Spark分布式集群搭建 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Ubuntu18.04&#43;Docker&#43;Hadoop&#43;Spark分布式集群搭建" />
<meta property="og:description" content="题前记：因为课程需求，下面将搭建过程用学术论文的格式写。（其实我并不想写的，没办法，学习作业嘛QAQ。。。）
我的联系方式：630056108
文章目录 Docker上的Hadoop大数据平台搭建与测试1. 简介与原理1.1 Docker介绍1.1.1 容器1.1.1.1 容器历史1.1.1.2 容器原理 1.1.2 Docker1.1.2.1 Docker历史1.1.2.2 Docker原理 1.2 Hadoop简介1.2.1 Hadoop历史1.2.2 Hadoop架构1.2.2.1 Hadoop的文件系统核心模块1.2.2.2 Hadoop的数据计算核心模块 1.2.3 Hadoop运行流程 2. 环境搭建2.1 本机环境2.2 物料说明2.3 安装基本环境2.4 测试环境 3. 总结附录1 Docker常用命令附页2 Hadoop常见命令参考文献（章） Docker上的Hadoop大数据平台搭建与测试 摘要：Docker是一个开源的应用容器，它可以让开发者将应用及其所需的依赖包打包到一个可移植的镜像中，可实现虚拟化。Hadoop是一开源的分布式系统基础架构，用于分布式计算，它可以给用户提供集群的高速运算和存储。本文简要介绍了Docker和Hadoop的发展史，Docker的架构，Hadoop的模块组件。并最终一步步搭建了Docker和Hadoop集群，并进行了测试。
大数据有四大特点（4V），分为别为：Volume（大量）、Variety（多样）、Velocity（高速）、Veracity（准确性）。新的特点需要使用新的技术，传统的HPC计算已经难以应付4V的挑战，相应而生的分布式计算架构很好地应对了问题。Hadoop便是其中之一的分布式架构平台，它有高可靠、高扩展、高效、高容错、低成本等优点，越来越受到关注与应用。
本文首先简述了容器和Docker与Hadoop的发展史，其次概述了Docker与Hadoop的架构模块，然后详细描述如何在真机上搭建环境并测试，最后进行总结和展望。
1. 简介与原理 1.1 Docker介绍 Docker 是一种运行于 Linux 和 Windows 上的软件，用于创建、管理和编排容器。是在 GitHub 上开发的 Moby 开源项目的一部分。
1.1.1 容器 容器（Coninter）是一种在单个系统上提供多个隔离的系统环境的技术。
1.1.1.1 容器历史 容器最早提出于1979年的Unix chroot。它是一个 UNIX 操作系统上的系统调用，用于将一个进程及其子进程的根目录改变到文件系统中的一个新位置，让这些进程只能访问到该目录。这个功能的想法是为每个进程提供独立的磁盘空间。
随后有2000 — FreeBSD Jails、2001 — Linux VServer、2004 — Solaris Containers、2005 — OpenVZ、2006 — Process Containers、2007 — Control Groups、2008 — LXC、2011 — Warden、2013 — LMCTFY、2013 — Docker、2014 — Rocket、2016 — Windows Containers。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/67b4ddec7596b6e72ed11223b98d0398/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-10-28T18:24:44+08:00" />
<meta property="article:modified_time" content="2019-10-28T18:24:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Ubuntu18.04&#43;Docker&#43;Hadoop&#43;Spark分布式集群搭建</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>题前记：因为课程需求，下面将搭建过程用学术论文的格式写。<s>（<strong>其实我并不想写的，没办法，学习作业嘛QAQ</strong>。。。）</s><br> 我的联系方式：630056108</p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#DockerHadoop_3" rel="nofollow">Docker上的Hadoop大数据平台搭建与测试</a></li><li><ul><li><a href="#1__7" rel="nofollow">1. 简介与原理</a></li><li><ul><li><a href="#11_Docker_8" rel="nofollow">1.1 Docker介绍</a></li><li><ul><li><a href="#111__10" rel="nofollow">1.1.1 容器</a></li><li><ul><li><a href="#1111__12" rel="nofollow">1.1.1.1 容器历史</a></li><li><a href="#1112__15" rel="nofollow">1.1.1.2 容器原理</a></li></ul> 
     </li><li><a href="#112_Docker_26" rel="nofollow">1.1.2 Docker</a></li><li><ul><li><a href="#1121_Docker_27" rel="nofollow">1.1.2.1 Docker历史</a></li><li><a href="#1122_Docker_30" rel="nofollow">1.1.2.2 Docker原理</a></li></ul> 
    </li></ul> 
    </li><li><a href="#12_Hadoop_43" rel="nofollow">1.2 Hadoop简介</a></li><li><ul><li><a href="#121_Hadoop_45" rel="nofollow">1.2.1 Hadoop历史</a></li><li><a href="#122_Hadoop_51" rel="nofollow">1.2.2 Hadoop架构</a></li><li><ul><li><a href="#1221_Hadoop_53" rel="nofollow">1.2.2.1 Hadoop的文件系统核心模块</a></li><li><a href="#1222_Hadoop_58" rel="nofollow">1.2.2.2 Hadoop的数据计算核心模块</a></li></ul> 
     </li><li><a href="#123_Hadoop_64" rel="nofollow">1.2.3 Hadoop运行流程</a></li></ul> 
   </li></ul> 
   </li><li><a href="#2__75" rel="nofollow">2. 环境搭建</a></li><li><ul><li><a href="#21__77" rel="nofollow">2.1 本机环境</a></li><li><a href="#22__86" rel="nofollow">2.2 物料说明</a></li><li><a href="#23__93" rel="nofollow">2.3 安装基本环境</a></li><li><a href="#24__425" rel="nofollow">2.4 测试环境</a></li></ul> 
   </li><li><a href="#3__453" rel="nofollow">3. 总结</a></li><li><a href="#1_Docker_455" rel="nofollow">附录1 Docker常用命令</a></li><li><a href="#2_Hadoop_501" rel="nofollow">附页2 Hadoop常见命令</a></li><li><a href="#_559" rel="nofollow">参考文献（章）</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="DockerHadoop_3"></a>Docker上的Hadoop大数据平台搭建与测试</h2> 
<p>摘要：Docker是一个开源的应用容器，它可以让开发者将应用及其所需的依赖包打包到一个可移植的镜像中，可实现虚拟化。Hadoop是一开源的分布式系统基础架构，用于分布式计算，它可以给用户提供集群的高速运算和存储。本文简要介绍了Docker和Hadoop的发展史，Docker的架构，Hadoop的模块组件。并最终一步步搭建了Docker和Hadoop集群，并进行了测试。<br> 　大数据有四大特点（4V），分为别为：Volume（大量）、Variety（多样）、Velocity（高速）、Veracity（准确性）。新的特点需要使用新的技术，传统的HPC计算已经难以应付4V的挑战，相应而生的分布式计算架构很好地应对了问题。Hadoop便是其中之一的分布式架构平台，它有高可靠、高扩展、高效、高容错、低成本等优点，越来越受到关注与应用。<br> 　本文首先简述了容器和Docker与Hadoop的发展史，其次概述了Docker与Hadoop的架构模块，然后详细描述如何在真机上搭建环境并测试，最后进行总结和展望。</p> 
<h3><a id="1__7"></a>1. 简介与原理</h3> 
<h4><a id="11_Docker_8"></a>1.1 Docker介绍</h4> 
<p>Docker 是一种运行于 Linux 和 Windows 上的软件，用于创建、管理和编排容器。是在 GitHub 上开发的 Moby 开源项目的一部分。</p> 
<h5><a id="111__10"></a>1.1.1 容器</h5> 
<p>容器（Coninter）是一种在单个系统上提供多个隔离的系统环境的技术。</p> 
<h6><a id="1111__12"></a>1.1.1.1 容器历史</h6> 
<p>容器最早提出于1979年的Unix chroot。它是一个 UNIX 操作系统上的系统调用，用于将一个进程及其子进程的根目录改变到文件系统中的一个新位置，让这些进程只能访问到该目录。这个功能的想法是为每个进程提供独立的磁盘空间。<br> 随后有2000 — FreeBSD Jails、2001 — Linux VServer、2004 — Solaris Containers、2005 — OpenVZ、2006 — Process Containers、2007 — Control Groups、2008 — LXC、2011 — Warden、2013 — LMCTFY、2013 — Docker、2014 — Rocket、2016 — Windows Containers。</p> 
<h6><a id="1112__15"></a>1.1.1.2 容器原理</h6> 
<p><img src="https://images2.imgbox.com/6c/69/GwwuVL86_o.png" alt="Virtual Machines Vs Containers（图片来自: Docker Inc.）">Linux上的容器是一个可以在单个Linux主机上提供多个相互隔离的操作系统级虚拟技术。与虚拟机技术（Virtual Machine）相比，容器（Container）不需要运行专门的访客（Guest）操作系统，也不需要模拟指令集。容器共享宿主机（Host）的操作系统内核，并使用访客操作系统的系统库来提供所需的功能。因此容器常常要比虚拟机快很多，且更加灵活和低消耗。</p> 
<blockquote> 
 <p>容器借助 Linux 内核的 Namespaces、Apparmor、SELinux 情景模式（profile）、chroot 和<br> CGroup 等功能来提供类似于虚拟机的隔离环境。Linux<br> 的安全模块可以确保正确地控制容器对宿主机和内核的访问，从而避免各种入侵活动。此外，在宿主机上可以运行不同的 Linux<br> 发行版，只要它们运行在同样的 CPU 架构下。</p> 
 <p>简单来说，容器提供的是一种基于各种 Linux 发行版创建容器镜像的方法、一套管理容器生命周期的 API、与该 API<br> 交互的客户端工具、保存快照的功能、在宿主机之间迁移容器实例的能力，等等。</p> 
</blockquote> 
<h5><a id="112_Docker_26"></a>1.1.2 Docker</h5> 
<h6><a id="1121_Docker_27"></a>1.1.2.1 Docker历史</h6> 
<p>2013年，dotCloud为寻求新的突破，更名为Docker，并发布了开源的容器Docker。Docker 引入了一整套容器管理的生态系统，包括分层的镜像模型，容器注册库，友好的 Rest API等。<br> 2016 年微软也在 Windows 上提供了容器的支持，Docker 可以以原生方式运行在 Windows 上，而不是需要使用 Linux 虚拟机。</p> 
<h6><a id="1122_Docker_30"></a>1.1.2.2 Docker原理</h6> 
<p>核心技术有：</p> 
<ol><li>namespace：隔离其运行环境，使得容器中的进程看起来就像一个独立环境中运行一样。</li><li>cgroup：为系统中所运行任务（进程）的用户定义组群分配资源。可以监控管理员配置的 cgroup，拒绝 cgroup 访问某些资源，甚至在运行的系统中动态配置 cgroup。主要功能包括了：限制资源使用、优先级控制、审计计费、 挂起和恢复进程</li><li>守护进程：直接与主操作系统进行通信、为各个Docker容器分配资源、将容器与主操作系统隔离，并将各个容器互相隔离。</li></ol> 
<p><img src="https://images2.imgbox.com/d4/48/H99yKCXR_o.png" alt="Docker结构"><br> Docker的基础结构有（从底层到上层）</p> 
<ol><li>基础设施：即各种硬件设施和配套的底层软件</li><li>主操作系统：有Linux、Windows、MacOS等。用于支持Docker运行</li><li>各种依赖：打包在Docker镜像（Image）之中，容器（Container）依赖于镜像创建</li><li>应用：应用的源代码与依赖项打包在Docker镜像中，不同的应用需要不同的Docker镜像。不同镜像之间相互隔离。</li></ol> 
<h4><a id="12_Hadoop_43"></a>1.2 Hadoop简介</h4> 
<p>Hadoop更注重代码向数据迁移，有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上，且它提供高吞吐量（high throughput）。</p> 
<h5><a id="121_Hadoop_45"></a>1.2.1 Hadoop历史</h5> 
<p>2003-2004年，Google公布了部分GFS和MapReduce思想的细节，受此启发的Doug Cutting等人用2年的业余时间实现了DFS和MapReduce机制。<br> 2005年，Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。<br> 2006年2月被分离出来，成为一套完整独立的软件，起名为Hadoop。<br> 2008年4月— 赢得世界最快1TB数据排序在900个节点上用时209秒。<br> 2011年12月27日–1.0.0版本释出。标志着Hadoop已经初具生产规模。</p> 
<h5><a id="122_Hadoop_51"></a>1.2.2 Hadoop架构</h5> 
<p>Hadoop分为两个模块，一个是文件系统核心模块(HDFS)，另一个是数据计算核心模块(MapReduce)。</p> 
<h6><a id="1221_Hadoop_53"></a>1.2.2.1 Hadoop的文件系统核心模块</h6> 
<p>HDFS采用主从架构（master/slaves）。一个HDFS集群由一个主节点（Namenode）和一定数量的从节点（Datanodes）组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。Datanode是一个节点一个，负责节点上的存储。<br> 从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的名字空间操作。Datanode负责处理文件系统客户端的读写请求，在Namenode的统一调度下进行数据块的创建、删除和复制。<br> secondaryNameNode用于hadoop当中元数据信息的辅助管理。<br> <img src="https://images2.imgbox.com/5e/c9/senTT2tW_o.gif" alt="HDFS架构（来源Hadoop手册）"></p> 
<h6><a id="1222_Hadoop_58"></a>1.2.2.2 Hadoop的数据计算核心模块</h6> 
<p>Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。<br> 一个作业将数据集分割为独立的数据块，由Map任务以并行的方式处理数据。<br> MapReduce框架由一个单独的主作业控制（master JobTracker ）和每个集群节点一个从任务控制（slave TaskTracke）共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。<br> ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分<br> NodeManager：负责执行主节点ResourceManager分配的任务</p> 
<h5><a id="123_Hadoop_64"></a>1.2.3 Hadoop运行流程</h5> 
<p><img src="https://images2.imgbox.com/f8/33/z8b59JBY_o.png" alt="Hadoop的MapReduce流程">MapReduce分3个阶段执行，分别是映射(Map)阶段、洗牌(Shuffle)阶段、归约(Reduce)阶段。<br> MapReduce使用操纵键值对来处理数据<br> Map: <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
        
        
          K 
         
        
          1 
         
        
       
         , 
        
        
        
          V 
         
        
          1 
         
        
       
         ) 
        
       
         → 
        
       
         l 
        
       
         i 
        
       
         s 
        
       
         t 
        
       
         ( 
        
        
        
          K 
         
        
          2 
         
        
       
         , 
        
        
        
          V 
         
        
          2 
         
        
       
         ) 
        
       
         ) 
        
       
      
        (K_{1}, V_{1})→list(K_{2}, V_{2})) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathdefault">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.07153em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span style="margin-right: 0.22222em;" class="mord mathdefault">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span style="margin-right: 0.01968em;" class="mord mathdefault">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathdefault">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.07153em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span style="margin-right: 0.22222em;" class="mord mathdefault">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span><br> Reduce:<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
        
        
          K 
         
        
          1 
         
        
       
         , 
        
       
         l 
        
       
         i 
        
       
         s 
        
       
         t 
        
       
         ( 
        
        
        
          V 
         
        
          2 
         
        
       
         ) 
        
       
         ) 
        
       
         → 
        
       
         l 
        
       
         i 
        
       
         s 
        
       
         t 
        
       
         ( 
        
        
        
          K 
         
        
          3 
         
        
       
         , 
        
        
        
          V 
         
        
          3 
         
        
       
         ) 
        
       
         ) 
        
       
      
        (K_{1}, list(V_{2}))→list(K_{3}, V_{3})) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathdefault">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.07153em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span style="margin-right: 0.01968em;" class="mord mathdefault">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord"><span style="margin-right: 0.22222em;" class="mord mathdefault">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span style="margin-right: 0.01968em;" class="mord mathdefault">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathdefault">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.07153em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span style="margin-right: 0.22222em;" class="mord mathdefault">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span><br> Shuffle表示数据进行整理（核心机制：对数据进行分区，排序，缓存）即拉取partition,merge,sort,combine<br> 一个MapReduce的工作过程是：</p> 
<ol><li>向集群服务器发送Map与Reduce任务。</li><li>管理数据传递。</li><li>本地计算，减少网络通数据通信。</li><li>完成任务后，收集数据，将合适的结果返回给Master服务器</li></ol> 
<h3><a id="2__75"></a>2. 环境搭建</h3> 
<p>Hadoop有三种模式（单机模式，伪分布模式，全分布模式）。本文选用全分布模式搭建集群。</p> 
<h4><a id="21__77"></a>2.1 本机环境</h4> 
<p>本文基于的环境是Ubuntu18.04 （amd64）<br> 配置如下：</p> 
<table><thead><tr><th>配件</th><th>名称</th></tr></thead><tbody><tr><td>CPU</td><td>Intel Core-I7 8750H @2.2GHz</td></tr><tr><td>内存</td><td>16 GB</td></tr><tr><td>硬盘</td><td>160G SSD</td></tr><tr><td>显卡</td><td>Nvidia Geforce GTX1060(6G) (有无无影响)</td></tr></tbody></table> 
<h4><a id="22__86"></a>2.2 物料说明</h4> 
<p>宿主机：Ubuntu18.04<br> Docker<br> JDK<br> Hadoop<br> Spark<br> 操作路径为/home/</p> 
<h4><a id="23__93"></a>2.3 安装基本环境</h4> 
<ol><li>安装Docker容器</li></ol> 
<pre><code class="prism language-bash"><span class="token comment"># 删除可能有的旧版本</span>
<span class="token function">sudo</span> <span class="token function">apt-get</span> remove docker docker-engine docker.io containerd runc
<span class="token comment"># 更新apt</span>
<span class="token function">sudo</span> <span class="token function">apt-get</span> update
<span class="token comment"># 让apt支持https</span>
<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> apt-transport-https  ca-certificates  <span class="token function">curl</span>  gnupg-agent  software-properties-common
<span class="token comment"># 安装官方GPG密钥</span>
<span class="token function">curl</span> -fsSL https://download.docker.com/linux/ubuntu/gpg <span class="token operator">|</span> <span class="token function">sudo</span> apt-key add -
<span class="token function">sudo</span> add-apt-repository <span class="token string">"deb [arch=amd64] https://download.docker.com/linux/ubuntu <span class="token variable"><span class="token variable">$(</span>lsb_release -cs<span class="token variable">)</span></span> stable"</span>
// 阿里云
// <span class="token function">curl</span> -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg <span class="token operator">|</span> <span class="token function">sudo</span> apt-key add -
// <span class="token function">sudo</span> add-apt-repository <span class="token string">"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu <span class="token variable"><span class="token variable">$(</span>lsb_release -cs<span class="token variable">)</span></span> stable"</span>
<span class="token comment"># 安装docker-ce</span>
<span class="token function">sudo</span> <span class="token function">apt-get</span> update
<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> docker-ce
<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> docker-ce-cli
<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> containerd.io
</code></pre> 
<ol start="2"><li>在Docker中拉取Ubuntu18.04镜像并创建容器</li></ol> 
<pre><code class="prism language-bash"><span class="token comment"># 拉取ubuntu 18.04镜像</span>
<span class="token function">sudo</span> docker pull ubuntu:18.04
<span class="token comment"># 查找拉取的镜像id</span>
<span class="token function">sudo</span> docker images
<span class="token comment"># 启动一个master容器</span>
<span class="token function">sudo</span> docker run -dit -h master <span class="token operator">&lt;</span>image-id<span class="token operator">&gt;</span>
<span class="token comment"># 查找创建的容器id</span>
<span class="token function">sudo</span> docker <span class="token function">ps</span>
<span class="token comment"># 进入容器</span>
<span class="token function">sudo</span> docker <span class="token function">exec</span> -it <span class="token operator">&lt;</span>container-id<span class="token operator">&gt;</span> /bin/bash
</code></pre> 
<p>注：&lt;container-id&gt;和&lt;container-id&gt;需要换成对应的id，及生成的hash，如不清楚，可以查看附录1中查看Docker中image、container的命令。<br> 绑定端口</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> docker run -dit -h master -p 9870:9870 -p 8080:8080 -p 4040:4040 -p 8088:8088 -p 8099:8099 <span class="token operator">&lt;</span>image-id<span class="token operator">&gt;</span>
</code></pre> 
<ol start="3"><li>配置master容器<br> 首先要确认自己已经进入到master容器中<br> <img src="https://images2.imgbox.com/53/7e/x2SdUAxT_o.png" alt="已经入master节点">正确的应该是上面第二行呈现的样子。如果不正确，请检测前面运行是否正确，找不到问题，就"sudo docker ps -a"查看已创建的Container，然后"sudo docker start &lt;container id&gt;"，最后"sudo docker exec -it &lt;container-id&gt; /bin/bash"<br> 确认完后，开始配置master容器</li></ol> 
<pre><code class="prism language-bash"><span class="token comment"># 更新apt</span>
<span class="token function">apt-get</span> update
<span class="token function">apt-get</span> pagrade
<span class="token comment"># 安装net-tools</span>
<span class="token function">apt-get</span> <span class="token function">install</span> net-tools
<span class="token comment"># 安装vim</span>
<span class="token function">apt-get</span> <span class="token function">install</span> vim
<span class="token comment"># 安装ssh，配置无密码登录</span>
<span class="token function">apt-get</span> <span class="token function">install</span> openssh-server
ssh-keygen -t rsa -P <span class="token string">''</span>
回车
<span class="token function">cat</span> ~/.ssh/id_rsa.pub <span class="token operator">&gt;&gt;</span> ~/.ssh/authorized_keys
<span class="token comment"># 查询是否开启ssh</span>
/etc/init.d/ssh status
<span class="token comment"># 开启ssh</span>
/etc/init.d/ssh start
<span class="token comment"># 配置开机启动</span>
vim /etc/rc.local
<span class="token comment"># 在exit 0语句前加入</span>
/etc/init.d/ssh start
</code></pre> 
<ol start="4"><li>安装java<br> 这里使用wget下载java，如果本文给出的url无法正确获取到java文件，请自行查找最新的url予以替换，或直接使用“apt install default-jre”安装开源版本。</li></ol> 
<pre><code class="prism language-bash"><span class="token comment">#安装wget库</span>
apt <span class="token function">install</span> <span class="token function">wget</span>
<span class="token comment">#进入usr/local/src目录下</span>
<span class="token function">cd</span> /usr/local/src
<span class="token comment">#获取JDK1.8包</span>
<span class="token function">wget</span> --no-check-certificate --no-cookies --header <span class="token string">"Cookie: oraclelicense=accept-securebackup-cookie"</span> http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz
<span class="token comment">#解压jdk-8u161-linux-x64.tar.gz</span>
<span class="token function">tar</span> -zxvf jdk-8u131-linux-x64.tar.gz
<span class="token comment">#重命名为JDK8</span>
<span class="token function">mv</span> jdk1.8.0_131 jdk8
<span class="token comment">#配置环境变量</span>
vim  ~/.bashrc <span class="token comment">#打开环境变量配置文件</span>
<span class="token comment">#增加下面内容到该文件最后</span>
<span class="token function">export</span> JAVA_HOME<span class="token operator">=</span>/usr/local/src/jdk8
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$JAVA_HOME</span>/bin:<span class="token variable">$PATH</span>  
<span class="token function">export</span> CLASSPATH<span class="token operator">=</span>.:<span class="token variable">$JAVA_HOME</span>/lib/dt.jar:<span class="token variable">$JAVA_HOME</span>/lib/tools.jar 
<span class="token comment">#使环境生效</span>
<span class="token function">source</span> ~/.bashrc
</code></pre> 
<ol start="5"><li>安装Hadoop</li></ol> 
<pre><code class="prism language-bash"><span class="token comment"># 安装hadoop</span>
<span class="token function">wget</span> http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz
<span class="token comment"># 解压</span>
<span class="token function">tar</span> -zxvf hadoop-3.2.0.tar.gz
<span class="token comment"># 重命名</span>
<span class="token function">mv</span>  hadoop-3.2.0 hadoop
<span class="token comment"># 添加环境变量</span>
<span class="token function">sudo</span> vim ~/.bashrc
<span class="token comment"># 添加以下代码</span>
<span class="token comment"># hadoop</span>
<span class="token function">export</span> HADOOP_HOME<span class="token operator">=</span>/home/hadoop
<span class="token function">export</span> CLASSPATH<span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>$HADOOP_HOME/bin/hadoop classpath<span class="token variable">)</span></span><span class="token keyword">:</span><span class="token variable">$CLASSPATH</span>
<span class="token function">export</span> HADOOP_COMMON_LIB_NATIVE_DIR<span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>/lib/native
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$HADOOP_HOME</span>/bin:<span class="token variable">$HADOOP_HOME</span>/sbin
<span class="token comment"># 生效</span>
<span class="token function">sudo</span> <span class="token function">source</span> ~/.bashrc
</code></pre> 
<ol start="6"><li>配置分布式hadoop</li></ol> 
<pre><code class="prism language-bash"><span class="token comment"># 当前路径为/home/hadoop/</span>
<span class="token comment"># 创建三个文件夹备用</span>
<span class="token function">mkdir</span> -p hdfs/<span class="token punctuation">{<!-- --></span>data,name,tmp<span class="token punctuation">}</span>
</code></pre> 
<p>修改配置</p> 
<pre><code class="prism language-bash"><span class="token function">cd</span> /home/hadoop/etc/hadoop
</code></pre> 
<p>以下修改XXX，均使用“viｍ XXX”方式进行编写，不再赘述<br> 修改hadoop-env.sh</p> 
<pre><code class="prism language-bash"><span class="token comment"># 在文中只改这一行</span>
<span class="token function">export</span> JAVA_HOME<span class="token operator">=</span>/home/bigdata/jdk1.8
</code></pre> 
<p>修改core-site.xml</p> 
<pre><code class="prism language-bash"><span class="token operator">&lt;</span>configuration<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>fs.defaultFS<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>hdfs://master:9000<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>hadoop.tmp.dir<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>/home/bigdata/hdfs/tmp<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/configuration<span class="token operator">&gt;</span>
</code></pre> 
<p>修改hdfs-site.xml</p> 
<pre><code class="prism language-bash"><span class="token operator">&lt;</span>configuration<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.namenode.name.dir<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>/home/bigdata/hadoop/hdfs/name<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.http.address<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>0.0.0.0:50070<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.datanode.data.dir<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>/home/bigdata/hdfs/data<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>dfs.replication<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
                <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>2<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/configuration<span class="token operator">&gt;</span>
</code></pre> 
<p>修改mapred-site.xml，注意这里的目录一定要和上面定义的目录相统一，否则DataNode会崩溃，无法集群</p> 
<pre><code class="prism language-bash"><span class="token operator">&lt;</span>configuration<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>mapreduce.framework.name<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>yarn<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
         <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.app.mapreduce.am.env<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>HADOOP_MAPRED_HOME<span class="token operator">=</span>/home/hadoop<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>mapreduce.map.env<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>HADOOP_MAPRED_HOME<span class="token operator">=</span>/home/hadoop<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>mapreduce.reduce.env<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>HADOOP_MAPRED_HOME<span class="token operator">=</span>/home/hadoop<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/configuration<span class="token operator">&gt;</span>
</code></pre> 
<p>修改yarn-site.xml</p> 
<pre><code class="prism language-bash"><span class="token operator">&lt;</span>configuration<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.nodemanager.aux-services<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>mapreduce_shuffle<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>property<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>yarn.resourcemanager.hostname<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>value<span class="token operator">&gt;</span>master<span class="token operator">&lt;</span>/value<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>/property<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/configuration<span class="token operator">&gt;</span>
</code></pre> 
<p>创建workers文件（Hadoop3将slaves文件改名为workers了）</p> 
<pre><code class="prism language-bash"><span class="token function">touch</span> workers
vim workers
<span class="token comment"># 添加：</span>
master
slave01
slave02
</code></pre> 
<ol start="7"><li>格式化HDFS，这里只需要运行一次，如果运行多了需要删除工作目录下HDFS文件，然后重新格式化，详情自行搜索</li></ol> 
<pre><code class="prism language-bash"><span class="token function">cd</span> /home/hadoop/bin
<span class="token comment"># 该命令只需运行一次</span>
hdfs namenode -format
</code></pre> 
<p>修改运行文件</p> 
<pre><code class="prism language-bash"><span class="token function">cd</span> /home/hadoop/sbin
<span class="token comment"># 在start-dfs.sh，stop-dfs.sh文件顶部加入以下配置</span>
<span class="token comment">#!/usr/bin/env bash</span>
HDFS_DATANODE_USER<span class="token operator">=</span>root
HADOOP_SECURE_SECURE_USER<span class="token operator">=</span>hdfs
HDFS_NAMENODE_USER<span class="token operator">=</span>root
HDFS_SECONDARYNAMENODE_USER<span class="token operator">=</span>root

<span class="token comment"># 在start-yarn.sh，stop-yarn.sh文件顶部加入以下配置</span>
<span class="token comment">#!/usr/bin/env bash</span>
YARN_RESOURCEMANAGER_USER<span class="token operator">=</span>root
HADOOP_SECURE_DN_USER<span class="token operator">=</span>yarn
YARN_NODEMANAGER_USER<span class="token operator">=</span>root
</code></pre> 
<ol start="8"><li>安装Scala</li></ol> 
<pre><code class="prism language-bash"><span class="token function">wget</span> https://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz
<span class="token function">tar</span> -zxvf scala-2.11.8.tgz
<span class="token function">mv</span> scala-2.11.8 scala
<span class="token comment"># 添加环境变量</span>
vim ~/.bashrc
<span class="token comment"># 添加以下代码</span>
<span class="token comment"># scala</span>
<span class="token function">export</span> SCALA_HOME<span class="token operator">=</span>/home/scala
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$SCALA_HOME</span>/bin
<span class="token comment"># 生效</span>
<span class="token function">source</span> ~/.bashrc
</code></pre> 
<ol start="9"><li>安装Spark</li></ol> 
<pre><code class="prism language-bash"><span class="token function">wget</span> http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
<span class="token function">tar</span> -zxvf spark-2.4.3-bin-hadoop2.7.tgz
<span class="token function">mv</span> spark-2.4.3-bin-hadoop2.7 spark

<span class="token comment"># 添加环境变量</span>
vim ~/.bashrc
<span class="token comment"># 添加以下代码</span>
<span class="token comment"># spark</span>
<span class="token function">export</span> SPARK_HOME<span class="token operator">=</span>/home/spark
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$SPARK_HOME</span>/bin
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$PATH</span><span class="token keyword">:</span><span class="token variable">$SPARK_HOME</span>/sbin
<span class="token comment"># 生效</span>
<span class="token function">source</span> ~/.bashrc
</code></pre> 
<ol start="10"><li>配置文件</li></ol> 
<pre><code class="prism language-bash"><span class="token function">cd</span> /home/spark/conf
<span class="token function">cp</span> spark-env.sh.template spark-env.sh
<span class="token comment"># spark-env.sh添加以下代码</span>
<span class="token function">export</span> JAVA_HOME<span class="token operator">=</span>/home/jdk1.8
<span class="token function">export</span> HADOOP_HOME<span class="token operator">=</span>/home/hadoop
<span class="token function">export</span> HADOOP_CONF_DIR<span class="token operator">=</span>/home/hadoop/etc/hadoop
<span class="token function">export</span> SCALA_HOME<span class="token operator">=</span>/home/scala
<span class="token function">export</span> SPARK_HOME<span class="token operator">=</span>/home/spark
<span class="token function">export</span> SPARK_MASTER_HOST<span class="token operator">=</span>master
<span class="token function">export</span> SPARK_MASTER_PORT<span class="token operator">=</span>7077
<span class="token function">export</span> SPARK_MASTER_WEBUI_PORT<span class="token operator">=</span>8099
<span class="token function">export</span> SPARK_WORKER_CORES<span class="token operator">=</span>3
<span class="token function">export</span> SPARK_WORKER_INSTANCES<span class="token operator">=</span>1
<span class="token function">export</span> SPARK_WORKER_MEMORY<span class="token operator">=</span>5G
<span class="token function">export</span> SPARK_WORKER_WEBUI_PORT<span class="token operator">=</span>8081
<span class="token function">export</span> SPARK_EXECUTOR_CORES<span class="token operator">=</span>1
<span class="token function">export</span> SPARK_EXECUTOR_MEMORY<span class="token operator">=</span>1G
<span class="token function">export</span> LD_LIBRARY_PATH<span class="token operator">=</span><span class="token variable">${LD_LIBRARY_PATH}</span><span class="token keyword">:</span><span class="token variable">$HADOOP_HOME</span>/lib/native

<span class="token function">cp</span> slaves.template slaves
<span class="token comment">#  slaves添加以下代码</span>
master
slave01
slave02
</code></pre> 
<ol start="11"><li>提交master并克隆出slaves</li></ol> 
<pre><code class="prism language-bash"><span class="token comment"># 退出容器</span>
<span class="token keyword">exit</span>
<span class="token comment"># 提交容器更改</span>
<span class="token function">sudo</span> docker commit <span class="token operator">&lt;</span>container-id<span class="token operator">&gt;</span> ubuntu-hadoop-spark:1.0
<span class="token comment"># 用刚刚提交的ubuntu-hadoop-spark:1.0创建两个slave</span>
<span class="token function">sudo</span> docker run -dit -h slave01 <span class="token operator">&lt;</span>image-id<span class="token operator">&gt;</span>
<span class="token function">sudo</span> docker run -dit -h slave02 <span class="token operator">&lt;</span>image-id<span class="token operator">&gt;</span>
<span class="token comment"># 运行两个slave</span>
<span class="token function">sudo</span> docker <span class="token function">exec</span> -it slave01 /bin/bash
<span class="token function">sudo</span> docker <span class="token function">exec</span> -it slave02 /bin/bash
</code></pre> 
<p><strong>到这里之后就配置好一个master两个worker了</strong><br> 13. hosts配置<br> 分别进入master、slave01和slave02，用ifconfig命令找出相应的ip，并分别修改hosts文件</p> 
<pre><code class="prism language-bash">vim /etc/hosts
<span class="token comment"># 添加</span>
xxx.xxx.xxx.xx1   master 
xxx.xxx.xxx.xx2   slave01
xxx.xxx.xxx.xx3   slave02
</code></pre> 
<ol start="14"><li>启动</li></ol> 
<p>分别进入三个容器，启动命令相同</p> 
<pre><code class="prism language-bash"><span class="token comment"># 启动hadoop</span>
<span class="token function">cd</span> /home/hadoop/sbin
./start-dfs.sh
./start-yarn.sh
<span class="token comment"># 启动spark</span>
<span class="token function">cd</span> /home/spark/sbin
./start-all/sh
</code></pre> 
<ol start="15"><li>使用 JPS检测启动<br> 如果集群不成功，检测路径等问题，NameNode一会就消失是format的问题，需要重新格式化<br> 两个管理网页<br> http://(master ip):8088/<br> http://(master ip):50070/</li></ol> 
<h4><a id="24__425"></a>2.4 测试环境</h4> 
<ol><li>首先查看是否运行成功<br> 这里是start-all.sh的样子，也可以分别启动hdfs和yern<img src="https://images2.imgbox.com/09/ee/XPQWjzYe_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/2c/c6/ShAFFJh4_o.png" alt="在这里插入图片描述"><br> 这样是启动成功了的</li><li>创建目录</li></ol> 
<pre><code class="prism language-bash">hadoop fs -mkdir /input
</code></pre> 
<p>然后就能看见创建的目录了<br> <img src="https://images2.imgbox.com/b8/95/uLb3L3kh_o.png" alt="在这里插入图片描述">3. 放入文件<br> 我这里就用hadoop中的LICENSE文件做count了</p> 
<pre><code class="prism language-bash">hadoop fs -put /home/hadoop/LICENSE.txt /input
</code></pre> 
<ol start="3"><li>放入wordcount程序</li></ol> 
<pre><code class="prism language-bash">hadoop jar /home/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0.jar wordcount /input /output
</code></pre> 
<p>注意与自己的路径相符和文件名，我这里是3.2.0.你那里可能版本不一样<br> 4. 运行wordcount</p> 
<pre><code class="prism language-bash">wordcount /input /output
</code></pre> 
<p>结果：<br> <img src="https://images2.imgbox.com/50/16/m8il4T6h_o.png" alt="在这里插入图片描述">6. 将结果下载下来<br> 这里的ip请填master对应ip，在下载时会自动跳到slave上，请修改成正确的ip<br> <img src="https://images2.imgbox.com/41/30/roGgNptw_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/21/89/Q5Xm1DeU_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/10/44/6i1Uo8Fh_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3__453"></a>3. 总结</h3> 
<p>Docker是一个非常好用的开源容器，它节约了开发、维护、测试时对环境的配置问题，并且很好的解决了虚拟机效率不高的问题。本文主要对它的历史和架构做了简单描述，通过在Docker上创建一个Image，并在上搭建了全分布式Hadoop。Hadoop是运用广泛的开源分布式计算平台，本文主要介绍了它的历史和架构，并通过搭建它的环境和用一个例子来简要概述了它的流程。为我们进一步研究分布式并行算法和处理分析大数据提供了平台。</p> 
<h3><a id="1_Docker_455"></a>附录1 Docker常用命令</h3> 
<pre><code class="prism language-bash"><span class="token comment"># 重启docker服务 </span>
<span class="token function">sudo</span> <span class="token function">service</span> docker restart
<span class="token comment"># 关闭docker服务 </span>
<span class="token function">sudo</span> <span class="token function">service</span> docker stop
<span class="token comment"># 开启docker服务</span>
<span class="token function">sudo</span> <span class="token function">service</span> docker start

<span class="token comment"># 拉取ubuntu 18.04镜像</span>
<span class="token function">sudo</span> docker pull ubuntu:18.04
<span class="token comment"># docker create 命令为指定的镜像（image）添加了一个可读写层，构成了一个新的容器。注意，这个容器并没有运行。 </span>
<span class="token function">sudo</span> docker create <span class="token operator">&lt;</span>image-id<span class="token operator">&gt;</span>
<span class="token comment"># docker run = docker create + docker start</span>
<span class="token function">sudo</span> docker run  -p 宿主机端口:docker端口 <span class="token operator">&lt;</span>image-id<span class="token operator">&gt;</span> 
<span class="token comment"># 后台运行</span>
<span class="token function">sudo</span> docker run -dit <span class="token operator">&lt;</span>image-id<span class="token operator">&gt;</span>
<span class="token comment"># 运行直接进入bash，退出后后台不运行</span>
<span class="token function">sudo</span> docker run -it <span class="token operator">&lt;</span>image-id<span class="token operator">&gt;</span> /bin/bash 
<span class="token comment"># 查看本地镜像</span>
<span class="token function">sudo</span> docker images
<span class="token comment"># 删除镜像</span>
<span class="token function">sudo</span> docker rmi <span class="token operator">&lt;</span>image-id<span class="token operator">&gt;</span>

<span class="token comment"># docker start命令为容器文件系统创建了一个进程隔离空间。注意，每一个容器只能够有一个进程隔离空间。 </span>
docker start <span class="token operator">&lt;</span>container-id<span class="token operator">&gt;</span>
<span class="token comment"># 查看当前运行的容器</span>
docker <span class="token function">ps</span>
<span class="token comment"># 查询存在的容器</span>
docker <span class="token function">ps</span> -a
<span class="token comment"># 删除容器</span>
docker <span class="token function">rm</span>  <span class="token operator">&lt;</span>container-id<span class="token operator">&gt;</span>
<span class="token comment"># 强制删除容器</span>
docker <span class="token function">rm</span> -f  <span class="token operator">&lt;</span>container-id<span class="token operator">&gt;</span>
<span class="token comment"># 不能够删除一个正在运行的容器，会报错。需要先停止容器。</span>
<span class="token comment"># 进入容器内部</span>
<span class="token function">sudo</span> docker <span class="token function">exec</span> -it <span class="token punctuation">(</span>container name or id<span class="token punctuation">)</span> /bin/bash
<span class="token comment"># 退出容器</span>
<span class="token keyword">exit</span>
<span class="token comment"># 容器重命名</span>
docker container <span class="token function">rename</span> <span class="token operator">&lt;</span>container-id<span class="token operator">&gt;</span> newname

<span class="token comment"># 提交</span>
docker commit <span class="token operator">&lt;</span>container-id<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>image-name<span class="token operator">&gt;</span>
</code></pre> 
<p>注：文中所给&lt;container-id&gt;、&lt;image-id&gt;都要用对应的id进行替换，不要直接照抄。</p> 
<h3><a id="2_Hadoop_501"></a>附页2 Hadoop常见命令</h3> 
<pre><code class="prism language-bash">1、启动hadoop所有进程
start-all.sh等价于start-dfs.sh + start-yarn.sh
但是一般不推荐使用start-all.sh<span class="token punctuation">(</span>因为开源框架中内部命令启动有很多问题<span class="token punctuation">)</span>。
2、单进程启动。
sbin/start-dfs.sh
---------------
    sbin/hadoop-daemons.sh --config <span class="token punctuation">..</span> --hostname <span class="token punctuation">..</span> start namenode <span class="token punctuation">..</span>.
    sbin/hadoop-daemons.sh --config <span class="token punctuation">..</span> --hostname <span class="token punctuation">..</span> start datanode <span class="token punctuation">..</span>.
    sbin/hadoop-daemons.sh --config <span class="token punctuation">..</span> --hostname <span class="token punctuation">..</span> start sescondarynamenode <span class="token punctuation">..</span>.
    sbin/hadoop-daemons.sh --config <span class="token punctuation">..</span> --hostname <span class="token punctuation">..</span> start zkfc <span class="token punctuation">..</span>.         //
sbin/start-yarn.sh
--------------  
    libexec/yarn-config.sh
    sbin/yarn-daemon.sh --config <span class="token variable">$YARN_CONF_DIR</span>  start resourcemanager
    sbin/yarn-daemons.sh  --config <span class="token variable">$YARN_CONF_DIR</span>  start nodemanager
3、常用命令
    1、查看指定目录下内容
   hdfs dfs –ls <span class="token punctuation">[</span>文件目录<span class="token punctuation">]</span>
    hdfs dfs -ls -R   /                   //显式目录结构
    eg: hdfs dfs –ls /user/wangkai.pt
   2、打开某个已存在文件
    hdfs dfs –cat <span class="token punctuation">[</span>file_path<span class="token punctuation">]</span>
   eg:hdfs dfs -cat /user/wangkai.pt/data.txt
  3、将本地文件存储至hadoop
     hdfs dfs –put <span class="token punctuation">[</span>本地地址<span class="token punctuation">]</span> <span class="token punctuation">[</span>hadoop目录<span class="token punctuation">]</span>
     hdfs dfs –put /home/t/file.txt  /user/t  
  4、将本地文件夹存储至hadoop
    hdfs dfs –put <span class="token punctuation">[</span>本地目录<span class="token punctuation">]</span> <span class="token punctuation">[</span>hadoop目录<span class="token punctuation">]</span>
    hdfs dfs –put /home/t/dir_name /user/t
   <span class="token punctuation">(</span>dir_name是文件夹名<span class="token punctuation">)</span>
  5、将hadoop上某个文件down至本地已有目录下
     hadoop dfs -get <span class="token punctuation">[</span>文件目录<span class="token punctuation">]</span> <span class="token punctuation">[</span>本地目录<span class="token punctuation">]</span>
     hadoop dfs –get /user/t/ok.txt /home/t
  6、删除hadoop上指定文件
     hdfs  dfs –rm <span class="token punctuation">[</span>文件地址<span class="token punctuation">]</span>
     hdfs dfs –rm /user/t/ok.txt
  7、删除hadoop上指定文件夹（包含子目录等）
     hdfs dfs –rm <span class="token punctuation">[</span>目录地址<span class="token punctuation">]</span>
     hdfs dfs –rmr /user/t
  8、在hadoop指定目录内创建新目录
      hdfs dfs –mkdir /user/t
      hdfs  dfs -mkdir - p /user/centos/hadoop
  9、在hadoop指定目录下新建一个空文件
    使用touchz命令：
    hdfs dfs  -touchz  /user/new.txt
  10、将hadoop上某个文件重命名
   使用mv命令：
   hdfs dfs –mv  /user/test.txt  /user/ok.txt   （将test.txt重命名为ok.txt）
  11、将hadoop指定目录下所有内容保存为一个文件，同时down至本地
   hdfs dfs –getmerge /user /home/t
  12、将正在运行的hadoop作业kill掉
   hadoop job –kill  <span class="token punctuation">[</span>job-id<span class="token punctuation">]</span>
  13.查看帮助
  hdfs dfs -help        
</code></pre> 
<h3><a id="_559"></a>参考文献（章）</h3> 
<p>[1] <a href="https://www.bigdataframework.org/four-vs-of-big-data/" rel="nofollow">The 4 Characteristics of Big Data</a><br> [2] <a href="https://www.jianshu.com/p/fa8f6932b9e9" rel="nofollow">Ubuntu18.04 + docker + hadoop + spark 搭建分布式集群</a><br> [3] <a href="https://www.cnblogs.com/itxuexiwang/p/5236610.html" rel="nofollow">Linux 容器技术史话：从 chroot 到未来</a><br> [4] Hadoop大数据平台的搭建 崔文斌等 山东农业大学学报2013, 44( 4 ): 550-555<br> [5] <a href="https://www.jianshu.com/p/e1f7b8d5184c" rel="nofollow">docker底层原理介绍</a><br> [6] <a href="https://baike.baidu.com/item/Hadoop/3526507?fr=aladdin" rel="nofollow">Hadoop百度百科</a><br> [7] <a href="http://hadoop.apache.org/docs" rel="nofollow">Hadoop官网手册</a><br> [8] <a href="https://blog.csdn.net/Peter_Changyb/article/details/82682422">Hadoop的shuffle原理和过程图解</a><br> [9] <a href="https://www.cnblogs.com/LHWorldBlog/p/8514994.html" rel="nofollow">Hadoop常用命令总结</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/cc1dc9ec9b5e56164092699d14c9e03a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Stanford CS143 Compiler Fall2014 个人笔记</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/48ac19f403ded44cf2eb2a7014f66e88/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">（1）区间完全覆盖问题（最少）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>