<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>DDP分布式训练的官方demo及相关知识 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="DDP分布式训练的官方demo及相关知识" />
<meta property="og:description" content="参考链接
&#39;&#39;&#39; 1.分布式训练：并行化，跨进程和跨集群的计算 2.torch.distributed.init_process_group() 来初始化进程组,需要指定worker的通信机制, 一般为nccl(NVIDIA推出)，1个进程对应1个gpu. nccl是nvidia的显卡通信方式，用于把模型参数、梯度传递到训练的每个节点上。 3.DDP让进程组的所有worker进行通信 4.torch.utils.data.DataLoader中的batch_size指的是每个进程下的batch_size。也就是说， 总batch_size是这里的batch_size再乘以并行数(world_size)。 ######################################################################### rank是指在整个分布式任务中进程的序号；local_rank是指在一个node上进程的相对序号 nnodes是指物理节点数量 (node:物理节点，可以是一台机器也可以是一个容器，节点内部可以有多个GPU ) node_rank是物理节点的序号 nproc_per_node是指每个物理节点上面进程的数量 ######################################## 上一个运算题： 每个node包含16个GPU，且nproc_per_node=8，nnodes=3，机器的node_rank=5，请问word_size是多少？ 答案：word_size = 3*8 = 24 结论：word_size = nproc_per_node * nnodes ########################################################################## 多进程组启动方法：torch.distributed.launch 或 torchrun 常用启动方法示例：python3 -m torch.distributed.launch --nproc_per_node 2 main.py &#39;&#39;&#39; import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP class ToyModel(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(10, 10) self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/e1db6f474e5503e84754cd5c62861836/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-28T16:23:14+08:00" />
<meta property="article:modified_time" content="2023-12-28T16:23:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">DDP分布式训练的官方demo及相关知识</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><a class="link-info" href="https://www.cnblogs.com/azureology/p/16632988.html" rel="nofollow" title="参考链接">参考链接</a></p> 
<pre><code class="language-python">'''
1.分布式训练：并行化，跨进程和跨集群的计算
2.torch.distributed.init_process_group() 来初始化进程组,需要指定worker的通信机制,
    一般为nccl(NVIDIA推出)，1个进程对应1个gpu.
    nccl是nvidia的显卡通信方式，用于把模型参数、梯度传递到训练的每个节点上。
3.DDP让进程组的所有worker进行通信
4.torch.utils.data.DataLoader中的batch_size指的是每个进程下的batch_size。也就是说，
    总batch_size是这里的batch_size再乘以并行数(world_size)。

#########################################################################
rank是指在整个分布式任务中进程的序号；local_rank是指在一个node上进程的相对序号
nnodes是指物理节点数量 (node:物理节点，可以是一台机器也可以是一个容器，节点内部可以有多个GPU )
node_rank是物理节点的序号
nproc_per_node是指每个物理节点上面进程的数量
########################################
上一个运算题： 每个node包含16个GPU，且nproc_per_node=8，nnodes=3，机器的node_rank=5，请问word_size是多少？
答案：word_size = 3*8 = 24
结论：word_size = nproc_per_node * nnodes
##########################################################################

多进程组启动方法：torch.distributed.launch  或 torchrun
常用启动方法示例：python3 -m torch.distributed.launch --nproc_per_node 2 main.py
'''

import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP


class ToyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(10, 5)

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))


def demo_basic():
    dist.init_process_group('nccl')
    rank = dist.get_rank()
    print(f'running on {rank}')

    # get the number of GPUs available
    n_gpus = torch.cuda.device_count()
    # get every rank
    device_id = rank % n_gpus
    model = ToyModel().to(device_id)
    '''
    多卡多线程，设置broadcast_buffers=False,会报错:
    'RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu! 
    (when checking argument for argument mat1 in method wrapper_CUDA_addmm)'
    示例
    ddp_model = DDP(model, broadcast_buffers=False)    
    '''
    ddp_model = DDP(model, device_ids=[device_id])

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)
    optimizer.zero_grad()
    inputs = torch.randn(3, 10)
    labels = torch.randn(3, 10).to(device_id)
    output = ddp_model(inputs)
    loss = loss_fn(output, labels)
    loss.backward()
    optimizer.step()


if __name__ == '__main__':
    demo_basic()
</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e86ab0b3bda52a83fd44bb381336a6a5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">简单快速把本地的工程上传到gitlab/github新建的工程的主分支/已存在工程的其他分支中</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fa9177e6d8adba595deee29e382c180d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">软件测试之自动化测试框架（超详细）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>