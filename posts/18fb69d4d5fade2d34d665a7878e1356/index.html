<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>领域自适应(Domain Adaptation)方法综述 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="领域自适应(Domain Adaptation)方法综述" />
<meta property="og:description" content="目录
1 迁移学习的直观理解
2 迁移学习的种类
3 领域自适应简述
4 DA的研究方向
5 DA方法的种类
6 深度学习中的DA方法
6.1 基于差异的方法
6.2 基于对抗的方法
6.3 基于重构的方法
1 迁移学习的直观理解 人类容易在类似的任务上利用先前的经验，比如学过自行车就很容易学会摩托车，学会打羽毛球也能帮助学习打网球，学过小提琴也会对学习二胡有帮助。也就是把一个领域上学习的知识迁移到另一个领域上，目的也是让计算机有举一反三的能力（大概是实现AGI的一个重要的坎），或者是去尝试充分利用已经训练过的某个领域的知识来解决当前的任务（这样可以解决数据少的问题）。
在迁移学习中要强调源域（Source Domain）、源任务（Source Task）、目标域（Target Domain）和目标任务（Target Domain） 的概念。在普通机器学习方法里，源域和目标域是一样的，就是训练的是什么数据测试的也是这个领域的数据（比如“摄像头捕捉的行人图片”）；源任务和目标任务也是一样的，即希望模型能做什么事情，训练的就是做这件事（比如“分类猫和狗”）。而在迁移学习中源域和目标域可能是不一样的，源任务和目标任务也可能是不一样的（甚至可能一个是分类一个是回归），我的理解是源域-目标域、源任务-目标任务至少有一对不一样才能称为迁移学习。
2 迁移学习的种类 在2012年的SJ Pan的综述里将迁移学习按照有标记的样本的情况分为下面三大类，可以解决不同的问题。
3 领域自适应简述 Domain Adaptation是一种源任务和目标任务一样，但是源域和目标域的数据分布不一样，并且源域有大量的标记好的样本，目标域则没有（或者只有非常少的）有标记的样本的迁移学习方法。这样就是怎么把源域上从大量的有标记样本中学习的知识迁移到目标域上，来解决相同的问题，而目标域上能利用的大多只有没有标记的样本。
这里要解释一下“数据分布不一样”是什么意思，就比如下图中(a)组是不同来源的自行车和笔记本电脑的照片，有从购物网站下载的，也有数码相机拍的生活照，也有网络上获取的照片等，它们虽然都表达自行车和笔记本电脑，但是数据分布是不同的。
比如用(b)组的门牌号数据集SVHN去训练模型，去提取SVNH和MNIST的特征，然后将其可视化到一个平面内，是下图左边的样子，蓝色点是源域（SVNH）的样本，红色的点是目标域（MNIST）的样本，也就是说直接在源域上训练得到的分类器的分类边界无法很好的区分目标域的样本。而领域自适应这种迁移学习方法想达到的效果就是下图右边这样，让源域和目标域中的样本能对齐，这样模型就能在目标域上很好的使用了。
4 DA的研究方向 在领域自适应里面也会细分出很多方向。如果源域和目标域距离太大（比如源域是文字，目标域是图像），就可能需要进程多步的迁移，将这个非常大的迁移划分成一步一步的小段迁移，这就是下图中的多步领域自适应（Multi-step DA） 通过选择合适的中间域来转换成一个个单步领域自适应（One-step DA），这样就只要去研究单步迁移怎么做。
然后单步迁移又可以根据源域和目标域数据情况可以分成同质（Homogeneous，即数据空间一样，只是数据分布不一样）和异质（Heterogeneous，数据空间都不同）两种。
接下来，在同质或者异质的DA中又分别可以根据目标域数据的打标签情况分为监督的、半监督的、无监督的DA。学术界研究最多的是无监督的DA，这个比较困难而且价值比较高。
5 DA方法的种类 传统的的ML方法是最小化损失：
基于特征的自适应（Feature Adaption）是将源域样本和目标域样本用一个映射Φ 调整到同一个特征空间，这样在这个特征空间样本能够“对齐”，这也是最常用的方法：
基于实例的自适应（Instance Adaption）是考虑到源域中总有一些样本和目标域样本很相似，那么就将源域的所有样本的Loss在训练时都乘以一个权重（即表示“看重”的程度），和目标域越相似的样本，这个权重就越大：
基于模型参数的自适应（Model Adaption）是找到新的参数θ ′ 通过参数的迁移使得模型能更好的在目标域上工作：
如果目标域数据没有标签，就没法用Fine-Tune把目标域数据扔进去训练，这时候无监督的自适应方法就是基于特征的自适应。因为有很多能衡量源域和目标域数据的距离的数学公式，那么就能把距离计算出来嵌入到网络中作为Loss来训练，这样就能优化让这个距离逐渐变小，最终训练出来的模型就将源域和目标域就被放在一个足够近的特征空间里了。
这些衡量源域和目标域数据距离的数学公式有KL Divergence、MMD、H-divergence和Wasserstein distance等。
6 深度学习中的DA方法 注意，以下三种方法主体都属于5种基于特征的自适应方法。
6.1 基于差异的方法 例如经典的用于无监督DA的DDC方法，它是使用MMD（Maximum Mean Discrepancy） ，即找一个核函数，将源域和目标域都映射到一个再生核的Hilbert空间上，在这个空间上取这个两个域数据分别作均值之后的差，然后将这个差作为距离。用这个方法训练网络的Loss是：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/18fb69d4d5fade2d34d665a7878e1356/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-27T09:18:33+08:00" />
<meta property="article:modified_time" content="2023-04-27T09:18:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">领域自适应(Domain Adaptation)方法综述</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2></h2> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"></p> 
<p id="1%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3-toc" style="margin-left:0px;"><a href="#1%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3" rel="nofollow">1 迁移学习的直观理解</a></p> 
<p id="2%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A7%8D%E7%B1%BB-toc" style="margin-left:0px;"><a href="#2%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A7%8D%E7%B1%BB" rel="nofollow">2 迁移学习的种类</a></p> 
<p id="%C2%A03%20%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94%E7%AE%80%E8%BF%B0-toc" style="margin-left:0px;"><a href="#%C2%A03%20%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94%E7%AE%80%E8%BF%B0" rel="nofollow">3 领域自适应简述</a></p> 
<p id="4%20DA%E7%9A%84%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91-toc" style="margin-left:0px;"><a href="#4%20DA%E7%9A%84%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91" rel="nofollow">4 DA的研究方向</a></p> 
<p id="5%20DA%E6%96%B9%E6%B3%95%E7%9A%84%E7%A7%8D%E7%B1%BB-toc" style="margin-left:0px;"><a href="#5%20DA%E6%96%B9%E6%B3%95%E7%9A%84%E7%A7%8D%E7%B1%BB" rel="nofollow">5 DA方法的种类</a></p> 
<p id="6%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84DA%E6%96%B9%E6%B3%95-toc" style="margin-left:0px;"><a href="#6%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84DA%E6%96%B9%E6%B3%95" rel="nofollow">6 深度学习中的DA方法</a></p> 
<p id="6.1%20%E5%9F%BA%E4%BA%8E%E5%B7%AE%E5%BC%82%E7%9A%84%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#6.1%20%E5%9F%BA%E4%BA%8E%E5%B7%AE%E5%BC%82%E7%9A%84%E6%96%B9%E6%B3%95" rel="nofollow">6.1 基于差异的方法</a></p> 
<p id="6.2%20%E5%9F%BA%E4%BA%8E%E5%AF%B9%E6%8A%97%E7%9A%84%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#6.2%20%E5%9F%BA%E4%BA%8E%E5%AF%B9%E6%8A%97%E7%9A%84%E6%96%B9%E6%B3%95" rel="nofollow">6.2 基于对抗的方法</a></p> 
<p id="6.3%20%E5%9F%BA%E4%BA%8E%E9%87%8D%E6%9E%84%E7%9A%84%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#6.3%20%E5%9F%BA%E4%BA%8E%E9%87%8D%E6%9E%84%E7%9A%84%E6%96%B9%E6%B3%95" rel="nofollow">6.3 基于重构的方法</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2>1 迁移学习的直观理解</h2> 
<p>人类容易<strong>在类似的任务上利用先前的经验</strong>，比如学过自行车就很容易学会摩托车，学会打羽毛球也能帮助学习打网球，学过小提琴也会对学习二胡有帮助。也就是<strong>把一个领域上学习的知识迁移到另一个领域上</strong>，目的也是让计算机有举一反三的能力（大概是实现AGI的一个重要的坎），或者是去尝试充分利用已经训练过的某个领域的知识来解决当前的任务（这样可以解决数据少的问题）。</p> 
<p>在迁移学习中要强调<strong>源域（Source Domain）、源任务（Source Task）、目标域（Target Domain）和目标任务（Target Domain）</strong> 的概念。在普通机器学习方法里，源域和目标域是一样的，就是训练的是什么数据测试的也是这个领域的数据（比如“摄像头捕捉的行人图片”）；源任务和目标任务也是一样的，即希望模型能做什么事情，训练的就是做这件事（比如“分类猫和狗”）。而在迁移学习中<strong>源域和目标域可能是不一样的，源任务和目标任务也可能是不一样的</strong>（甚至可能一个是分类一个是回归），我的理解是源域-目标域、源任务-目标任务至少有一对不一样才能称为迁移学习。</p> 
<h2 id="2%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A7%8D%E7%B1%BB">2 迁移学习的种类</h2> 
<p>在<a href="https://ieeexplore.ieee.org/abstract/document/5288526/" rel="nofollow" title="2012年的SJ Pan的综述">2012年的SJ Pan的综述</a>里将迁移学习<strong>按照有标记的样本的情况</strong>分为下面三大类，可以解决不同的问题。</p> 
<p><img alt="" height="775" src="https://images2.imgbox.com/73/40/7Uve0NpW_o.png" width="1200"></p> 
<h2 id="%C2%A03%20%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94%E7%AE%80%E8%BF%B0">3 领域自适应简述</h2> 
<p>Domain Adaptation是一种<strong>源任务和目标任务一样，但是源域和目标域的数据分布不一样</strong>，并且<strong>源域有大量的标记好的样本，目标域则没有（或者只有非常少的）有标记的样本</strong>的迁移学习方法。这样就是怎么把源域上从大量的有标记样本中学习的知识迁移到目标域上，来解决相同的问题，而目标域上能利用的大多只有没有标记的样本。</p> 
<p>这里要解释一下“数据分布不一样”是什么意思，就比如下图中(a)组是不同来源的自行车和笔记本电脑的照片，有从购物网站下载的，也有数码相机拍的生活照，也有网络上获取的照片等，它们虽然都表达自行车和笔记本电脑，但是数据分布是不同的。</p> 
<p><img alt="" height="598" src="https://images2.imgbox.com/30/60/JG8DOzBT_o.png" width="1200"></p> 
<p> 比如用(b)组的门牌号数据集SVHN去训练模型，去提取SVNH和MNIST的特征，然后将其可视化到一个平面内，是下图左边的样子，蓝色点是源域（SVNH）的样本，红色的点是目标域（MNIST）的样本，也就是说<strong>直接在源域上训练得到的分类器的分类边界无法很好的区分目标域的样本</strong>。而领域自适应这种迁移学习方法想达到的效果就是下图右边这样，让源域和目标域中的样本能对齐，这样模型就能在目标域上很好的使用了。<br><img alt="" height="736" src="https://images2.imgbox.com/25/9e/u5NeAqrj_o.png" width="1200"></p> 
<h2 id="4%20DA%E7%9A%84%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91">4 DA的研究方向</h2> 
<p>在领域自适应里面也会细分出很多方向。如果源域和目标域距离太大（比如源域是文字，目标域是图像），就可能需要进程多步的迁移，将这个非常大的迁移划分成一步一步的小段迁移，这就是下图中的<strong>多步领域自适应（Multi-step DA）</strong> 通过选择合适的中间域来转换成一个个<strong>单步领域自适应（One-step DA）</strong>，这样就只要去研究单步迁移怎么做。</p> 
<p>然后单步迁移又可以根据源域和目标域数据情况可以分成<strong>同质</strong>（Homogeneous，即数据空间一样，只是数据分布不一样）和<strong>异质</strong>（Heterogeneous，数据空间都不同）两种。</p> 
<p><img alt="Homogeneous=&gt;\chi _{s}=\chi _{\tau },P(\chi _{s})\neq P(\chi _{\tau })" class="mathcode" src="https://images2.imgbox.com/ac/47/xVkMWcXy_o.png"></p> 
<p><img alt="Homogeneous=&gt;\chi _{s}\neq \chi _{\tau }" class="mathcode" src="https://images2.imgbox.com/35/bc/CwVfWvJF_o.png"></p> 
<p>接下来，在同质或者异质的DA中又分别可以<strong>根据目标域数据的打标签情况</strong>分为<strong>监督的、半监督的、无监督的DA</strong>。学术界研究最多的是无监督的DA，这个比较困难而且价值比较高。</p> 
<p><img alt="" height="627" src="https://images2.imgbox.com/92/3f/X6LA7GVC_o.png" width="1200"></p> 
<h2 id="5%20DA%E6%96%B9%E6%B3%95%E7%9A%84%E7%A7%8D%E7%B1%BB">5 DA方法的种类</h2> 
<p>传统的的ML方法是最小化损失：</p> 
<p><img alt="min\frac{1}{n}\sum_{i=1}^{n}L(x_{i},y_{i},\theta )" class="mathcode" src="https://images2.imgbox.com/46/d0/4494RAoO_o.png"></p> 
<p><strong>基于特征的自适应</strong>（Feature Adaption）是将源域样本和目标域样本用一个映射Φ 调整到同一个特征空间，这样在这个特征空间样本能够“对齐”，这也是最常用的方法：</p> 
<p><img alt="min\frac{1}{n}\sum_{i=1}^{n}L(\Phi (x_{i}^{s}),y_{i}^{s},\theta )" class="mathcode" src="https://images2.imgbox.com/0d/6d/DP58Ud3a_o.png"></p> 
<p><strong>基于实例的自适应</strong>（Instance Adaption）是考虑到源域中总有一些样本和目标域样本很相似，那么就将源域的所有样本的Loss在训练时都乘以一个权重<img alt="w_{i}" class="mathcode" src="https://images2.imgbox.com/ae/77/aIZFBBWs_o.png">（即表示“看重”的程度），和目标域越相似的样本，这个权重就越大：</p> 
<p><img alt="min\frac{1}{n}\sum_{i=1}^{n}w_{i}L(x_{i}^{s},y_{i}^{s},\theta )" class="mathcode" src="https://images2.imgbox.com/e7/56/eWvKHp1K_o.png"></p> 
<p><strong>基于模型参数的自适应</strong>（Model Adaption）是找到新的参数θ ′ 通过参数的迁移使得模型能更好的在目标域上工作：</p> 
<p><img alt="min\frac{1}{n}\sum_{i=1}^{n}L(x_{i}^{s},y_{i}^{s},\theta ^{'})" class="mathcode" src="https://images2.imgbox.com/66/fa/3dwZoBK2_o.png"></p> 
<p>如果目标域数据没有标签，就没法用Fine-Tune把目标域数据扔进去训练，这时候无监督的自适应方法就是基于特征的自适应。因为有很多能<strong>衡量源域和目标域数据的距离的数学公式</strong>，那么就能<strong>把距离计算出来嵌入到网络中作为Loss来训练</strong>，这样就能优化让这个距离逐渐变小，最终训练出来的模型就将源域和目标域就被放在一个足够近的特征空间里了。</p> 
<p>这些衡量源域和目标域数据距离的数学公式有KL Divergence、MMD、H-divergence和Wasserstein distance等。</p> 
<h2 id="6%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84DA%E6%96%B9%E6%B3%95"><strong>6 深度学习中的DA方法</strong></h2> 
<p>注意，以下三种方法主体都属于5种基于特征的自适应方法。</p> 
<h3 id="6.1%20%E5%9F%BA%E4%BA%8E%E5%B7%AE%E5%BC%82%E7%9A%84%E6%96%B9%E6%B3%95"><strong>6.1 基于差异的方法</strong></h3> 
<p>例如经典的用于无监督DA的<strong>DDC</strong>方法，它是使用<strong>MMD（Maximum Mean Discrepancy）</strong> ，即找一个核函数，将源域和目标域都映射到一个再生核的Hilbert空间上，在这个空间上取这个两个域数据分别作均值之后的差，然后将这个差作为距离。用这个方法训练网络的Loss是：</p> 
<p><img alt="L=L_{C}(X_{s},y)+\lambda \sum_{l\in L}^{}L_{M}(D_{s}^{l},D_{t}^{l})" class="mathcode" src="https://images2.imgbox.com/77/09/w4ns4V5h_o.png"></p> 
<p>其中第一项就是源域之前的模型的Loss（比如分类任务就是分类Loss），然后第二项是在指定层l ll上的MMD距离之和，乘了个表示重要性的系数λ \lambdaλ。在训练时有两个网络，一边是源域的，一边是目标域的，它们共享参数，然后在较深的某些层去计算MMD距离，然后按上面的公式那样加在一起作为整个模型的Loss。</p> 
<p><img alt="" height="921" src="https://images2.imgbox.com/8c/bd/cE199E2a_o.png" width="829"> </p> 
<p>这个DDC方法有很多改进，比如<strong>DAN（ICML,2015） </strong>就是用了多个核函数的线性组合，并且在多个层上计算MMD距离</p> 
<p><img alt="" height="507" src="https://images2.imgbox.com/ac/35/8xAvkUc8_o.png" width="1200"></p> 
<p>而<strong>RTN（NIPS,2016）</strong> 前半部分还是DAN，但是光靠DAN特征未必能对齐的那么好，所以在之前直接接源域分类器的地方改成了一个残差结构，用来学习源域和目标域分类器的差异。这种方法就是相当于在DAN上补充了5中基于模型参数的自适应方法。</p> 
<p><img alt="" height="343" src="https://images2.imgbox.com/a5/80/GbdW6SpC_o.png" width="1200"></p> 
<p>还有<strong>JAN（arXiv,2016）</strong> 里提出了一个<strong>JMMD（联合分布的MMD）</strong>，通过优化这个JMMD能让源域和目标域特征和标签的联合分布更近，这样效果更好。</p> 
<p><img alt="" height="458" src="https://images2.imgbox.com/fa/8a/jtjCAVSq_o.png" width="1200"></p> 
<h3 id="6.2%20%E5%9F%BA%E4%BA%8E%E5%AF%B9%E6%8A%97%E7%9A%84%E6%96%B9%E6%B3%95">6.2 基于对抗的方法</h3> 
<p>如<strong>RevGrad（ICML,2015）</strong> 的基本思路就是用GAN去让生成器生成特征，然后让判别器判别它是源域的还是目标域的特征，如果判别不出来就说明在这个特征空间里源域和目标域是一致的。</p> 
<p>下图中绿色部分是一个特征提取器，源域和目标域数据都扔进去，它就是用来生成（或者叫提取）特征的，然后紫色部分是对源域数据的特征做分类的分类器，红色部分是对源域数据和目标域数据的特征做判别的判别器，这个判别器要不断增强（能很好的判别是源域的还是目标域的特征），同时生成器也要增强，让生成出来的特征能混淆判别器的判别，这样最后生成（提取）出的特征就是源域和目标域空间里一致的了。</p> 
<p><img alt="" height="703" src="https://images2.imgbox.com/29/6c/IfzOuPth_o.png" width="1200"></p> 
<p>这个可以用GAN的最小化-最大化的思想去训练，也可以用论文中的<strong>梯度反转层（Gradient Reversal Layer）</strong> 的方法，就是在上图中白色空心箭头的位置加了个梯度反转层，在前向传播的过程中就是正常的网络，即最小化Loss让红色部分的判别器性能更好，再反向传播的过程中把梯度取负，即优化绿色部分的特征提取器，来尽量让红色部分的判别器分不清特征是源域的还是目标域的。这个方法就是一个训练技巧。</p> 
<p>对于它的改进有<strong>CAN（CVPR,2018）</strong>，它把深度网络连续的若干层作为一个block，这样划分成几个block，然后对每个block加一个判别器。它提出<strong>希望在网络高层的block中的特征和域的信息无关</strong>，因为最后要得到的就是不区分源域和目标域数据的网络；而希<strong>望在网络的低层的block中的特征和域的信息有关</strong>，因为底层在提取边缘信息，希望这些边缘信息能更好提取目标域的特征。</p> 
<p>还有<strong>MADA（AAAI,2018）</strong>。之前的方法都是源域和目标域的类别都是相同的这些，但是有时候源域和目标域类别不一定相同，比如目标域类别可以是源域类别的子集。这个方法就是提出<strong>不应该是域到域的对齐，而是应该精细到类别到类别的对齐</strong>。这种方式就是只在最后一层用判别器，但是<strong>对于每个类别都单独使用一个判别器</strong>，这种就是<strong>引入语义信息（类别信息）的对齐</strong>，能让特征空间对齐的更好。但是因为在无监督的DA里目标域样本没有标签，所以这里要用源域分类器去对目标域样本输出属于每个类的概率，属于哪个类的概率更大就让那个类的判别器发挥更大的作用。这种方法就是相当于在RevGrad上补充了5种基于实例的自适应方法。</p> 
<p><img alt="" height="701" src="https://images2.imgbox.com/df/21/W9To9ya1_o.png" width="1200"></p> 
<h3 id="6.3%20%E5%9F%BA%E4%BA%8E%E9%87%8D%E6%9E%84%E7%9A%84%E6%96%B9%E6%B3%95">6.3 基于重构的方法</h3> 
<p><strong>DRCN（2016,ECCV）</strong> 如下图结构，左侧是一个Encoder，也是将源域和目标域样本都扔进去生成特征用的，然后对于源域特征用一个分类器去分类，这样使得Encoder生成的特征能够很好的区分源域的样本（即是一个比较好的特征），对于目标域特征用一个Decoder去解码，使得能尽量还原目标域的样本。这样下来生成的特征所在的特征空间在源域和目标域样本上比较近。</p> 
<p><img alt="" height="870" src="https://images2.imgbox.com/e4/51/btfJzlTz_o.png" width="1200"></p> 
<p><strong>DSN（NIPS,2016） </strong>将源域和目标域的样本分别拆分成两部分，一部分是两个域私有的Encoder，即尝试编码各自域中特定的信息，另一部分是两个域共有的Encoder，显然想到得到的就是这种共有特征。在分类时尽量<strong>让私有的特征和共有的特征正交</strong>，这样体现出它们更不相关，两个域各有一个损失<img alt="L_{difference}^{}" class="mathcode" src="https://images2.imgbox.com/4c/70/u9g4PARs_o.png"> 。还要保证<strong>私有特征和共有特征通过Decoder能尽可能还原出之前的样本</strong>，两个域各有一个损失<img alt="L_{rccon}" class="mathcode" src="https://images2.imgbox.com/76/86/Mu2yYyyV_o.png">。还要保证<strong>两个域生成的共有特征尽可能相像</strong>，对应一个损失<img alt="L_{similarity}" class="mathcode" src="https://images2.imgbox.com/fb/e0/WaoqCxNb_o.png">。训练好后，用源域样本（生成的共有特征）训练一个分类器，这个分类器在目标域上也有较好的性能。</p> 
<p><img alt="" height="1053" src="https://images2.imgbox.com/76/98/gP3CjiF2_o.png" width="1200"></p> 
<p>  <a class="link-info" href="https://blog.csdn.net/SHU15121856/article/details/106874558" title="原文链接">原文链接</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/00e88efa51a463884b65c170cc7ab425/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">十四、Redis——数据结构</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3fe748e21cc7fb3d406db408405b8e5a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MySql中一条语句递归查询所有子节点，递归删除所有子节点</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>