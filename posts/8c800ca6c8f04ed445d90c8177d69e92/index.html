<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python爬虫实战——小红书 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="python爬虫实战——小红书" />
<meta property="og:description" content="目录
1、博主页面分析
2、在控制台预先获取所有作品页的URL 3、在 Python 中读入该文件并做准备工作 4、处理图文类型作品
5、处理视频类型作品
6、异常访问而被中断的现象
7、完整参考代码 任务：在 win 环境下，利用 Python、webdriver、JavaScript等，获取 xiaohongshu 某个博主的全部作品。
本文仅做学习和交流使用。
1、博主页面分析 section 代表每一项作品，但即使博主作品有很多，在未登录状态下，只会显示 20 项左右。向下滚动页面，section 发生改变（个数不变），标签中的 index 会递增。 向下滚动页面时，到一定的范围时，会发送一个获取作品数据的请求，该请求每次只请求 30 项作品数据。该请求携带了 cookies 以及其他不确定值的参数。 2、在控制台预先获取所有作品页的URL 为了获取博主的全部作品数据，在登录的状态下访问目标博主页面，在控制台中注入JavaScript 脚本（在没有滚动过页面的情况下）。该脚本不断滚动页面到最底部，每次滚动一段距离后，都获取每一个作品的信息（通过a标签的 href 获取到作品页URL；通过判断a标签是否有一个 class=&#39;play-icon&#39; 的后代元素来判断是图文还是视频类型的作品，如果有该标签就是视频作品，反之则是图文作品）。
将作品页 URL 作为键，作品是视频作品还是图文作品作为值，添加到 js 对象中。
显示完所有的作品，即滚动到最底部时，将所有获取到的作品信息导出为 txt 文件。
在控制台中执行：
// 页面高度 const vh = window.innerHeight let work_obj = {} // 延迟 function delay(ms){ return new Promise(resolve =&gt; setTimeout(resolve, ms)); } async function action() { let last_height = document." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/8c800ca6c8f04ed445d90c8177d69e92/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-14T22:29:10+08:00" />
<meta property="article:modified_time" content="2024-03-14T22:29:10+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python爬虫实战——小红书</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="1%E3%80%81%E5%8D%9A%E4%B8%BB%E9%A1%B5%E9%9D%A2%E5%88%86%E6%9E%90-toc" style="margin-left:0px;"><a href="#1%E3%80%81%E5%8D%9A%E4%B8%BB%E9%A1%B5%E9%9D%A2%E5%88%86%E6%9E%90" rel="nofollow">1、博主页面分析</a></p> 
<p id="2%E3%80%81%E5%9C%A8%E6%8E%A7%E5%88%B6%E5%8F%B0%E9%A2%84%E5%85%88%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89%E4%BD%9C%E5%93%81%E9%A1%B5%E7%9A%84URL%C2%A0-toc" style="margin-left:0px;"><a href="#2%E3%80%81%E5%9C%A8%E6%8E%A7%E5%88%B6%E5%8F%B0%E9%A2%84%E5%85%88%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89%E4%BD%9C%E5%93%81%E9%A1%B5%E7%9A%84URL%C2%A0" rel="nofollow">2、在控制台预先获取所有作品页的URL </a></p> 
<p id="3%E3%80%81%E5%9C%A8%20Python%20%E4%B8%AD%E8%AF%BB%E5%85%A5%E8%AF%A5%E6%96%87%E4%BB%B6%E5%B9%B6%E5%81%9A%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C%C2%A0-toc" style="margin-left:0px;"><a href="#3%E3%80%81%E5%9C%A8%20Python%20%E4%B8%AD%E8%AF%BB%E5%85%A5%E8%AF%A5%E6%96%87%E4%BB%B6%E5%B9%B6%E5%81%9A%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C%C2%A0" rel="nofollow">3、在 Python 中读入该文件并做准备工作 </a></p> 
<p id="4%E3%80%81%E5%A4%84%E7%90%86%E5%9B%BE%E6%96%87%E7%B1%BB%E5%9E%8B%E4%BD%9C%E5%93%81-toc" style="margin-left:0px;"><a href="#4%E3%80%81%E5%A4%84%E7%90%86%E5%9B%BE%E6%96%87%E7%B1%BB%E5%9E%8B%E4%BD%9C%E5%93%81" rel="nofollow">4、处理图文类型作品</a></p> 
<p id="5%E3%80%81%E5%A4%84%E7%90%86%E8%A7%86%E9%A2%91%E7%B1%BB%E5%9E%8B%E4%BD%9C%E5%93%81-toc" style="margin-left:0px;"><a href="#5%E3%80%81%E5%A4%84%E7%90%86%E8%A7%86%E9%A2%91%E7%B1%BB%E5%9E%8B%E4%BD%9C%E5%93%81" rel="nofollow">5、处理视频类型作品</a></p> 
<p id="6%E3%80%81%E5%BC%82%E5%B8%B8%E8%AE%BF%E9%97%AE%E8%80%8C%E8%A2%AB%E4%B8%AD%E6%96%AD%E7%9A%84%E7%8E%B0%E8%B1%A1-toc" style="margin-left:0px;"><a href="#6%E3%80%81%E5%BC%82%E5%B8%B8%E8%AE%BF%E9%97%AE%E8%80%8C%E8%A2%AB%E4%B8%AD%E6%96%AD%E7%9A%84%E7%8E%B0%E8%B1%A1" rel="nofollow">6、异常访问而被中断的现象</a></p> 
<p id="7%E3%80%81%E5%AE%8C%E6%95%B4%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81%C2%A0-toc" style="margin-left:0px;"><a href="#7%E3%80%81%E5%AE%8C%E6%95%B4%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81%C2%A0" rel="nofollow">7、完整参考代码 </a></p> 
<hr id="hr-toc"> 
<p></p> 
<p>        任务：在 win 环境下，利用 Python、webdriver、JavaScript等，获取 xiaohongshu 某个博主的全部作品。</p> 
<p>        本文仅做学习和交流使用。</p> 
<p></p> 
<h2 id="1%E3%80%81%E5%8D%9A%E4%B8%BB%E9%A1%B5%E9%9D%A2%E5%88%86%E6%9E%90"><span style="color:#be191c;">1、博主页面分析</span></h2> 
<p><img alt="" height="1027" src="https://images2.imgbox.com/56/e0/lDCseqgB_o.png" width="1200"></p> 
<p>        section 代表每一项作品，但即使博主作品有很多，在未登录状态下，只会显示 20 项左右。向下滚动页面，section 发生改变（个数不变），标签中的 index 会递增。  </p> 
<p class="img-center"><img alt="" height="1006" src="https://images2.imgbox.com/b5/e9/S6eE0WVC_o.png" width="864"></p> 
<p></p> 
<p>        向下滚动页面时，到一定的范围时，会发送一个获取作品数据的请求，该请求每次只请求 30 项作品数据。该请求携带了 cookies 以及其他不确定值的参数。 </p> 
<p class="img-center"><img alt="" height="780" src="https://images2.imgbox.com/24/97/zjaxje2T_o.png" width="1200"></p> 
<p></p> 
<h2 id="2%E3%80%81%E5%9C%A8%E6%8E%A7%E5%88%B6%E5%8F%B0%E9%A2%84%E5%85%88%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89%E4%BD%9C%E5%93%81%E9%A1%B5%E7%9A%84URL%C2%A0"><span style="color:#be191c;">2、在控制台预先获取所有作品页的URL </span></h2> 
<p>        为了获取博主的全部作品数据，在登录的状态下访问目标博主页面，在控制台中注入JavaScript 脚本（在没有滚动过页面的情况下）。该脚本不断滚动页面到最底部，每次滚动一段距离后，都获取每一个作品的信息（通过a标签的 href 获取到作品页URL；通过判断a标签是否有一个 class='play-icon' 的后代元素来判断是图文还是视频类型的作品，如果有该标签就是视频作品，反之则是图文作品）。</p> 
<p>        将作品页 URL 作为键，作品是视频作品还是图文作品作为值，添加到 js 对象中。</p> 
<p>        显示完所有的作品，即滚动到最底部时，将所有获取到的作品信息导出为 txt 文件。</p> 
<p class="img-center"><img alt="" height="1077" src="https://images2.imgbox.com/5c/99/nmeNHsrH_o.png" width="911"></p> 
<p>         在控制台中执行：</p> 
<pre><code class="language-javascript">// 页面高度
const vh = window.innerHeight
let work_obj = {}

// 延迟
function delay(ms){  
    return new Promise(resolve =&gt; setTimeout(resolve, ms));  
}

async function action() {  
  let last_height = document.body.offsetHeight;
  window.scrollTo(0, window.scrollY + vh * 1.5)
  
  ul = 	document.querySelector('#userPostedFeeds').querySelectorAll('.cover')

  ul.forEach((e,index)=&gt;{
    // length 为 0 时是图片，为 1 时为视频
    work_obj[e.href] = ul[index].querySelector('.play-icon') ? 1 : 0
  })
	// 延迟500ms
  await delay(500);
  // console.log(last_height, document.body.offsetHeight)
  
  // 判断是否滚动到底部
  if(document.body.offsetHeight &gt; last_height){
    action()
  }else{
    console.log('end')
    // 作品的数量
    console.log(Object.keys(work_obj).length)

    // 转换格式，并下载为txt文件
    var content = JSON.stringify(work_obj);  
    var blob = new Blob([content], {type: "text/plain;charset=utf-8"});  
    var link = document.createElement("a");  
    link.href = URL.createObjectURL(blob);  
    link.download = "xhs_works.txt";  
    link.click();
  }
}

action()</code></pre> 
<p>        写出的 txt 文件内容如下：</p> 
<p><img alt="" height="731" src="https://images2.imgbox.com/63/43/7FOUHqrN_o.png" width="1161"></p> 
<p></p> 
<h2 id="3%E3%80%81%E5%9C%A8%20Python%20%E4%B8%AD%E8%AF%BB%E5%85%A5%E8%AF%A5%E6%96%87%E4%BB%B6%E5%B9%B6%E5%81%9A%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C%C2%A0"><span style="color:#be191c;">3、在 Python 中读入该文件并做准备工作</span> </h2> 
<pre><code class="language-python"># 获取当前时间
def get_current_time():
    now = datetime.now()
    format_time = now.strftime("_%Y-%m-%d__%H-%M-%S-%f__")
    return format_time

# 下载的作品保存的路径，以作者主页的 id 号命名
ABS_BASE_URL = f'G:\\639476c10000000026006023'

# 检查作品是否已经下载过
def check_download_or_not(work_id, is_pictures):
    end_str = 'pictures' if is_pictures else 'video'
    # work_id 是每一个作品的目录，检查目录是否存在并且是否有内容，则能判断对应的作品是否被下载过
    path = f'{ABS_BASE_URL}/{work_id}-{end_str}'
    if os.path.exists(path) and os.path.isdir(path):
        if os.listdir(path):
            return True
    return False

# 下载资源
def download_resource(url, save_path):
    response = requests.get(url, stream=True)
    if response.status_code == 200:
        with open(save_path, 'wb') as file:
            for chunk in response.iter_content(1024):
                file.write(chunk)</code></pre> 
<p>读入文件，判断作品数量然后进行任务分配： </p> 
<pre><code class="language-python"># 读入文件
content = ''
with open('./xhs_works.txt', mode='r', encoding='utf-8') as f:
    content = json.load(f)

# 转换成 [[href, is_pictures],[href, is_pictures],...] 类型
# 每一维中分别是作品页的URL、作品类型
url_list = [list(pair) for pair in content.items()]

# 有多少个作品
length = len(url_list)

if length &gt; 3:
    ul = [url_list[0: int(length / 3) + 1], url_list[int(length / 3) + 1: int(length / 3) * 2 + 1],url_list[int(length / 3) * 2 + 1: length]]
    # 开启三个线程并分配任务
    for child_ul in ul:
        thread = threading.Thread(target=thread_task, args=(child_ul,))
        thread.start()
else:
    thread_task(url_list)</code></pre> 
<p>若使用多线程，每一个线程处理自己被分配到的作品列表： </p> 
<pre><code class="language-python"># 每一个线程遍历自己分配到的作品列表，进行逐项处理
def thread_task(ul):
    for item in ul:
        href = item[0]
        is_pictures = (True if item[1] == 0 else False)
        res = work_task(href, is_pictures)
        if res == 0:        # 被阻止正常访问
            break</code></pre> 
<p>处理每一项作品： </p> 
<pre><code class="language-python"># 处理每一项作品
def work_task(href, is_pictures):
    # href 中最后的一个路径参数就是博主的id
    work_id = href.split('/')[-1]

    # 判断是否已经下载过该作品
    has_downloaded = check_download_or_not(work_id, is_pictures)

    # 没有下载，则去下载
    if not has_downloaded:
        if not is_pictures:
            res = deal_video(work_id)
        else:
            res = deal_pictures(work_id)
        if res == 0:
            return 0            # 无法正常访问
    else:
        print('当前作品已被下载')
        return 2
    return 1</code></pre> 
<p></p> 
<p></p> 
<h2 id="4%E3%80%81%E5%A4%84%E7%90%86%E5%9B%BE%E6%96%87%E7%B1%BB%E5%9E%8B%E4%BD%9C%E5%93%81"><span style="color:#be191c;">4、处理图文类型作品</span></h2> 
<p>        对于图文类型，每一张图片都作为 div 元素的背景图片进行展示，图片对应的 URL 在 div 元素的 style 中。 可以先获取到 style 的内容，然后根据圆括号进行分隔，最后得到图片的地址。</p> 
<p>        这里拿到的图片是没有水印的。</p> 
<p><img alt="" height="379" src="https://images2.imgbox.com/a4/63/k0mcYnff_o.png" width="1060"></p> 
<pre><code class="language-python"># 处理图片类型作品的一系列操作
def download_pictures_prepare(res_links, path, date):
    # 下载作品到目录
    index = 0
    for src in res_links:
        download_resource(src, f'{path}/{date}-{index}.webp')
        index += 1

# 处理图片类型的作品
def deal_pictures(work_id):
    # 直接 requests 请求回来，style 是空的，使用 webdriver 获取当前界面的源代码
    temp_driver = webdriver.Chrome()
    temp_driver.set_page_load_timeout(5)
    temp_driver.get(f'https://www.xiaohongshu.com/explore/{work_id}')
    sleep(1)
    try:
        # 如果页面中有 class='feedback-btn' 这个元素，则表示不能正常访问
        temp_driver.find_element(By.CLASS_NAME, 'feedback-btn')
    except NoSuchElementException:        # 没有该元素，则说明能正常访问到作品页面
        WebDriverWait(temp_driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'swiper-wrapper')))
        
        # 获取页面的源代码
        source_code = temp_driver.page_source
        temp_driver.quit()

        html = BeautifulSoup(source_code, 'lxml')
        swiper_sliders = html.find_all(class_='swiper-slide')
        # 当前作品的发表日期
        date = html.find(class_='bottom-container').span.string.split(' ')[0].strip()

        # 图片路径
        res_links = []
        for item in swiper_sliders:
            # 在 style 中提取出图片的 url
            url = item['style'].split('url(')[1].split(')')[0].replace('&amp;quot;', '').replace('"', '')
            if url not in res_links:
                res_links.append(url)

        #为图片集创建目录
        path = f'{ABS_BASE_URL}/{work_id}-pictures'
        try:
            os.makedirs(path)
        except FileExistsError:
            # 目录已经存在，则直接下载到该目录下
            download_pictures_prepare(res_links, path, date)
        except Exception as err:
            print(f'deal_pictures 捕获到其他错误：{err}')
        else:
            download_pictures_prepare(res_links, path, date)
        finally:
            return 1
    except Exception as err:
        print(f'下载图片类型作品 捕获到错误：{err}')
        return 1
    else:
        print(f'访问作品页面被阻断，下次再试')
        return 0</code></pre> 
<p></p> 
<p></p> 
<h2 id="5%E3%80%81%E5%A4%84%E7%90%86%E8%A7%86%E9%A2%91%E7%B1%BB%E5%9E%8B%E4%BD%9C%E5%93%81"><span style="color:#be191c;">5、处理视频类型作品</span></h2> 
<p>        获取到的视频有水印。 </p> 
<p><img alt="" height="230" src="https://images2.imgbox.com/41/dc/NUwA4bUK_o.png" width="1065"></p> 
<pre><code class="language-python"># 处理视频类型的作品
def deal_video(work_id):
    temp_driver = webdriver.Chrome()
    temp_driver.set_page_load_timeout(5)
    temp_driver.get(f'https://www.xiaohongshu.com/explore/{work_id}')
    sleep(1)
    try:
        temp_driver.find_element(By.CLASS_NAME, 'feedback-btn')
    except NoSuchElementException:
        WebDriverWait(temp_driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'player-container')))
        source_code = temp_driver.page_source
        temp_driver.quit()

        html = BeautifulSoup(source_code, 'lxml')
        video_src = html.find(class_='player-el').video['src']
        # 作品发布日期
        date = html.find(class_='bottom-container').span.string.split(' ')[0].strip()

        # 为视频作品创建目录，以 作品的id号 + video 命名目录
        path = f'{ABS_BASE_URL}/{work_id}-video'
        try:
            os.makedirs(path)
        except FileExistsError:
            download_resource(video_src, f'{path}/{date}.mp4')
        except Exception as err:
            print(f'deal_video 捕获到其他错误：{err}')
        else:
            download_resource(video_src, f'{path}/{date}.mp4')
        finally:
            return 1
    except Exception as err:
        print(f'下载视频类型作品 捕获到错误：{err}')
        return 1
    else:
        print(f'访问视频作品界面被阻断，下次再试')
        return 0</code></pre> 
<p></p> 
<p></p> 
<h2 id="6%E3%80%81%E5%BC%82%E5%B8%B8%E8%AE%BF%E9%97%AE%E8%80%8C%E8%A2%AB%E4%B8%AD%E6%96%AD%E7%9A%84%E7%8E%B0%E8%B1%A1"><span style="color:#be191c;">6、异常访问而被中断的现象</span></h2> 
<p>         频繁的访问和下载资源会被重定向到如下的页面，可以通过获取到该页面的特殊标签来判断是否被重定向连接，如果是，则及时中断访问，稍后再继续。</p> 
<p>        使用 webdriver 访问页面，页面打开后，在 try 中查找是否有 class='feedback-btn' 元素（即下方的 我要反馈 的按钮）。如果有该元素，则在 else 中进行提示并返回错误码退出任务。如果找不到元素，则会触发 NoSuchElementException 的错误，在 except 中继续任务即可。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/92/e3/o63n171A_o.png" width="1200"></p> 
<pre><code class="language-python">    try:
        temp_driver.find_element(By.CLASS_NAME, 'feedback-btn')
    except NoSuchElementException:
        # 正常访问到作品页面
        pass
    except Exception as err:
        # 其他的异常
        return 1
    else:
        # 不能访问到作品页面
        return 0</code></pre> 
<p></p> 
<p></p> 
<h2 id="7%E3%80%81%E5%AE%8C%E6%95%B4%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81%C2%A0"><span style="color:#be191c;">7、完整参考代码 </span></h2> 
<pre><code class="language-python">
import json
import threading
import requests,os
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from datetime import datetime
from selenium import webdriver
from time import sleep
from bs4 import BeautifulSoup

# 获取当前时间
def get_current_time():
    now = datetime.now()
    format_time = now.strftime("_%Y-%m-%d__%H-%M-%S-%f__")
    return format_time

# 下载的作品保存的路径，以作者主页的 id 号命名
ABS_BASE_URL = f'G:\\639476c10000000026006023'

# 检查作品是否已经下载过
def check_download_or_not(work_id, is_pictures):
    end_str = 'pictures' if is_pictures else 'video'
    # work_id 是每一个作品的目录，检查目录是否存在并且是否有内容，则能判断对应的作品是否被下载过
    path = f'{ABS_BASE_URL}/{work_id}-{end_str}'
    if os.path.exists(path) and os.path.isdir(path):
        if os.listdir(path):
            return True
    return False

# 下载资源
def download_resource(url, save_path):
    response = requests.get(url, stream=True)
    if response.status_code == 200:
        with open(save_path, 'wb') as file:
            for chunk in response.iter_content(1024):
                file.write(chunk)

# 处理图片类型作品的一系列操作
def download_pictures_prepare(res_links, path, date):
    # 下载作品到目录
    index = 0
    for src in res_links:
        download_resource(src, f'{path}/{date}-{index}.webp')
        index += 1

# 处理图片类型的作品
def deal_pictures(work_id):
    # 直接 requests 请求回来，style 是空的，使用 webdriver 获取当前界面的源代码
    temp_driver = webdriver.Chrome()
    temp_driver.set_page_load_timeout(5)
    temp_driver.get(f'https://www.xiaohongshu.com/explore/{work_id}')
    sleep(1)
    try:
        temp_driver.find_element(By.CLASS_NAME, 'feedback-btn')
    except NoSuchElementException:
        WebDriverWait(temp_driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'swiper-wrapper')))
        source_code = temp_driver.page_source
        temp_driver.quit()

        html = BeautifulSoup(source_code, 'lxml')
        swiper_sliders = html.find_all(class_='swiper-slide')
        # 当前作品的发表日期
        date = html.find(class_='bottom-container').span.string.split(' ')[0].strip()

        # 图片路径
        res_links = []
        for item in swiper_sliders:
            url = item['style'].split('url(')[1].split(')')[0].replace('&amp;quot;', '').replace('"', '')
            if url not in res_links:
                res_links.append(url)

        #为图片集创建目录
        path = f'{ABS_BASE_URL}/{work_id}-pictures'
        try:
            os.makedirs(path)
        except FileExistsError:
            # 目录已经存在，则直接下载到该目录下
            download_pictures_prepare(res_links, path, date)
        except Exception as err:
            print(f'deal_pictures 捕获到其他错误：{err}')
        else:
            download_pictures_prepare(res_links, path, date)
        finally:
            return 1
    except Exception as err:
        print(f'下载图片类型作品 捕获到错误：{err}')
        return 1
    else:
        print(f'访问作品页面被阻断，下次再试')
        return 0


# 处理视频类型的作品
def deal_video(work_id):
    temp_driver = webdriver.Chrome()
    temp_driver.set_page_load_timeout(5)
    temp_driver.get(f'https://www.xiaohongshu.com/explore/{work_id}')
    sleep(1)
    try:
        # 访问不到正常内容的标准元素
        temp_driver.find_element(By.CLASS_NAME, 'feedback-btn')
    except NoSuchElementException:
        WebDriverWait(temp_driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'player-container')))
        source_code = temp_driver.page_source
        temp_driver.quit()

        html = BeautifulSoup(source_code, 'lxml')
        video_src = html.find(class_='player-el').video['src']
        # 作品发布日期
        date = html.find(class_='bottom-container').span.string.split(' ')[0].strip()

        # 为视频作品创建目录
        path = f'{ABS_BASE_URL}/{work_id}-video'
        try:
            os.makedirs(path)
        except FileExistsError:
            download_resource(video_src, f'{path}/{date}.mp4')
        except Exception as err:
            print(f'deal_video 捕获到其他错误：{err}')
        else:
            download_resource(video_src, f'{path}/{date}.mp4')
        finally:
            return 1
    except Exception as err:
        print(f'下载视频类型作品 捕获到错误：{err}')
        return 1
    else:
        print(f'访问视频作品界面被阻断，下次再试')
        return 0

# 检查作品是否已经下载，如果没有下载则去下载
def work_task(href, is_pictures):
    work_id = href.split('/')[-1]
    has_downloaded = check_download_or_not(work_id, is_pictures)
    # 没有下载，则去下载
    if not has_downloaded:
        if not is_pictures:
            res = deal_video(work_id)
        else:
            res = deal_pictures(work_id)
        if res == 0:
            return 0            # 受到阻断
    else:
        print('当前作品已被下载')
        return 2
    return 1

def thread_task(ul):
    for item in ul:
        href = item[0]
        is_pictures = (True if item[1] == 0 else False)
        res = work_task(href, is_pictures)
        if res == 0:        # 被阻止正常访问
            break

if __name__ == '__main__':
    content = ''
    with open('xhs_works.txt', mode='r', encoding='utf-8') as f:
        content = json.load(f)
    url_list = [list(pair) for pair in content.items()]
    length = len(url_list)
    if length &gt; 3:
        ul = [url_list[0: int(length / 3) + 1], url_list[int(length / 3) + 1: int(length / 3) * 2 + 1],
              url_list[int(length / 3) * 2 + 1: length]]
        for child_ul in ul:
            thread = threading.Thread(target=thread_task, args=(child_ul,))
            thread.start()
    else:
        thread_task(url_list)
</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2af009303bbccdda8f9e644a61f3e77d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">每日OJ题_简单多问题dp②_力扣213. 打家劫舍 II</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/62719b830b57f6ff9ed486c8fe24d763/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Spring Boot 获取maven打包时间</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>