<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch之诗词生成3--utils - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch之诗词生成3--utils" />
<meta property="og:description" content="先上代码：
import numpy as np import settings def generate_random_poetry(tokenizer, model, s=&#39;&#39;): &#34;&#34;&#34; 随机生成一首诗 :param tokenizer: 分词器 :param model: 用于生成古诗的模型 :param s: 用于生成古诗的起始字符串，默认为空串 :return: 一个字符串，表示一首古诗 &#34;&#34;&#34; # 将初始字符串转成token token_ids = tokenizer.encode(s) # 去掉结束标记[SEP] token_ids = token_ids[:-1] while len(token_ids) &lt; settings.MAX_LEN: # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布 output = model(np.array([token_ids, ], dtype=np.int32)) _probas = output.numpy()[0, -1, 3:] del output # print(_probas) # 按照出现概率，对所有token倒序排列 p_args = _probas.argsort()[::-1][:100] # 排列后的概率顺序 p = _probas[p_args] # 先对概率归一 p = p / sum(p) # 再按照预测出的概率，随机选择一个词作为预测结果 target_index = np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/7315b18a346699e6c7d5ca41aed89661/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-14T17:25:54+08:00" />
<meta property="article:modified_time" content="2024-03-14T17:25:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch之诗词生成3--utils</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>先上代码：</p> 
<pre><code>
import numpy as np
import settings


def generate_random_poetry(tokenizer, model, s=''):
    """
    随机生成一首诗
    :param tokenizer: 分词器
    :param model: 用于生成古诗的模型
    :param s: 用于生成古诗的起始字符串，默认为空串
    :return: 一个字符串，表示一首古诗
    """
    # 将初始字符串转成token
    token_ids = tokenizer.encode(s)
    # 去掉结束标记[SEP]
    token_ids = token_ids[:-1]
    while len(token_ids) &lt; settings.MAX_LEN:
        # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布
        output = model(np.array([token_ids, ], dtype=np.int32))
        _probas = output.numpy()[0, -1, 3:]
        del output
        # print(_probas)
        # 按照出现概率，对所有token倒序排列
        p_args = _probas.argsort()[::-1][:100]
        # 排列后的概率顺序
        p = _probas[p_args]
        # 先对概率归一
        p = p / sum(p)
        # 再按照预测出的概率，随机选择一个词作为预测结果
        target_index = np.random.choice(len(p), p=p)
        target = p_args[target_index] + 3
        # 保存
        token_ids.append(target)
        if target == 3:
            break
    return tokenizer.decode(token_ids)


def generate_acrostic(tokenizer, model, head):
    """
    随机生成一首藏头诗
    :param tokenizer: 分词器
    :param model: 用于生成古诗的模型
    :param head: 藏头诗的头
    :return: 一个字符串，表示一首古诗
    """
    # 使用空串初始化token_ids，加入[CLS]
    token_ids = tokenizer.encode('')
    token_ids = token_ids[:-1]
    # 标点符号，这里简单的只把逗号和句号作为标点
    punctuations = ['，', '。']
    punctuation_ids = {tokenizer.token_to_id(token) for token in punctuations}
    # 缓存生成的诗的list
    poetry = []
    # 对于藏头诗中的每一个字，都生成一个短句
    for ch in head:
        # 先记录下这个字
        poetry.append(ch)
        # 将藏头诗的字符转成token id
        token_id = tokenizer.token_to_id(ch)
        # 加入到列表中去
        token_ids.append(token_id)
        # 开始生成一个短句
        while True:
            # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布
            output = model(np.array([token_ids, ], dtype=np.int32))
            _probas = output.numpy()[0, -1, 3:]
            del output
            # 按照出现概率，对所有token倒序排列
            p_args = _probas.argsort()[::-1][:100]
            # 排列后的概率顺序
            p = _probas[p_args]
            # 先对概率归一
            p = p / sum(p)
            # 再按照预测出的概率，随机选择一个词作为预测结果
            target_index = np.random.choice(len(p), p=p)
            target = p_args[target_index] + 3
            # 保存
            token_ids.append(target)
            # 只有不是特殊字符时，才保存到poetry里面去
            if target &gt; 3:
                poetry.append(tokenizer.id_to_token(target))
            if target in punctuation_ids:
                break
    return ''.join(poetry)
</code></pre> 
<p>我们首先看第一个函数generate_random_poetry，用于随机生成古诗。</p> 
<pre><code>def generate_random_poetry(tokenizer, model, s=''):
    """
    随机生成一首诗
    :param tokenizer: 分词器
    :param model: 用于生成古诗的模型
    :param s: 用于生成古诗的起始字符串，默认为空串
    :return: 一个字符串，表示一首古诗
    """</code></pre> 
<p> tokenizer是一个分词器，用于将文本转化为模型可以理解的token序列。</p> 
<pre><code># 将初始字符串转成token
    token_ids = tokenizer.encode(s)
    # 去掉结束标记[SEP]
    token_ids = token_ids[:-1]</code></pre> 
<p>是将我们传入的字符串转化为id序列。并使用切片操作切除token序列的最后一个元素。这个元素是用来标记结束标志[SEP]的。</p> 
<p>我们接着看：</p> 
<pre><code> while len(token_ids) &lt; settings.MAX_LEN:
        # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布
        output = model(np.array([token_ids, ], dtype=np.int32))
        _probas = output.numpy()[0, -1, 3:]
        del output</code></pre> 
<p>这里我们就可以看出为什么我们要对最后一个 token进行切片处理了，是为了控制预测的长度。</p> 
<p>之后我们进行预测，通过model模型对当前的token_ids序列进行预测，model接受一个形状为（batch_size,sequence_length）的输入，这里batch_size为1，sequence_length为token_ids的长度。因此，我们将token_ids包装成一个形状为(1,sequence_length)的numpy数组，并将其传递给model进行预测。</p> 
<p>输出的结果是一个3D数组，形状为（1，sequence_length，vocab_size）。其中vocab_size是词汇表，得到的是对下一个词的预测。</p> 
<p>然后通过output.numpy()将输出转化为numpy数组，由于我们只关注当前序列的最后一个token的预测结果，所以使用[0,-1,3:]来获取这部分概率分布。具体的，[0,-1]表示获取第一个样例的最后一个token的预测，而[3:]表示去除前三个token,即[PAD],[UNK]和[CLS]。</p> 
<p>最后，通过del output释放output变量所占用的内存空间，这样做是为了及时释放内存，避免内存占用过多。</p> 
<p>解释一下四个特殊的token在自然语言处理任务中的不同作用。</p> 
<ul><li>[PAD]（填充）：在序列中用于填充长度不足的部分，使得序列长度统一，在很多深度学习模型中，要求输入的序列长度是相同的，因此当序列长度不足时，可以使用[PAD]进行填充，使得序列达到统一的长度。</li><li>[UNK]（未知）：用于表示模型未见过的或者无法识别的词汇，当模型在处理文本时遇到不在词汇表中的词汇，就会用[UNK]表示，这样可以避免模型对于未知词汇的处理错误。</li><li>[CLS]（分类）：在自然语言处理中的一些任务中，例如文本分类，文本语义相似度等，[CLS]被用作特殊的起始标记。模型会将[CLS]作为整个序列的表示，并用于后续任务的处理。例如：对于文本分类任务，模型可以根据[CLS]表示进行预测分类标签。</li><li>[SEP]（分隔符）：用于表示序列的分隔，常见于处理多个句子或多个文本之间的任务。他可以用来分隔句子，句子对，文本片段等，在一些任务中，如文本对分类，问答系统等，输入可能包含多个句子或文本片段，模型可以通过识别[SEP]来理解不同句子之间的关系。另外，[SEP]还可以用于标记序列的结束，在一些 情况下，序列的长度是固定的，使用[SEP]标记还可以表示序列的结束。</li></ul> 
<p></p> 
<p>继续看我们的代码：</p> 
<pre><code> # 按照出现概率，对所有token倒序排列
        p_args = _probas.argsort()[::-1][:100]
        # 排列后的概率顺序
        p = _probas[p_args]
        # 先对概率归一
        p = p / sum(p)
        # 再按照预测出的概率，随机选择一个词作为预测结果
        target_index = np.random.choice(len(p), p=p)
        target = p_args[target_index] + 3
        # 保存
        token_ids.append(target)</code></pre> 
<p> 首先，通过argsort()函数对得到的所有outputs进行从小到大的排序，然后经过[::-1]将排序结果进行反转，这样就变成了从大到小的排序，[:100]表示只选择排名前100的token。返回的是_probas中元素从小到大排序的索引数组。</p> 
<p>得到索引数组之后，我们通过p=_probas[p_args]，根据排名靠前的token的索引p_args，从预测的概率分布_probas中获取对应的概率值，这样得到的p就是排列后的概率顺序（概率值从大到小）。</p> 
<p>之后对概率进行归一化，将概率值除以概率之和，使其概率和为1。</p> 
<p>target_index = np.random.choice(len(p), p=p)根据参数p（给定的概率分布）在长度为len(p)的范围内进行随机选择，并返回选择的索引target_index。（显然，元素对应的p值越大，被选择的概率就越高）。</p> 
<p>后面的target = p_args[target_index] + 3，其中+3是必不可少的，我刚刚因为没有+3就运行报错了，因为我们得到的target是对应于token_ids的索引，而token_ids的前三项是特殊字符，为了和实际中的模型相对应，我们需要将预测的token编号做一些偏离处理。</p> 
<p>最后我们将我们的预测列表中加上我们预测的词的索引值，也就是target。</p> 
<p>看着一段：</p> 
<pre> if target == 3:
        break
return tokenizer.decode(token_ids)</pre> 
<p>如果生成的target的值是3，表示生成的token是特殊的[CLS]标记，这意味着生成的序列已结束。<br> 返回我们解码之后的结果也就是生成的诗词。等于3的情况是存在的，3意味着结束标志SPE，当句子的长度合适时，其预测的下一个词很可能是3，我们规定3也属于可预测范围。</p> 
<p>看生成藏头诗的代码：</p> 
<pre>def generate_acrostic(tokenizer, model, head):
    """
    随机生成一首藏头诗
    :param tokenizer: 分词器
    :param model: 用于生成古诗的模型
    :param head: 藏头诗的头
    :return: 一个字符串，表示一首古诗
    """
    # 使用空串初始化token_ids，加入[CLS]
    token_ids = tokenizer.encode('')
    token_ids = token_ids[:-1]
    # 标点符号，这里简单的只把逗号和句号作为标点
    punctuations = ['，', '。']
    punctuation_ids = {tokenizer.token_to_id(token) for token in punctuations}
    # 缓存生成的诗的list
    poetry = []
    # 对于藏头诗中的每一个字，都生成一个短句
    for ch in head:
        # 先记录下这个字
        poetry.append(ch)
        # 将藏头诗的字符转成token id
        token_id = tokenizer.token_to_id(ch)
        # 加入到列表中去
        token_ids.append(token_id)
        # 开始生成一个短句
        while True:
            # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布
            output = model(np.array([token_ids, ], dtype=np.int32))
            _probas = output.numpy()[0, -1, 3:]
            del output
            # 按照出现概率，对所有token倒序排列
            p_args = _probas.argsort()[::-1][:100]
            # 排列后的概率顺序
            p = _probas[p_args]
            # 先对概率归一
            p = p / sum(p)
            # 再按照预测出的概率，随机选择一个词作为预测结果
            target_index = np.random.choice(len(p), p=p)
            target = p_args[target_index] + 3
            # 保存
            token_ids.append(target)
            # 只有不是特殊字符时，才保存到poetry里面去
            if target &gt; 3:
                poetry.append(tokenizer.id_to_token(target))
            if target in punctuation_ids:
                break
    return ''.join(poetry)</pre> 
<p>我们看一下tokenizer.id_to_token(3,2,1,0)<img alt="" height="213" src="https://images2.imgbox.com/c1/3e/etxW5Zhx_o.png" width="811"></p> 
<p>这段代码和上面的随机生成古诗类似，不同的是：</p> 
<p>上面的是完整生成一首古诗，其中就算遇到“，”或者“。”不会中断，直到遇到结束符号[SEP]才会中止。</p> 
<p>而我们生成的藏头诗是对每一个字进行生成，这里遇到“，”，“。”就进行下一个字的继续生成。然后将每个字的自动生成进行拼接。得到完整的诗句。（当然，遇到[SEP]也是进行终止的，保证了句式的协调）。</p> 
<pre>return ''.join(poetry)</pre> 
<p>作用是将我们的peotry中的内容（短句），连接成一个字符串。并将其作为函数值返回。</p> 
<p>如果不加join函数的话，输出是一个列表：<img alt="" height="354" src="https://images2.imgbox.com/97/cc/NbCVP4ns_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/51264bc1d901eb54c8be95fa6627dadf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux信号灯</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/90dfd4f1d9070bce7876961d322c091e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Hadoop大数据应用：HDFS 集群节点缩容</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>