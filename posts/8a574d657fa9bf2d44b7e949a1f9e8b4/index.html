<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Transformer——词向量 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Transformer——词向量" />
<meta property="og:description" content="词向量 在自然语言处理任务中，模型的输入大多为单个字或者词。但是字词都是自然语言的表述，对于以二进制为处理语言的计算机来说，其并不认识这个字词。所以需要将字词转换为计算机认识的数据。
转换的方法有很多，我们接下来将介绍其中最简单的一种：独热编码，这是一种将字词转换为计算机可以识别的数字向量的方式。
注：为什么不可以使用Unicode编码？其不也是唯一的吗？
Unicode编码是表示单个字的编码方式，但是在预模型中，很多词向量表示的是一个词，比如：“我” “有” “一只” “猫”
其中“一只”是一个词，其如果用Unicode编码表示，其长度维度和其他字词不一致。
One-Hot 独热编码 独热编码是一种将离散型（可以划分为n个类别）的数据转化为计算机能够理解的方式，其基本思想是在 n n n维向量空间中，使用 n n n个单位向量，每个单位向量代表一个类别。也就是说，如果我们有 n n n个类别，那么我们就会有一个长度为 n n n的向量，其中有一个元素为1，其余元素为0。这个1的位置就代表了这个类别在向量中的位置。
下面是“我” “有” “一只” “猫”的独热编码表示：
词语我有一只猫我1000有0100一只0010猫0001 独热编码虽然可以表示一个字词，但是假如说我的词库非常大。为了让每一个字词都在其中有着唯一的表示，我每个字词的表示就都是 n n n维的，这会带来极大的存储成本。同时，在独热编码中，每个单词都是由一个完全不同的向量表示的。这意味着相似的单词，其向量表示完全不同。这就导致我们在计算独热编码两两的相似度时，得到的欧氏距离都是相同的。
注1：词库：
也称为词汇表或字典，是用于自然语言处理（NLP）的基本工具之一。它是一个包含所有可能单词的集合，每个单词都有一个与之对应的索引。
注2：总结：独热编码在高纬度计算和语义信息表示上面有缺陷。
为了解决这个问题，我们需要引入一个新概念：词向量
词向量 词向量是一种用来表示单词的向量，它比独热编码更高级。词向量的长度通常比词库的大小要小得多，例如，我们可以用一个200维的向量来表示所有的单词，而不是使用一个100,000维的向量。此外，词向量可以捕捉到单词之间的语义关系，例如，“猫”和“狗”的词向量可能在向量空间中非常接近。
注：词向量的语义关系捕获
在自然语言处理（NLP）中，常见的词向量训练方法有Word2Vec、GloVe和FastText等。
在Word2Vec算法中，它通过学习预测上下文，使得语义相近的词在向量空间中靠得更近。GloVe算法则是通过利用全局统计信息（即词共现矩阵）来生成词向量。这种方法可以捕获到更丰富的词语共现信息，因此可以更好地表达词与词之间的关系。FastText算法则是通过考虑词的上下文信息以及词内部的字母级信息，从而更好地处理形态丰富的语言，以及处理词典中没有的词。 词向量是独热编码的一种改进和优化，其可以由独热编码 w x w_x wx​乘以权重矩阵 Q Q Q得到，公式如下：
w x ∗ Q = c x ( 词向量 ) w_x*Q=c_x(词向量) wx​∗Q=cx​(词向量)
上面的例子，可能会得到如下的词向量表示：
词语维度1维度2维度3我0.10.30.2有0.20.40.1一只0.40.10.3猫0.30.20.4 注：权重矩阵Q
权重矩阵Q是通过模型训练过程中的优化算法得到的。 以深度学习模型为例，权重矩阵Q是模型中的参数，通过反向传播和梯度下降等优化算法，不断调整这些权重，使得模型在训练数据上的预测误差最小。
word2vec Word2Vec是Google于2013年提出的一种用于生成词向量的两层神经网络模型。它的目标是根据给定的语境预测单词或根据单词预测语境。其主要用CBOW（Continuous Bag of Words）和Skip-Gram模型来做预测语境。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/8a574d657fa9bf2d44b7e949a1f9e8b4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-05T20:27:30+08:00" />
<meta property="article:modified_time" content="2024-03-05T20:27:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Transformer——词向量</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>词向量</h3> 
<p>在自然语言处理任务中，模型的输入大多为单个字或者词。但是字词都是自然语言的表述，对于以二进制为处理语言的计算机来说，其并不认识这个字词。<strong>所以需要将字词转换为计算机认识的数据</strong>。</p> 
<p>转换的方法有很多，我们接下来将介绍其中最简单的一种：<strong>独热编码</strong>，这是一种将字词转换为计算机可以识别的数字向量的方式。</p> 
<blockquote> 
 <p>注：为什么不可以使用Unicode编码？其不也是唯一的吗？<br> Unicode编码是表示单个字的编码方式，但是在预模型中，很多词向量表示的是一个词，比如：“我” “有” “一只” “猫”<br> 其中“一只”是一个词，其如果用Unicode编码表示，其长度维度和其他字词不一致。</p> 
</blockquote> 
<h4><a id="OneHot__8"></a>One-Hot 独热编码</h4> 
<p>独热编码是一种将离散型（可以划分为n个类别）的数据转化为计算机能够理解的方式，其基本思想是在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         n 
        
       
      
        n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>维向量空间中，使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         n 
        
       
      
        n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>个单位向量，每个单位向量代表一个类别。也就是说，如果我们有<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         n 
        
       
      
        n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>个类别，那么我们就会有一个长度为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         n 
        
       
      
        n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>的向量，其中有一个元素为1，其余元素为0。这个1的位置就代表了这个类别在向量中的位置。</p> 
<p>下面是“我” “有” “一只” “猫”的独热编码表示：</p> 
<table><thead><tr><th align="center">词语</th><th align="center">我</th><th align="center">有</th><th align="center">一只</th><th align="center">猫</th></tr></thead><tbody><tr><td align="center">我</td><td align="center">1</td><td align="center">0</td><td align="center">0</td><td align="center">0</td></tr><tr><td align="center">有</td><td align="center">0</td><td align="center">1</td><td align="center">0</td><td align="center">0</td></tr><tr><td align="center">一只</td><td align="center">0</td><td align="center">0</td><td align="center">1</td><td align="center">0</td></tr><tr><td align="center">猫</td><td align="center">0</td><td align="center">0</td><td align="center">0</td><td align="center">1</td></tr></tbody></table> 
<p>独热编码虽然可以表示一个字词，但是假如说我的词库非常大。为了让每一个字词都在其中有着<strong>唯一</strong>的表示，我每个字词的表示就都是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         n 
        
       
      
        n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>维的，这会带来极大的存储成本。同时，在独热编码中，每个单词都是由一个完全不同的向量表示的。这意味着相似的单词，其向量表示完全不同。这就导致我们在计算独热编码两两的相似度时，得到的欧氏距离都是相同的。</p> 
<blockquote> 
 <p>注1：词库：<br> 也称为词汇表或字典，是用于自然语言处理（NLP）的基本工具之一。它是一个包含所有可能单词的集合，每个单词都有一个与之对应的索引。</p> 
</blockquote> 
<blockquote> 
 <p>注2：总结：独热编码在<strong>高纬度计算</strong>和<strong>语义信息表示</strong>上面有缺陷。</p> 
</blockquote> 
<p>为了解决这个问题，我们需要引入一个新概念：<strong>词向量</strong></p> 
<h4><a id="_26"></a>词向量</h4> 
<p>词向量是一种用来表示单词的向量，它比独热编码更高级。词向量的长度通常比词库的大小要小得多，例如，我们可以用一个200维的向量来表示所有的单词，而不是使用一个100,000维的向量。此外，词向量可以捕捉到单词之间的语义关系，例如，“猫”和“狗”的词向量可能在向量空间中非常接近。</p> 
<blockquote> 
 <p>注：词向量的语义关系捕获<br> 在自然语言处理（NLP）中，常见的词向量训练方法有Word2Vec、GloVe和FastText等。</p> 
 <ul><li>在Word2Vec算法中，它通过学习预测上下文，使得语义相近的词在向量空间中靠得更近。</li><li>GloVe算法则是通过利用全局统计信息（即词共现矩阵）来生成词向量。这种方法可以捕获到更丰富的词语共现信息，因此可以更好地表达词与词之间的关系。</li><li>FastText算法则是通过考虑词的上下文信息以及词内部的字母级信息，从而更好地处理形态丰富的语言，以及处理词典中没有的词。</li></ul> 
</blockquote> 
<p>词向量是独热编码的一种改进和优化，其可以由独热编码<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          w 
         
        
          x 
         
        
       
      
        w_x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.0269em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>乘以权重矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Q 
        
       
      
        Q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span>得到，公式如下：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           w 
          
         
           x 
          
         
        
          ∗ 
         
        
          Q 
         
        
          = 
         
         
         
           c 
          
         
           x 
          
         
        
          ( 
         
        
          词向量 
         
        
          ) 
         
        
       
         w_x*Q=c_x(词向量) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6153em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.0269em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord cjk_fallback">词向量</span><span class="mclose">)</span></span></span></span></span></span><br> 上面的例子，可能会得到如下的词向量表示：</p> 
<table><thead><tr><th>词语</th><th>维度1</th><th>维度2</th><th>维度3</th></tr></thead><tbody><tr><td>我</td><td>0.1</td><td>0.3</td><td>0.2</td></tr><tr><td>有</td><td>0.2</td><td>0.4</td><td>0.1</td></tr><tr><td>一只</td><td>0.4</td><td>0.1</td><td>0.3</td></tr><tr><td>猫</td><td>0.3</td><td>0.2</td><td>0.4</td></tr></tbody></table> 
<blockquote> 
 <p>注：权重矩阵Q<br> 权重矩阵Q是通过模型训练过程中的优化算法得到的。 以深度学习模型为例，权重矩阵Q是模型中的参数，通过反向传播和梯度下降等优化算法，不断调整这些权重，使得模型在训练数据上的预测误差最小。</p> 
</blockquote> 
<h5><a id="word2vec_47"></a>word2vec</h5> 
<p>Word2Vec是Google于2013年提出的一种用于生成词向量的两层神经网络模型。它的目标是根据给定的语境预测单词或根据单词预测语境。其主要用<strong>CBOW（Continuous Bag of Words）<strong>和</strong>Skip-Gram</strong>模型来做预测语境。</p> 
<ul><li><strong>Skip-Gram</strong>：输入是一个词，输出是该词周围的一些词。模型的目标是最大化给定单词的情况下，其上下文词出现的概率。适用于处理大型语料库，因为它对罕见词的处理效果比较好。</li><li><strong>CBOW</strong>：和Skip-Gram模型刚好相反，输入是某个词的上下文，输出是这个词。模型的目标是最大化给定上下文的情况下，中心词出现的概率。这种模型训练速度更快，但对罕见词的处理效果不如Skip-Gram。</li></ul> 
<p>假设我们有一个句子：“我有一只猫”，我们使用Skip-Gram模型，并设定窗口大小为2，那么对于每个词，我们都会考虑它前后各两个词。<br> 以词"一只"为例，它前面两个词是"我"和"有"，后面两个词是"猫"。那么，我们的训练样本就是（“一只”，“我”），（“一只”，“有”），（“一只”，“猫”）。在训练过程中，我们的模型需要学习到这样的信息：当"一只"出现的时候，“我”，"有"和"猫"是可能出现在它周围的词。</p> 
<p></p> 
<div class="inscode-box">
 <iframe width="100%" height="500px" src="https://inscode.csdn.net/@shenmingxueit/Python/embed" scrolling="no" frameborder="no" allowfullscreen="allowfullscreen" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals"></iframe>
</div> 
<p></p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> Word2Vec

<span class="token comment"># 训练文本</span>
sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">"我"</span><span class="token punctuation">,</span> <span class="token string">"有"</span><span class="token punctuation">,</span> <span class="token string">"一只"</span><span class="token punctuation">,</span> <span class="token string">"猫"</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

<span class="token comment"># 训练模型</span>
model <span class="token operator">=</span> Word2Vec<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> window<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># 打印"一只"的词向量</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span><span class="token string">"一只"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>注：我们的例子只有一个很短的句子，在实际应用中，Word2Vec通常需要在大量文本数据上进行训练，才能得到有用的词向量。</p> 
</blockquote> 
<h5><a id="_69"></a>词向量的局限性</h5> 
<p>词向量虽然能够捕获词与词间的语义关系，但也存在一些局限性。例如，传统的词向量模型无法处理一词多义的问题，因为它们都是为每个词分配一个固定的向量。然而，最新的预训练模型（如BERT、GPT等）通过引入动态词向量的概念，成功解决了这个问题。在这些模型中，一个词的向量表示会随着其上下文的变化而变化，从而能够捕获到词的多义性。</p> 
<h2><a id="_71"></a>参考</h2> 
<ol><li>chat-gpt4</li><li><a href="https://www.cnblogs.com/nickchen121/p/16470583.html" rel="nofollow">预训练语言模型的前世今生</a></li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5590faecf65f06ea2c00574e875cf66f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">jetbrains历史版本下载地址</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3b5cc55f84bec8a6cbfa8370d5aef966/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">论文翻译 - Are aligned neural networks adversarially aligned？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>