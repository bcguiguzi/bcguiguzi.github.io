<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于LangChain的LLM应用开发3——记忆 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于LangChain的LLM应用开发3——记忆" />
<meta property="og:description" content="此情可待成追忆，只是当时已惘然。我们人类会有很多或美好或痛苦的回忆，有的回忆会渐渐模糊，有的回忆午夜梦醒，会浮上心头。
然而现在的大语言模型都是没有记忆的，都是无状态的，大语言模型自身不会记住和你对话之间的历史消息。根本用不着“时时勤拂拭”，天然就是“本来无一物”。每一次的请求交互、api调用都是独立的，完全没有关联。那些聊天机器人看起来有记忆，是因为借助代码的帮助，提供历史消息作为和LLM对话的上下文。嗯，就跟我们大脑不太够用了，要拿小本本或者打开Obsidian/Notion/语雀……来查找一样。（你去拜访某些单位，还可以看到前台拿着一本已经翻到包浆的小本子来查电话。）
所以，现在的大语言模型，就跟福尔摩斯一样，可能作为推理引擎更加好用：只要提供足够的上下文信息，那么即使坐在家中，也比愚蠢的苏格兰警探更清楚案情。（可以考虑打造一个叫“夏洛克”的大语言模型？ ）运筹帷幄之中，决胜千里之外。
本节我们就来看一下LangChain提供的4种Memory（记忆）组件（Vector data memory和Entity memory不展开），每种组件都有其适用场景。
主要的记忆组件 ConversationBufferMemory 这个记忆组件允许储存对话的消息，并且可以把消息抽取到一个变量。
ConversationBufferWindowMemory 这个记忆会保持K轮对话的列表。只保存最近的K轮对话。旧对话会清除。
ConversationTokenBufferMemory 这个记忆组件跟ConversationBufferWindowMemory差不多，同样把旧对话清除，只是是按Token的长度限制。
ConversationSummaryMemory 这个记忆组件会调用大语言模型，对旧的会话进行总结。
Vector data memory 这个组件把文本（来自会话或者其他地方的）保存到向量数据库，检索最相关的文本块。
Entity memories 调用LLM，记住关于特定实体的细节信息。
可以同时使用多个记忆组件，如调用会话记忆&#43;实体记忆来检索个人信息。还可以将会话内容保存到传统数据库（如键值存储Redis或者关系数据库mysql等等），应用要落地这个是必不可少的。
下面来具体看每个组件的例子。
同样是先通过.env文件初始化环境，具体操作参考上一篇。
import os from dotenv import load_dotenv, find_dotenv _ = load_dotenv(find_dotenv()) # read local .env file import warnings warnings.filterwarnings(&#39;ignore&#39;) deployment = &#34;gpt-35-turbo&#34; model = &#34;gpt-3.5-turbo&#34; ConversationBufferMemory # from langchain.chat_models import ChatOpenAI from langchain.chat_models import AzureChatOpenAI from langchain.chains import ConversationChain from langchain.memory import ConversationBufferMemory llm = AzureChatOpenAI(temperature=0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/ec380c56b58944ae4c2e41e6bf205d5f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-22T16:52:32+08:00" />
<meta property="article:modified_time" content="2023-10-22T16:52:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于LangChain的LLM应用开发3——记忆</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>此情可待成追忆，只是当时已惘然。我们人类会有很多或美好或痛苦的回忆，有的回忆会渐渐模糊，有的回忆午夜梦醒，会浮上心头。</p> 
<p>然而现在的大语言模型都是没有记忆的，都是无状态的，大语言模型自身不会记住和你对话之间的历史消息。根本用不着“时时勤拂拭”，天然就是“本来无一物”。每一次的请求交互、api调用都是独立的，完全没有关联。那些聊天机器人看起来有记忆，是因为借助代码的帮助，提供历史消息作为和LLM对话的上下文。嗯，就跟我们大脑不太够用了，要拿小本本或者打开Obsidian/Notion/语雀……来查找一样。（你去拜访某些单位，还可以看到前台拿着一本已经翻到包浆的小本子来查电话。）</p> 
<p>所以，现在的大语言模型，就跟福尔摩斯一样，可能作为推理引擎更加好用：只要提供足够的上下文信息，那么即使坐在家中，也比愚蠢的苏格兰警探更清楚案情。（可以考虑打造一个叫“夏洛克”的大语言模型？ ）运筹帷幄之中，决胜千里之外。</p> 
<p>本节我们就来看一下LangChain提供的4种Memory（记忆）组件（Vector data memory和Entity memory不展开），每种组件都有其适用场景。</p> 
<h3><a id="_9"></a>主要的记忆组件</h3> 
<ul><li>ConversationBufferMemory</li></ul> 
<p>这个记忆组件允许储存对话的消息，并且可以把消息抽取到一个变量。</p> 
<ul><li>ConversationBufferWindowMemory</li></ul> 
<p>这个记忆会保持K轮对话的列表。只保存最近的K轮对话。旧对话会清除。</p> 
<ul><li>ConversationTokenBufferMemory</li></ul> 
<p>这个记忆组件跟ConversationBufferWindowMemory差不多，同样把旧对话清除，只是是按Token的长度限制。</p> 
<ul><li>ConversationSummaryMemory</li></ul> 
<p>这个记忆组件会调用大语言模型，对旧的会话进行总结。</p> 
<ul><li>Vector data memory</li></ul> 
<p>这个组件把文本（来自会话或者其他地方的）保存到向量数据库，检索最相关的文本块。</p> 
<ul><li>Entity memories</li></ul> 
<p>调用LLM，记住关于特定实体的细节信息。</p> 
<p>可以同时使用多个记忆组件，如调用会话记忆+实体记忆来检索个人信息。还可以将会话内容保存到传统数据库（如键值存储Redis或者关系数据库mysql等等），应用要落地这个是必不可少的。</p> 
<p>下面来具体看每个组件的例子。</p> 
<p>同样是先通过.env文件初始化环境，具体操作参考上一篇。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os

<span class="token keyword">from</span> dotenv <span class="token keyword">import</span> load_dotenv<span class="token punctuation">,</span> find_dotenv
_ <span class="token operator">=</span> load_dotenv<span class="token punctuation">(</span>find_dotenv<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># read local .env file</span>

<span class="token keyword">import</span> warnings
warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">'ignore'</span><span class="token punctuation">)</span>

deployment <span class="token operator">=</span> <span class="token string">"gpt-35-turbo"</span>
model <span class="token operator">=</span> <span class="token string">"gpt-3.5-turbo"</span>
</code></pre> 
<h3><a id="ConversationBufferMemory_54"></a><strong>ConversationBufferMemory</strong></h3> 
<pre><code class="prism language-python"><span class="token comment"># from langchain.chat_models import ChatOpenAI</span>
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chat_models <span class="token keyword">import</span> AzureChatOpenAI
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> ConversationChain
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferMemory

llm <span class="token operator">=</span> AzureChatOpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> model_name<span class="token operator">=</span>model<span class="token punctuation">,</span> deployment_name<span class="token operator">=</span>deployment<span class="token punctuation">)</span>
memory <span class="token operator">=</span> ConversationBufferMemory<span class="token punctuation">(</span><span class="token punctuation">)</span>
conversation <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>
    llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> 
    memory <span class="token operator">=</span> memory<span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token boolean">True</span> <span class="token comment">#设置为True，可以看到对话的详细过程</span>
<span class="token punctuation">)</span>
conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"你好，我是西滨。"</span><span class="token punctuation">)</span>
conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"1+1等于多少?"</span><span class="token punctuation">)</span>
conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"你还记得我的名字?"</span><span class="token punctuation">)</span>
</code></pre> 
<p>这里会创建ConversationChain，Chain是LangChain的核心概念，后面会详细讲述，这里先不管。memory = ConversationBufferMemory() 创建一个ConversationBufferMemory传给ConversationChain，我们打开verbose，看一下具体的输出。<br> <img src="https://images2.imgbox.com/6b/cc/IOQzXJcw_o.png" alt="在这里插入图片描述"></p> 
<p>一开始LangChain自动发送一段提示（***The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.）***过去开始对话，后面我们可以看到，每次对话的信息都会自动发过去，经过一轮对话之后，再问Ai“你还记得我的名字?”，Ai毫不犹豫的回答：“当然记得！你是西滨。”</p> 
<p>上面我们用memory这个变量来保存记忆，如果输出memory.buffer，可以看到对话的所有消息：</p> 
<p>Human: 你好，我是西滨。<br> AI: 你好，西滨！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？<br> Human: 1+1等于多少?<br> AI: 1+1等于2。<br> Human: 你还记得我的名字?<br> AI: 当然记得！你是西滨。</p> 
<p>可以手工调用save_context方法来把上下文信息传进去：</p> 
<pre><code>memory = ConversationBufferMemory()
memory.save_context({"input": "Hi"}, 
                    {"output": "What's up"})
memory.save_context({"input": "Not much, just hanging"}, 
                    {"output": "Cool"})
memory.load_memory_variables({})
</code></pre> 
<p>调用load_memory_variables({})来查看对应的记忆内容。（load_memory_variables中的花括号{}是一个空词典，可以在这里传递额外的参数进行高级定制）<br> {‘history’: “Human: Hi\nAI: What’s up\nHuman: Not much, just hanging\nAI: Cool”}</p> 
<p>ConversationBufferMemory可以存储到目前为止的对话消息，看起来很完美，但是随着对话越来越长，所需的记忆存储量也变得非常大，而向LLM发送大量Token的成本也会增加（现在大模型一般按照Token的数量收费，而且还是双向收费，你懂的）。</p> 
<p>解决这个问题，LangChain有三个不同的记忆组件来处理。</p> 
<h3><a id="ConversationBufferWindowMemory_105"></a>ConversationBufferWindowMemory</h3> 
<p>ConversationBufferWindowMemory 只保留一个窗口的记忆，也就是只保留最后若干轮对话消息。注意，这个跟微软的Bing Chat不太一样，微软是每个话题保留30轮，30轮对话一到，自动转向新话题；ConversationBufferWindowMemory的策略就是计算机算法典型的“滑动窗口”，永远都是保留最新的若干轮对话。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferWindowMemory
memory <span class="token operator">=</span> ConversationBufferWindowMemory<span class="token punctuation">(</span>k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>               
memory<span class="token punctuation">.</span>save_context<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">"Hi"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                    <span class="token punctuation">{<!-- --></span><span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"What's up"</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
memory<span class="token punctuation">.</span>save_context<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">"Not much, just hanging"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                    <span class="token punctuation">{<!-- --></span><span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"Cool"</span><span class="token punctuation">}</span><span class="token punctuation">)</span>

memory<span class="token punctuation">.</span>load_memory_variables<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>上面的k设为了1，那只保留最后1轮对话，对话信息就只剩下：<br> {‘history’: ‘Human: Not much, just hanging\nAI: Cool’}<br> 前面的”Human:Hi?\nAI: What’s up“ 已经去掉了。</p> 
<p>实际应用，我们会更多的采用ConversationBufferWindowMemory（K通常会设得比较大，需要根据具体情景调整），而不是ConversationBufferMemory，可以防止记忆存储量随着对话的进行而无限增长，同时也有比较好的效果。。</p> 
<h3><a id="ConversationTokenBufferMemory_126"></a>ConversationTokenBufferMemory</h3> 
<p>ConversationTokenBufferMemory通过另一种方式来解决记忆存储量增长的问题：限制保存在记忆的令牌数量。</p> 
<p>要先安装tiktoken，底层用于计算Token数目：<br> !pip install tiktoken</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationTokenBufferMemory
llm <span class="token operator">=</span> AzureChatOpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> model_name<span class="token operator">=</span>model<span class="token punctuation">,</span> deployment_name<span class="token operator">=</span>deployment<span class="token punctuation">)</span>
memory <span class="token operator">=</span> ConversationTokenBufferMemory<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> max_token_limit<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">)</span>
memory<span class="token punctuation">.</span>save_context<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">"AI is what?!"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                    <span class="token punctuation">{<!-- --></span><span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"Amazing!"</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
memory<span class="token punctuation">.</span>save_context<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">"Backpropagation is what?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                    <span class="token punctuation">{<!-- --></span><span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"Beautiful!"</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
memory<span class="token punctuation">.</span>save_context<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">"Chatbots are what?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">{<!-- --></span><span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"Charming!"</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
memory<span class="token punctuation">.</span>load_memory_variables<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>注意上面token的限制设为了30，则最终保留下来的消息只有这些，保证总的消息内容长度不超过设置的令牌限制值max_token_limit。（这里涉及到计算Token的算法，不能按字符数计算。每种LLM计算Token的算法都不一样，所以调用ConversationTokenBufferMemory要把llm传进去。）：<br> {‘history’: ‘AI: Beautiful!\nHuman: Chatbots are what?\nAI: Charming!’}</p> 
<p>显然，没有保留最近两轮完整的对话消息，所以这个组件的效果可能没有ConversationBufferWindowMemory好，但是调用Api的性价比高一点。</p> 
<h3><a id="ConversationSummaryMemory_151"></a>ConversationSummaryMemory</h3> 
<p>ConversationSummaryMemory可以算是ConversationTokenBufferMemory的变体，同样是按令牌数限制，但是当它发现令牌数超了，不是把旧的消息丢掉，而是把当前所有的消息进行摘要，直到摘要的文本令牌数不超过设置的限制。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationSummaryBufferMemory
<span class="token comment"># create a long string</span>
schedule <span class="token operator">=</span> <span class="token triple-quoted-string string">"""There is a meeting at 8am with your product team. 
You will need your powerpoint presentation prepared. 
9am-12pm have time to work on your LangChain 
project which will go quickly because Langchain is such a powerful tool. 
At Noon, lunch at the italian resturant with a customer who is driving 
from over an hour away to meet you to understand the latest in AI. 
Be sure to bring your laptop to show the latest LLM demo."""</span>

memory <span class="token operator">=</span> ConversationSummaryBufferMemory<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> max_token_limit<span class="token operator">=</span><span class="token number">400</span><span class="token punctuation">)</span>
memory<span class="token punctuation">.</span>save_context<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">"Hello"</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"What's up"</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
memory<span class="token punctuation">.</span>save_context<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">"Not much, just hanging"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                    <span class="token punctuation">{<!-- --></span><span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"Cool"</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
memory<span class="token punctuation">.</span>save_context<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">"What is on the schedule today?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">{<!-- --></span><span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>schedule<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">}</span><span class="token punctuation">)</span>
memory<span class="token punctuation">.</span>load_memory_variables<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>如果max_token_limit设置为400，因为400个令牌足以存储所有的文本，可以看到ConversationSummaryMemory没有做任何的动作，把所有的消息都原样保存：<br> {‘history’: “Human: Hello\nAI: What’s up\nHuman: Not much, just hanging\nAI: Cool\nHuman: What is on the schedule today?\nAI: There is a meeting at 8am with your product team. \nYou will need your powerpoint presentation prepared. \n9am-12pm have time to work on your LangChain \nproject which will go quickly because Langchain is such a powerful tool. \nAt Noon, lunch at the italian resturant with a customer who is driving \nfrom over an hour away to meet you to understand the latest in AI. \nBe sure to bring your laptop to show the latest LLM demo.”}</p> 
<p>但是如果把max_token_limit设置为100，ConversationSummaryMemory会调用LLM，把消息保存为下面的摘要：</p> 
<p>{‘history’: ‘System: The human and AI exchange greetings. The human mentions that they are not doing much and the AI responds with a casual remark. The human then asks about their schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, working on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.’}</p> 
<p>如果是问没那么精确的问题，LLM仍然可以回答：</p> 
<pre><code class="prism language-python">conversation <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>
    llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> 
    memory <span class="token operator">=</span> memory<span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token boolean">False</span>
<span class="token punctuation">)</span>
conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"What would be a good demo to show?"</span><span class="token punctuation">)</span>
</code></pre> 
<p>'A good demo to show would be the latest Language Learning Model (LLM) demo. It showcases real-time translations, pronunciation feedback, grammar suggestions, interactive exercises, and quizzes. These features make it an ideal way to highlight the capabilities of our AI technology in the education and language learning industry.’</p> 
<p>但是再问具体的信息，由于摘要已经丢失了详细信息，所以LLM开始胡说八道。</p> 
<p><code>conversation.predict(**input="When will the meeting hold today?"**)</code></p> 
<p>'The meeting with the product team is scheduled for 10:00 AM today.’</p> 
<h3><a id="_201"></a>展望</h3> 
<p>虽然LangChain提供了不同的记忆组件，但是真正用起来还是有点麻烦，好消息是根据路透社报道，11月6日在OpenAI的开发者大会上，ChatGPT将推出带有记忆能力的大模型，也就是有状态的API接口。[3]</p> 
<p>和大模型一次对话的内容量称之为Context，是一个很重要的指标。Context越大，意味着你可以和大模型对话的次数越多，传递的信息越多，那么大模型反馈给你的结果才会更加准确。如果Context不够大，那么你只能抛弃一些信息，自然拿到的结果就会产生偏差。</p> 
<p>现在GPT-4默认的Context是8K，如果要支持32K的Context，则价格直接翻倍。Claude大模型支持的Context更大，可以支持100K的Context，所以Claude对于很多PDF文档阅读支持得很好。还有国产的Kimi Chat，据说支持约 20 万汉字的上下文，2.5 倍于 Anthropic 公司的 Claude-100k（实测约 8 万字），8 倍于 OpenAI 公司的 GPT-4-32k（实测约 2.5 万字）。可以说Context容量大小，也是大模型的核心竞争力之一。</p> 
<p>但这一切即将成为过去，GPT即将支持记忆能力。也就是GPT会通过缓存的方式记录之前和用户的对话。你不需要那么大的Context容量了，多次对话的性能和单次对话都是一样的。而且在大模型端，通过缓存的方式，可以极大降低应用的开销，让成本直接节省到二十分之一。</p> 
<p>期待……</p> 
<h3><a id="_213"></a>参考</h3> 
<ol><li>短课程：<a href="https://learn.deeplearning.ai/langchain/lesson/3/memory" rel="nofollow">https://learn.deeplearning.ai/langchain/lesson/3/memory</a></li><li>文档：<a href="https://python.langchain.com/docs/modules/memory/" rel="nofollow">https://python.langchain.com/docs/modules/memory/</a></li><li>Report: OpenAI to Introduce Updates to Make AI Models More Affordable: <a href="https://www.pymnts.com/news/artificial-intelligence/2023/openai-introduce-updates-make-ai-models-more-affordable/" rel="nofollow">https://www.pymnts.com/news/artificial-intelligence/2023/openai-introduce-updates-make-ai-models-more-affordable/</a></li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/70cc8565668ed94e408fabcca7c1fbc4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">GO学习之 数据库(mysql)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/42b566f6c355e777c4d8e17bc19d71e0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">图像处理（YOJ2.0中的题）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>