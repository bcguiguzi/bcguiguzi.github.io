<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于CENTOS7安装Kubernates集群 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于CENTOS7安装Kubernates集群" />
<meta property="og:description" content="一、节点规划 主机名
IP
角色
C7m1
10.137.213.240
k8s主节点
C7m2
10.137.213.241
k8s从节点
C7m3
10.137.213.242
k8s从节点
节点的软件版本：
操作系统： CentOS-7
Docker版本： 18.06.1-ce
Kubernetes版本： 1.13.1
所有节点都需要安装以下组件：
Docker：不用多说了吧
kubelet：运行于所有 Node上，负责启动容器和 Pod
kubeadm：负责初始化集群
kubectl： k8s命令行工具，通过其可以部署/管理应用 以及CRUD各种资源
二、准备工作 我们需要在所有节点都进行如下操作：
1.安装Docker 详细安装方法可参考我的博客文章：《基于CentOS7安装Docker》
https://blog.csdn.net/lazy_moon/article/details/85260896
2.关闭防火墙 systemctl disable firewalld systemctl stop firewalld 3.禁用SELINUX setenforce 0 vi /etc/selinux/config 修改config文件中的内容如下：
SELINUX=disabled 4.所有节点关闭 swap 由于使用kubeadm安装，需要关闭swap，临时关闭的方法如下：
swapoff –a 永久关闭swap：
cp -p /etc/fstab /etc/fstab.bak$(date &#39;&#43;%Y%m%d%H%M%S&#39;) sed -i &#34;s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g&#34; /etc/fstab mount -a reboot 5.设置主机名和host映射 这部分不必再多讲。
三、安装Kubernetes 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/3b0507cf985aeed908dfefcddf21d6cb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-12-30T11:19:54+08:00" />
<meta property="article:modified_time" content="2018-12-30T11:19:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于CENTOS7安装Kubernates集群</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、节点规划</h2> 
<table align="center" border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:10pt;">主机名</p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:10pt;">IP</p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:10pt;">角色</p> </td></tr><tr><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:10pt;">C7m1</p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:10pt;">10.137.213.240</p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:10pt;">k8s主节点</p> </td></tr><tr><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:10pt;">C7m2</p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:10pt;">10.137.213.241</p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:10pt;">k8s从节点</p> </td></tr><tr><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:10pt;">C7m3</p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:10pt;">10.137.213.242</p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:10pt;">k8s从节点</p> </td></tr></tbody></table> 
<p> </p> 
<p><strong>节点的软件版本：</strong></p> 
<p>操作系统： CentOS-7</p> 
<p>Docker版本： 18.06.1-ce</p> 
<p>Kubernetes版本： 1.13.1</p> 
<p><strong>所有节点都需要安装以下组件：</strong></p> 
<p>Docker：不用多说了吧</p> 
<p>kubelet：运行于所有 Node上，负责启动容器和 Pod</p> 
<p>kubeadm：负责初始化集群</p> 
<p>kubectl： k8s命令行工具，通过其可以部署/管理应用 以及CRUD各种资源</p> 
<h2>二、准备工作</h2> 
<p>我们需要在所有节点都进行如下操作：</p> 
<h3>1.安装Docker</h3> 
<p>详细安装方法可参考我的博客文章：《基于CentOS7安装Docker》</p> 
<p><a href="https://blog.csdn.net/lazy_moon/article/details/85260896">https://blog.csdn.net/lazy_moon/article/details/85260896</a></p> 
<h3>2.关闭防火墙</h3> 
<pre class="has"><code>systemctl disable firewalld
systemctl stop firewalld
</code></pre> 
<h3>3.禁用SELINUX</h3> 
<pre class="has"><code>setenforce 0
vi /etc/selinux/config
</code></pre> 
<p>修改config文件中的内容如下：</p> 
<pre class="has"><code>SELINUX=disabled</code></pre> 
<h3>4.所有节点关闭 swap</h3> 
<p>由于使用kubeadm安装，需要关闭swap，临时关闭的方法如下：</p> 
<pre class="has"><code>swapoff –a</code></pre> 
<p>永久关闭swap：</p> 
<pre class="has"><code>cp -p /etc/fstab /etc/fstab.bak$(date '+%Y%m%d%H%M%S')
sed -i "s/\/dev\/mapper\/centos-swap/\#\/dev\/mapper\/centos-swap/g" /etc/fstab
mount -a
reboot
</code></pre> 
<h3>5.设置主机名和host映射</h3> 
<p>这部分不必再多讲。</p> 
<h2>三、安装Kubernetes</h2> 
<h3>1.所有节点的操作</h3> 
<h4>（1）配置K8S的yum源</h4> 
<p>由于官方仓库无法访问，建议使用阿里云的仓库：</p> 
<pre class="has"><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre> 
<h4>（2）kubelet、kubeadm、kubectl的安装</h4> 
<p>使用以下指令：</p> 
<pre class="has"><code>yum install -y kubelet kubeadm kubectl</code></pre> 
<p>经过一段时间的下载和安装后：</p> 
<p><img alt="" class="has" height="280" src="https://images2.imgbox.com/63/dd/gLc72rRb_o.png" width="415"></p> 
<h4>（3）启动kubelet服务</h4> 
<pre class="has"><code>systemctl enable kubelet &amp;&amp; systemctl start kubelet</code></pre> 
<h3>2.Master节点配置</h3> 
<h4>（1）初始化k8s集群</h4> 
<p>由于国内网络问题，需要手动下载相关镜像并重新打tag：</p> 
<pre class="has"><code>docker pull mirrorgooglecontainers/kube-apiserver:v1.13.1
docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.1
docker pull mirrorgooglecontainers/kube-scheduler:v1.13.1
docker pull mirrorgooglecontainers/kube-proxy:v1.13.1
docker pull mirrorgooglecontainers/pause:3.1
docker pull mirrorgooglecontainers/etcd:3.2.24
docker pull coredns/coredns:1.2.6
docker pull registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64

docker tag mirrorgooglecontainers/kube-apiserver:v1.13.1 k8s.gcr.io/kube-apiserver:v1.13.1
docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.1 k8s.gcr.io/kube-controller-manager:v1.13.1
docker tag mirrorgooglecontainers/kube-scheduler:v1.13.1 k8s.gcr.io/kube-scheduler:v1.13.1
docker tag mirrorgooglecontainers/kube-proxy:v1.13.1 k8s.gcr.io/kube-proxy:v1.13.1
docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1
docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24
docker tag coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6
docker tag registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64 quay.io/coreos/flannel:v0.10.0-amd64

docker rmi mirrorgooglecontainers/kube-apiserver:v1.13.1           
docker rmi mirrorgooglecontainers/kube-controller-manager:v1.13.1  
docker rmi mirrorgooglecontainers/kube-scheduler:v1.13.1           
docker rmi mirrorgooglecontainers/kube-proxy:v1.13.1               
docker rmi mirrorgooglecontainers/pause:3.1                        
docker rmi mirrorgooglecontainers/etcd:3.2.24                      
docker rmi coredns/coredns:1.2.6
docker rmi registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64
</code></pre> 
<p>经过一段时间的初始化后：</p> 
<p><img alt="" class="has" height="88" src="https://images2.imgbox.com/c0/d9/z3fGEZxA_o.png" width="415"></p> 
<p>执行以下指令初始化master节点：</p> 
<pre class="has"><code>kubeadm init --kubernetes-version=v1.13.1 --apiserver-advertise-address 10.137.213.240 --pod-network-cidr=10.244.0.0/16</code></pre> 
<p>kubernetes-version表示kubernetes的版本</p> 
<p>apiserver-advertise-address表示master通信接口</p> 
<p>pod-network-cidr表示指定Pod的网络范围</p> 
<pre class="has"><code>[init] Using Kubernetes version: v1.13.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [c7m1 localhost] and IPs [10.137.213.240 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [c7m1 localhost] and IPs [10.137.213.240 127.0.0.1 ::1]
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [c7m1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.137.213.240]
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 27.504103 seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "c7m1" as an annotation
[mark-control-plane] Marking the node c7m1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node c7m1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: xcaamm.7a8cjcosexp38duh
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 10.137.213.240:6443 --token xcaamm.7a8cjcosexp38duh --discovery-token-ca-cert-hash sha256:b541d0f032fdbfe18d65e242711847a27c2b1c28b2a9be13a28d279b5bc6c214
</code></pre> 
<p><span style="color:#FF0000;">下面这段非常重要，每次初始化的时候一定要留着，salve增加到master的时候，需要再从节点上执行这段代码：</span></p> 
<pre class="has"><code>kubeadm join 10.137.213.240:6443 --token xcaamm.7a8cjcosexp38duh --discovery-token-ca-cert-hash sha256:b541d0f032fdbfe18d65e242711847a27c2b1c28b2a9be13a28d279b5bc6c214</code></pre> 
<p>如果出现以下错误：</p> 
<p><img alt="" class="has" height="37" src="https://images2.imgbox.com/6e/0c/k2jqobWq_o.png" width="415"></p> 
<p>pod-network-cidr=10.244.0.0/16是使用的flannel方案，使用这个网络方案需要设置系统参数，/proc/sys/net/bridge/bridge-nf-call-iptables 内容为1。</p> 
<p>执行以下指令：</p> 
<pre class="has"><code>echo "1" &gt;/proc/sys/net/bridge/bridge-nf-call-iptables</code></pre> 
<h4>（2）配置kubectl环境变量</h4> 
<p>类似于配置java环境变量：</p> 
<pre class="has"><code>echo "export KUBECONFIG=/etc/kubernetes/admin.conf" &gt;&gt; /etc/profile
source /etc/profile
</code></pre> 
<h4>（3）安装Pod网络</h4> 
<p>先在/usr/k8s目录下建立kube-flannel.yaml文件，内容如下：</p> 
<pre class="has"><code>---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-amd64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: amd64
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: arm64
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-arm64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-arm64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: arm
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-arm
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-arm
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-ppc64le
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: ppc64le
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-ppc64le
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-ppc64le
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-s390x
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: s390x
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-s390x
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-s390x
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
</code></pre> 
<p>执行下面的指令，根据yaml文件创建资源：</p> 
<pre class="has"><code>kubectl apply -f /usr/k8s/kube-flannel.yaml</code></pre> 
<p><img alt="" class="has" height="121" src="https://images2.imgbox.com/34/e3/gOw7mQmZ_o.png" width="334"></p> 
<p>Pod网络安装完成，可以执行如下命令检查一下 CoreDNS Pod此刻是否正常运行起来了，一旦其正常运行起来，则可以继续后续步骤。</p> 
<pre class="has"><code>kubectl get pods --all-namespaces -o wide</code></pre> 
<p><img alt="" class="has" height="70" src="https://images2.imgbox.com/fc/79/oQg7HoAt_o.png" width="415"></p> 
<pre class="has"><code>kubectl get nodes</code></pre> 
<p><img alt="" class="has" height="54" src="https://images2.imgbox.com/c7/1f/OXSZL2jk_o.png" width="251"></p> 
<p>主节点准备就绪。</p> 
<h3>3.增加从节点</h3> 
<h4>（1）查看master的token</h4> 
<p>在主节点上执行：</p> 
<pre class="has"><code>kubeadm token list</code></pre> 
<p>查看master的token：</p> 
<p><img alt="" class="has" height="17" src="https://images2.imgbox.com/40/31/ZgVs7JAT_o.png" width="415"></p> 
<h4>（2）增加从节点到主节点中</h4> 
<p>根据master初始化时的信息，分别在两个从节点上执行：</p> 
<pre class="has"><code>kubeadm join 10.137.213.240:6443 --token xcaamm.7a8cjcosexp38duh --discovery-token-ca-cert-hash sha256:b541d0f032fdbfe18d65e242711847a27c2b1c28b2a9be13a28d279b5bc6c214</code></pre> 
<p><img alt="" class="has" height="103" src="https://images2.imgbox.com/24/2a/viaVXPEd_o.png" width="415"></p> 
<p>在master中执行：</p> 
<pre class="has"><code>kubectl get nodes</code></pre> 
<p>查看节点，可以看到两个从节点已经加入成功了：</p> 
<p><img alt="" class="has" height="57" src="https://images2.imgbox.com/d2/f0/mJ0PGsqM_o.png" width="298"></p> 
<p>查看所有 Pod状态，在主节点上执行：</p> 
<pre class="has"><code>kubectl get pods --all-namespaces -o wide</code></pre> 
<p>Pod网络也完成了：</p> 
<p><img alt="" class="has" height="91" src="https://images2.imgbox.com/b9/ce/ttV99LZO_o.png" width="415"></p> 
<h2>四、安装 dashboard</h2> 
<p>可以给Kubernates安装一个可视化管理工具，便于集群管理。</p> 
<h3>1.下载镜像</h3> 
<p>在所有节点上下载镜像：</p> 
<pre class="has"><code>docker pull registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0
docker tag registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0
docker image rm registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetes-dashboard-amd64:v1.10.0
</code></pre> 
<h3>2.安装dashboard</h3> 
<h4>（1）建立dashboard.yaml文件</h4> 
<p>与建立kube-flannel.yaml文件类似，建立一个dashboard.yaml文件，内容如下：</p> 
<pre class="has"><code># Copyright 2017 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ------------------- Dashboard Secret ------------------- #

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kube-system
type: Opaque

---
# ------------------- Dashboard Service Account ------------------- #

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system

---
# ------------------- Dashboard Role &amp; Role Binding ------------------- #

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
rules:
  # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["create"]
  # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create"]
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"]
  verbs: ["get", "update", "delete"]
  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["kubernetes-dashboard-settings"]
  verbs: ["get", "update"]
  # Allow Dashboard to get metrics from heapster.
- apiGroups: [""]
  resources: ["services"]
  resourceNames: ["heapster"]
  verbs: ["proxy"]
- apiGroups: [""]
  resources: ["services/proxy"]
  resourceNames: ["heapster", "http:heapster:", "https:heapster:"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard-minimal
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system

---
# ------------------- Dashboard Deployment ------------------- #

kind: Deployment
apiVersion: apps/v1beta2
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
      - name: kubernetes-dashboard
        image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0
        ports:
        - containerPort: 8443
          protocol: TCP
        args:
          - --auto-generate-certificates
          - --token-ttl=5400
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          # - --apiserver-host=http://my-address:port
        volumeMounts:
        - name: kubernetes-dashboard-certs
          mountPath: /certs
          # Create on-disk volume to store exec logs
        - mountPath: /tmp
          name: tmp-volume
        livenessProbe:
          httpGet:
            scheme: HTTPS
            path: /
            port: 8443
          initialDelaySeconds: 30
          timeoutSeconds: 30
      volumes:
      - name: kubernetes-dashboard-certs
        hostPath:
          path: /home/share/certs
          type: Directory
      - name: tmp-volume
        emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule

---
# ------------------- Dashboard Service ------------------- #

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 31234
  selector:
    k8s-app: kubernetes-dashboard
  type: NodePort
</code></pre> 
<p>执行以下指令进行安装：</p> 
<pre class="has"><code>kubectl create -f /usr/k8s/dashboard.yaml</code></pre> 
<h4>（2）生成密钥和证书签名</h4> 
<p>执行以下指令：</p> 
<pre class="has"><code>openssl genrsa -des3 -passout pass:x -out dashboard.pass.key 2048
openssl rsa -passin pass:x -in dashboard.pass.key -out dashboard.key
openssl req -new -key dashboard.key -out dashboard.csr
</code></pre> 
<p style="margin-left:10pt;"> </p> 
<p>一路回车......</p> 
<p>然后将生成的 dashboard.key 和 dashboard.crt置于路径 /home/share/certs下，该路径会配置到下面即将要操作的dashboard-user-role.yaml文件中。这里我是在master中生成，然后复制到其他节点上。</p> 
<h4>（3）建立dashboard-user-role.yaml文件</h4> 
<p>访问dashboard需要建立用户</p> 
<p>与建立kube-flannel.yaml文件类似，建立一个dashboard-user-role.yaml文件，内容如下：</p> 
<pre class="has"><code>kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: admin
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: admin
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
</code></pre> 
<p> </p> 
<p>执行以下指令进行安装：</p> 
<pre class="has"><code>kubectl create -f /usr/k8s/dashboard-user-role.yaml</code></pre> 
<p>此时dashboard的状态应该是Running</p> 
<p><img alt="" class="has" height="144" src="https://images2.imgbox.com/cc/dc/2RR1SwCD_o.png" width="415"></p> 
<h3>3.访问dashboard</h3> 
<p>访问地址：<span style="color:#0563c1;"><u><a href="https://10.137.213.240:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" rel="nofollow">https://10.137.213.240:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></u></span></p> 
<p>（1）生成证书</p> 
<p>可能会出现以下错误：</p> 
<pre class="has"><code>{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "services \"https:kubernetes-dashboard:\" is forbidden: User \"system:anonymous\" cannot get services/proxy in the namespace \"kube-system\"",
  "reason": "Forbidden",
  "details": {
    "name": "https:kubernetes-dashboard:",
    "kind": "services"
  },
  "code": 403
}
</code></pre> 
<p>这是因为最新版的Kubernates默认启用了RBAC，并为未认证用户赋予了一个默认的身份：anonymous.</p> 
<p>对于API Server来说，它是使用证书进行认证的，我们需要先创建一个证书：</p> 
<p>1.首先找到kubectl命令的配置文件，默认情况下为/etc/kubernetes/admin.conf</p> 
<p>2.然后我们使用client-certificate-data和client-key-data生成一个p12文件，可使用下列命令：</p> 
<pre class="has"><code>grep 'client-certificate-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d &gt;&gt; kubecfg.crt
grep 'client-key-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d &gt;&gt; kubecfg.key
openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-client"
</code></pre> 
<p>最后导入上面生成的p12文件，重新打开浏览器，显示如下：</p> 
<p><img alt="" class="has" height="119" src="https://images2.imgbox.com/78/31/aF86fHo2_o.png" width="415"></p> 
<p>进入dashboard页面：</p> 
<p><img alt="" class="has" height="221" src="https://images2.imgbox.com/1c/24/lirNiSW6_o.png" width="415"></p> 
<h4>（2）获取token</h4> 
<p>查找dashboard的token列表：</p> 
<pre class="has"><code>kubectl get secret -n kube-system | grep dashboard</code></pre> 
<p><img alt="" class="has" height="54" src="https://images2.imgbox.com/87/b5/AvuTzVde_o.png" width="415"></p> 
<p>获取token：</p> 
<pre class="has"><code>kubectl -n kube-system  get secret kubernetes-dashboard-token-qkfsq -o jsonpath={.data.token}| base64 –d</code></pre> 
<p>kubernetes-dashboard-token-qkfsq就是上面列表中的名称：</p> 
<p><img alt="" class="has" height="159" src="https://images2.imgbox.com/0b/d2/aXvzclN1_o.png" width="415"></p> 
<h4>（3）登录dashboard</h4> 
<p>复制token，登录dashboard：</p> 
<p><img alt="" class="has" height="179" src="https://images2.imgbox.com/40/79/TXfD4rnS_o.png" width="415"></p> 
<p>登录成功：</p> 
<p><img alt="" class="has" height="229" src="https://images2.imgbox.com/2b/43/dcTJ8VTg_o.png" width="415"></p> 
<p> </p> 
<p>至此所有Kubernates集群的安装已经全部完成。</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p style="margin-left:10pt;"> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p style="margin-left:10pt;"> </p> 
<p style="margin-left:10pt;"> </p> 
<p style="margin-left:10pt;"> </p> 
<p style="margin-left:10pt;"> </p> 
<p style="margin-left:10pt;"> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/225316f17ca7b771564edfebe43bab6f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于CentOS7安装Docker</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/32c34a7cb3f81a92f0a1921079e507ed/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【NOIP2007提高组】矩阵取数游戏</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>