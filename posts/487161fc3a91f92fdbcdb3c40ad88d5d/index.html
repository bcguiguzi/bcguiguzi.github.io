<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>EDPLVO: Efficient Direct Point-Line Visual Odometry论文笔记 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="EDPLVO: Efficient Direct Point-Line Visual Odometry论文笔记" />
<meta property="og:description" content="EDPLVO: Efficient Direct Point-Line Visual Odometry 本文介绍了一种使用点和线的有效直接视觉里程计 (VO) 算法。直线上的像素一般采用直接法。但是，原始光度误差仅针对点定义。似乎很难将其扩展到线条。在以前的工作中，线上点的共线约束要么被忽略 [1]，要么将繁重的计算负载引入最终的优化系统 [2]。本文扩展了线的光度误差。我们证明了 2D 线上的点的 3D 点是由 2D 线的端点的反深度决定的，并为这个问题推导出了一个封闭形式的解决方案。该属性可以显着减少变量数量以加快优化速度，并且可以使共线约束完全满足。此外，我们引入了一种两步法来进一步加速优化，并证明了该方法的收敛性。实验结果表明，该的算法优于最先进的直接 VO 算法。
介绍 没有闭环的vslam称为视觉里程计也就是前端VO。
主要两点：
基于学习的方法（深度学习）：基于学习的方法近年来取得了显着进展。然而，由于这些方法需要强大的 GPU，它们对于嵌入式系统上的实时应用程序是不可行的。
传统的 VSLAM 和 VO 系统仍然更适合这些应用（轻量且实时）。
传统方法通常分为两类，即基于特征的（间接）和直接方法。
基于特征的方法长期以来一直主导着这一领域。同时，最近的研究表明，直接方法显示出高精度和鲁棒性，即使在通常对基于特征的方法具有挑战性的低纹理场景中也是如此。因此，本文重点介绍 VO 的直接方法。
基于特征点的方法：(最小化重投影误差)
根据图像上的特征匹配关系得到相邻帧间的相机运动估计，它需要对特征进行提取和匹配，然后根据匹配特征构建重投影误差函数，并将其最小化从而得到相机的相对运动。
单目：2D-2D对极几何；
2D-3D：PNP;
3D-3D:ICP。
==重投影误差：==利用我们计算得到的三维点的坐标（注意不是真实的）和我们计算得到的相机位姿（当然也不是真实的）进行第二次投影，也就是重投影。重投影误差：指的真实三维空间点在图像平面上的投影（也就是图像上的像素点）和重投影（其实是用我们的计算值得到的虚拟的像素点）的差值。
再说的简单点： 重投影误差指的是空间中某个3D点通过估计的相机位姿重投影得到的像素点，与实际拍摄图像中的像素点之间的误差（欧氏距离）
欧式距离也称欧几里得距离，是最常见的距离度量，衡量的是多维空间中两个点之间的 绝对距离 。说的再明白点 ：就是我们平时学的计算二维平面上两点之间的距离。
直接法：（光度误差）光流法改进，基于灰度不变假设
直接法是假设两帧图像中的匹配像素的灰度值不变，构建光度误差函数， 也将其最小化求解帧间的相机运动。
光流法：提取图像特征点，与特征点法不同的是，光流法通过图像灰度值 (RGB)值匹配特征点，光流描述了像素在图像中的运动，再利用三角/对极几何/PnP等算法估算相机运动。减少特征点匹配所需的计算量，精度也有保证。直接法：直接使用像素块，也可以提取图像角点 (角点像素突出因此能够进行更好的匹配)，通过计算灰度值(RGB)值直接解算得到运动估计(R,t)。利用像素移动直接计算得到相机的移动。一般需要位姿的初始估计，通过初始的位姿估计来得到匹配的图像特征点，通过灰度值(RGB)值优化位姿估计。运算速度快，但是由于灰度不变假设，过快的图像运动，容易陷入局部最优解。半直接法：以SVO为代表，将图像分块来做匹配。指通过对图像中的特征点图像块进行直接匹配来获取相机位姿,而不像直接匹配法那样对整个图像进行匹配。 随着一批不需提取特征的方法，如SVO（选取关键点来采用直接法，这类方法称为稀疏方法（sparse））；LSD（选取整幅图像中有梯度的部分来采用直接法，这种方法称为半稠密方法（simi-dense）），直接法渐露其自身优势。DSO（Direct Sparse Odometry，稀疏直接运动估计算法），DSO的前端和LSD-SLAM相似，后端则抛弃了图优化的框架。
总结一下：直接法将数据关联（data association）与位姿估计（pose estimation）放在了一个统一的非线性优化问题中，最小化光度误差。而特征点法则分步求解，即，先通过匹配特征点求出数据之间关联，再根据关联来估计位姿。这两步通常是独立的，在第二步中，可以通过重投影误差来判断数据关联中的外点，也可以用于修正匹配结果。
下面是介绍了直接法的一些具体实现：
LSD-SLAM
2014年 Large Scale Direct monocular SLAM
将直接法应用到了半稠密的单目SLAM中
1、提出了地图梯度与直接法的关系，以及像素梯度与极线方向在稠密重建中的角度关系；
2、在CPU上实现了实时半稠密场景的重建；
3、具有回环检测功能；" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/487161fc3a91f92fdbcdb3c40ad88d5d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-13T19:39:35+08:00" />
<meta property="article:modified_time" content="2022-11-13T19:39:35+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">EDPLVO: Efficient Direct Point-Line Visual Odometry论文笔记</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="EDPLVO_Efficient_Direct_PointLine_Visual_Odometry_0"></a>EDPLVO: Efficient Direct Point-Line Visual Odometry</h2> 
<p>本文介绍了一种使用<strong>点和线的有效直接视觉里程计 (VO) 算法</strong>。直线上的像素一般采用直接法。但是，原始光度误差仅针对点定义。似乎很难将其扩展到线条。在以前的工作中，线上点的共线约束要么被忽略 [1]，要么将繁重的计算负载引入最终的优化系统 [2]。本文扩展了线的光度误差。<strong>我们证明了 2D 线上的点的 3D 点是由 2D 线的端点的反深度决定的，并为这个问题推导出了一个封闭形式的解决方案。该属性可以显着减少变量数量以加快优化速度，并且可以使共线约束完全满足。此外，我们引入了一种两步法来进一步加速优化，并证明了该方法的收敛性</strong>。实验结果表明，该的算法优于最先进的直接 VO 算法。</p> 
<h3><a id="_4"></a>介绍</h3> 
<p>没有闭环的vslam称为视觉里程计也就是前端VO。</p> 
<p>主要两点：</p> 
<ol><li> <p>基于学习的方法（深度学习）：基于学习的方法近年来取得了显着进展。然而，由于这些方法需要<strong>强大的 GPU</strong>，它们对于<strong>嵌入式系统上的实时应用程序是不可行的</strong>。</p> <p><u>传统的 VSLAM 和 VO 系统仍然更适合这些应用（轻量且实时）。</u></p> </li><li> <p>传统方法通常分为两类，即基于特征的（间接）和直接方法。</p> <p>基于特征的方法长期以来一直主导着这一领域。同时，最近的研究表明，<strong>直接方法显示出高精度和鲁棒性，即使在通常对基于特征的方法具有挑战性的低纹理场景中也是如此</strong>。因此，本文重点介绍 VO 的直接方法。</p> 
  <ul><li> <p><strong>基于特征点的方法</strong>：(最小化<strong>重投影误差</strong>)</p> <p><u>根据图像上的特征匹配关系得到相邻帧间的相机运动估计，它<strong>需要对特征进行提取和匹配</strong>，然后根据匹配特征<strong>构建重投影误差函数</strong>，并将其最小化从而得到相机的相对运动。</u></p> <p>单目：2D-2D对极几何；</p> <p>2D-3D：PNP;</p> <p>3D-3D:ICP。</p> <p>==重投影误差：==利用我们计算得到的三维点的坐标（注意不是真实的）和我们计算得到的相机位姿（当然也不是真实的）进行第二次投影，也就是重投影。重投影误差：指的真实三维空间点在图像平面上的投影（也就是图像上的像素点）和重投影（其实是用我们的计算值得到的虚拟的像素点）的差值。</p> <p><strong>再说的简单点：</strong> 重投影误差指的是空间中某个3D点通过估计的相机位姿重投影得到的像素点，与实际拍摄图像中的像素点之间的误差（<mark>欧氏距离</mark>）</p> <p><strong>欧式距离</strong>也称欧几里得距离，是最常见的距离度量，衡量的是多维空间中两个点之间的 绝对距离 。说的再明白点 ：就是我们平时学的计算二维平面上两点之间的距离。</p> </li><li> <p><strong>直接法：</strong>（光度误差）光流法改进，基于灰度不变假设</p> <p><u>直接法是假设两帧图像中的匹配像素的灰度值不变，构建光度误差函数， 也将其最小化求解帧间的相机运动。</u></p> 
    <ul><li>光流法：提取图像特征点，与特征点法不同的是，<strong>光流法通过图像灰度值 (RGB)值匹配特征点</strong>，光流描述了像素在图像中的运动，再利用三角/对极几何/PnP等算法估算相机运动。减少特征点匹配所需的计算量，精度也有保证。</li><li>直接法：直接使用像素块，也可以提取图像角点 (角点像素突出因此能够进行更好的匹配)，<strong>通过计算灰度值(RGB)值直接解算得到运动估计(R,t)</strong>。利用像素移动直接计算得到相机的移动。一般需要位姿的初始估计，通过初始的位姿估计来得到匹配的图像特征点，通过灰度值(RGB)值优化位姿估计。运算速度快，但是由于灰度不变假设，过快的图像运动，容易陷入局部最优解。</li><li>半直接法：以SVO为代表，将图像分块来做匹配。指通过对图像中的特征点图像块进行直接匹配来获取相机位姿,而不像直接匹配法那样对整个图像进行匹配。</li></ul> <p>随着一批不需提取特征的方法，如SVO（选取关键点来采用直接法，这类方法称为稀疏方法（sparse））；LSD（选取整幅图像中有梯度的部分来采用直接法，这种方法称为半稠密方法（simi-dense）），<strong>直接法渐露其自身优势</strong>。DSO（Direct Sparse Odometry，稀疏直接运动估计算法），DSO的前端和LSD-SLAM相似，后端则抛弃了图优化的框架。</p> <p><mark>总结一下</mark>：直接法将数据关联（data association）与位姿估计（pose estimation）放在了一个统一的非线性优化问题中，<strong>最小化光度误差</strong>。而特征点法则分步求解，即，先通过匹配特征点求出数据之间关联，再根据关联来估计位姿。这两步通常是独立的，在第二步中，可以通过重投影误差来判断数据关联中的外点，也可以用于修正匹配结果。</p> <p><strong>下面是介绍了直接法的一些具体实现：</strong></p> <p><mark>LSD-SLAM</mark><br> 2014年 Large Scale Direct monocular SLAM</p> <p>将直接法应用到了半稠密的单目SLAM中<br> 1、提出了地图梯度与直接法的关系，以及像素梯度与极线方向在稠密重建中的角度关系；<br> 2、在CPU上实现了实时半稠密场景的重建；<br> 3、具有回环检测功能；<br> 4、一些技巧保证追踪的实时性与稳定性：<br> 在极线上等距离取5个点，度量其SSD；深度估计时，首先用随机数初始化深度，在估计完后又把深度均值归一化以调整尺度；度量深度不确定性时，不仅考虑三角化的几何关系，还考虑了极线与深度的夹角，归纳成一个光度不确定项；关键帧之间的约束使用了相似变换群及与之对应的李代数显式表达出尺度，在后端优化中可以将不同的尺度的场景考虑进来，减小尺度漂移现象。</p> <p>缺点：<br> 1、对相机内参和曝光非常敏感<br> 2、在相机快速运动时容易丢失<br> 3、依赖于特征点的方法进行回环检测</p> <p><mark>SVO</mark><br> 2014年 Semi-direct Visual Odemetry<br> 基于稀疏直接法的视觉里程计：<br> SVO跟踪了一些关键点（角点），然后像直接法那样根据关键点周围的信息（4*4小块进行块匹配）估计相机运动及其位置 。</p> <p>优点：<br> 1、速度极快，适用于计算平台受限的场合<br> 2、提出了深度滤波器的概念，推导了基于均匀-高斯混合分布的深度滤波器，用于关键点的位置估计，<strong>并使用了逆深度作为参数化形式。</strong></p> <p>缺点：<br> 1、误差大，不准确<br> 2、目标应用平台为无人机，相机运动主要为水平和上下移动，在平视相机中表现不佳。如单目初始化时使用了分解H矩阵（单应矩阵），需要假设特征点位于平面上；在关键帧选择时，使用了平移量作为确定新的关键帧的策略，而没有考虑旋转量。<br> 3、舍弃了后端优化和回环检测功能，没有建图功能。</p> <p><mark>DSO</mark><br> 2016年 DSO（Direct Sparse Odometry）<br> 慕尼黑工业大学（TUM）计算机视觉实验室发布的一个稀疏直接法的视觉里程计。</p> <p>后端使用一个由若干个关键帧组成的滑动窗口。除了维护这个窗口中的关键帧与地图点外，还会维护与优化相关的结构。特别地，这里指Gauss-Newton或Levenburg-Marquardt方法中的Hessian矩阵和b向量（仅先验部分）。</p> <p>提出了光度标定，认为对相机的曝光时间、暗角、伽马响应等参数进行标定后，能够让直接法更加鲁棒。对于未进行光度标定的相机，DSO也会在优化中动态估计光度参数。</p> <p>优点：可以生成相当稠密的点云，速度在正常运行的时候很快</p> <p>缺点：<br> 1、对场景光照要求高，要求尽量保持曝光时间的稳定<br> 2、不是个完整的SLAM，它没有回环检测、地图重用、丢失后的重定位。<br> 3、初始化部分也比较慢，当然双目或RGBD相机会容易很多。<br> 4、代码可扩展性比较差</p> <p>DSO在准确性，稳定性和速度上都比LSD好。LSD的优势在于回环检测。</p> </li></ul> </li></ol> 
<p>直接方法通常采用具有足够大梯度的像素，通常包括线上的角点和点。如图 所示，在许多人造场景中，线上的点数可以显着超过角点。通过光流跟踪拐角是明确定义的。然而，在线上的跟踪点是有问题的，因为在线上存在<strong>一维模糊性</strong>。放弃共线约束可能导致不太准确的深度估计。尽管已经探索了线来克服这个问题，但它们通常会显着增加优化中的计算负载 。这项工作基于我们之前的工作 DPLVO(<strong>点光度一致性+共线约束</strong>) ，我们寻求加速计算。本文的主要贡献包括：<br> <img src="https://images2.imgbox.com/e9/f5/ixq0ZHfR_o.jpg" alt="在这里插入图片描述"></p> 
<ol><li> <p>将光度误差扩展到线条。原始光度误差仅针对点定义，难以应用于线。我们没有像DPLVO中那样简单地将共线约束引入代价函数，而是提出了一种新方法来<strong>参数化 3D 共线点</strong>，以使将线合并到光度误差中可行。具体来说，我们证明了 <strong>2D 线上任意点的 3D 点是由 2D 线的两个端点的深度倒数决定的。此属性可以显着减少变量的数量</strong>。同时，我们的方法在优化过程中精确地满足共线约束，从而提高了精度。</p> </li><li> <p>由于在优化中引入了长期线关联，引入了一种<strong>两步方法来限制计算复杂度</strong>。</p> 
  <ul><li> <p>在每次迭代中，我们首先使用固定的逆深度和关键帧姿势拟合 3D 线。</p> </li><li> <p>然后我们使用<strong>新的线参数</strong>来调节反深度的优化和关键帧姿态。</p> <p>由此产生的两个优化问题很容易解决。我们证明了这种方法总是可以收敛的。</p> </li></ul> </li></ol> 
<h3><a id="_112"></a>相关工作</h3> 
<h4><a id="line_matching__114"></a>line matching 线匹配</h4> 
<p>​ 使用线的挑战之一是执行线匹配。基于描述子的线匹配方法在以前的算法中被广泛采用。线描述子 LBD通常用于此目的。由于传统的线检测方法，如 LSD ，可能不稳定，这可能会导致线匹配失败 。尽管基于深度学习的线检测方法显示出前景，但这些方法通常对计算要求很高。<strong>此外，由于移动摄像机观察到的3D线段会在一部分出现或移出摄像机视野（FoV）时发生变化，这种外观变化也可能导致匹配失败。</strong></p> 
<p>​ 最近，提出了<strong>基于跟踪的解决方案</strong>来克服这个问题。王等人 提出了提出利用时空相干性进行线匹配的线流。在DPLVO中，提出了用于线匹配的跟踪-扩展-再分配方法。该方法从一条线上采样一些点，然后通过<strong>最小化沿极线的光度误差</strong>来跟踪它们，这可以自然地并入<strong>DSO的前端</strong>。因此我们采用这种方法来建立线关联。</p> 
<p><mark>LBD描述子和LSD算法</mark></p> 
<p>简单的说，LSD提取到的直线如果需要后续的匹配等操作，必不可少的就是描述方法，LBD就是一种用来描述直线的描述子。所以，LBD描述子是用来辅助线特征来进行匹配的，如果不需要匹配，可以不进行LBD描述子计算。</p> 
<p><mark>LSD算法</mark></p> 
<p>LSD 算法是 2012 年由 Von 等人提出的，该算法的优势是可以自行控制误检的线段数量同时该算法不需要设置相关的提取参数。LSD 算法是通过计算图像的梯度值以及梯度的方向，将图像通过梯度场的方式展现出来（其实就是利用每个像素的梯度来判断线的位置）。<strong>LSD 首先对图像进行预处理即使用降采样和高斯滤波算法对图像进行降噪，然后计算像素的梯度方向和梯度值</strong>，如下图所示，最后将那些<strong>方向信息近似的像素点合并起来</strong>，形成图中三种颜色区域也叫作线段支持域。<br> <img src="https://images2.imgbox.com/14/37/FSOmkzbb_o.jpg" alt="在这里插入图片描述"></p> 
<p>当确定了大概的区域后，对其进行检验，对每一条线段的支持域做一个最小外接矩形包裹住区域内的所有像素点，然后设立一个<strong>阈值</strong>，通过判断矩形内部的像素点的梯度方向与矩形的主方向误差的大小来<strong>判断该像素点是否为内点</strong>，若求得的误差在所选定的阈值范围内，则认定该像素点为内点，当内点的比例达到一定数量值时就认为<strong>该线段支持域为一条线段</strong>。<br> <mark>LBD描述子</mark></p> 
<p>​ 图像处理学中，线特征的匹配相较于点特征的匹配较难，原因主要包括以下几条：</p> 
<p>​ 一、线段的端点无法被有效的识别出；</p> 
<p>​ 二、线段的提取过程中容易产生断裂现象；</p> 
<p>​ 三、场景中纹理较弱。</p> 
<p>LBD 的优势在于可以更好的描述线条的局部外观，同时具备更好的匹配性能，且计算成本更低。</p> 
<p><strong>在此不赘述LBD怎么推导表示。</strong></p> 
<p><mark>DSO</mark></p> 
<p>在单目SLAM中，所有地图点在一开始被观测到时，都只有一个<strong>2D的像素坐标</strong>，其深度是未知的。这种点在DSO中称为未成熟的地图点：Immature Points。随着相机的运动，DSO会在每张图像上追踪这些未成熟的地图点，<strong>这个过程称为trace——实际上是一个沿着极线搜索的过程</strong>，十分类似于svo的depth filter。Trace的过程会<strong>确定每个Immature Point的逆深度和它的变化范围</strong>。**如果Immature Point的深度（实际中为深度的倒数，即逆深度）在这个过程中收敛，那么我们就可以确定这个未成熟地图点的三维坐标，形成了一个正常的地图点。**具有三维坐标的地图点，在DSO中称为PointHessian活跃点。与FrameHessian相对，PointHessian亦记录了这个点的三维坐标，以及Hessian信息（二阶偏导，或者说指导它收敛的梯度信息）。</p> 
<p>pointHessians是所有活跃点的信息。<strong>所谓活跃点，是指它们在相机的视野中，其残差项仍在参与优化部分的计算。</strong></p> 
<h4><a id="Featurebased_Method_150"></a>Feature-based Method基于特征的方法</h4> 
<p><strong>仅使用点的基于特征的方法在低纹理区域的性能下降</strong>。线条可以补充低纹理环境中的点。对于点，已经建立好的ORB-SLAM框架可以很容易地扩展到线。但是这种策略<strong>显著增加了前端和后端的计算负载</strong>。室内环境中的线结构可以用来提高性能。由于曼哈顿世界假设，这些方法很难应用于室外场景。</p> 
<p><strong>曼哈顿世界的假设</strong>是所有墙壁彼此成直角并垂直于地板。</p> 
<h4><a id="Direct_Method_156"></a>Direct Method直接法</h4> 
<p>​ 直接方法是对估计相机位姿和点深度进行最小化光度误差。漂移对于 VO 系统来说是不可避免的。最近的一些工作将回环检测引入到漂移校正的直接方法中。虽然闭环可以纠正漂移，但它只是事后纠正关键帧姿势的漂移。准确的动态跟踪姿势对于许多实时应用程序也很重要，例如运动规划、控制和AR。线条提供了提高性能的另一种选择。</p> 
<p>​ <strong>光度误差是测量参考图像和目标图像中点的灰度值之间的差值</strong>。**与基于特征的方法不同，基于特征的方法可以为不同类型的特征定义几何距离，光度误差仅针对点定义。**因此，直接法很难推广到直线上。在文献中，<strong>一般采用共线约束来调节深度估计</strong>。在[11]和[32]（两篇使用线的文献）中，<strong>共线点拟合一条3D直线，然后共线点投影到这条直线上。在这些方法中，线没有与点和关键帧姿态共同优化。</strong></p> 
<p>​ <strong>在DPLVO中，<mark>共线约束与光度误差相结合，联合优化带有点和关键帧姿态的3D直线</mark>。但并不能保证在优化过程中能完全满足共线约束，且直线显著增加了优化的计算量。此外，DPLVO中的3D线表示在其第一个二维线观测的后投影平面上，具有两个自由度(DoF)。尽管这种参数化减少了未知的数量以加速优化，但<mark>二维直线的估计误差可能会导致次优结果</mark>。此外，通过拟合共线3D点初始化，<mark>DPLVO中的3D线，并在不考虑共线约束的情况下初始估计共线3D点。如果共线点质量较差，如图2 (b)所示，则三维直线初始化不准确甚至失败，从而导致三维直线退化或失去对共线点深度估计的调节能力</mark>。在这篇论文中，我们试图克服这些缺点，并展示如何有效优化具有点和关键帧姿态的3D线的全四自由度。</strong>（不知道怎么理解这里的四自由度 ）<br> <img src="https://images2.imgbox.com/fb/b5/EtGW1kWo_o.jpg" alt="在这里插入图片描述"></p> 
<h3><a id="NOTATIONS_AND_PRELIMINARIES_166"></a>NOTATIONS AND PRELIMINARIES符号和预备知识</h3> 
<p>本文<strong>用粗体字母表示向量和矩阵，用斜体小写字母和斜体大写字母分别表示标量和函数</strong>。</p> 
<h4><a id="_170"></a>光度误差</h4> 
<p>光度误差定义在<strong>参考图像Ii和目标图像Ij之间的光度误差</strong>。我们用<strong>旋转矩阵R∈SO3和平移向量t∈R3来表示一个摄像机姿态</strong>，或者更简洁地用变换矩阵T∈SE(3)来表示。我们将参考图像和目标图像的姿态分别表示为Ti和Tj。设<strong>Ω表示图像域</strong>。假设x∈Ω是在参考图像Ii中有一个<mark>深度为d的逆点</mark>（这个翻译是错的，看原文的话这里表示的是x是参考图像中<mark>逆深度为d</mark>的点）。假设x在x '∈Ωj处也被Ij观测到。x和x '之间的关系是这样的<br> <img src="https://images2.imgbox.com/25/ec/vG6jD8zX_o.jpg" alt="在这里插入图片描述"></p> 
<p>x 是图像内的点，d 为这个三维点的逆深度 那个像π似的是反投影矩阵 （而且是带有相机内参矩阵c的）。</p> 
<p>其中Πc: R3→Ω和Π−1c: Ω × R→R3分别表示带有相机内参矩阵c的投影和反投影函数**，Rij和tij分别表示Tj T−1i的旋转和平移分量**。我们采用在（1）中表示的光度误差，形式为（<mark>扔出光度误差公式</mark>）<br> <img src="https://images2.imgbox.com/66/8b/bhg959CW_o.jpg" alt="在这里插入图片描述"></p> 
<p>这里看个人理解是反投影矩阵还是反投影函数，需要确定是这个跟内参有关。</p> 
<p>i和j 对应两个相机视角； x就是两个视角看到的点；t为曝光时间 ；ai，aj，bi 和 bj 是仿射亮度变换参数，wx 是梯度相关的加权因子，Nx 是 x 周围的一组邻域，‖·‖γ 是 Huber 范数.</p> 
<p><mark>Huber范数</mark><strong>为了更好地消除异常噪声并提高插值精度，提出采用Huber范数代替L 2 范数对重建误差施加最小化约束</strong>，Huber范数的最小化约束实际上等价于对大重构误差（异常噪声）的L 1 范数最小化约束和对小重构误差（高斯随机噪声）的L 2 范数最小化约束，因此对异常噪声具有很好的鲁棒性。</p> 
<p><strong>插值又是啥呢？在离散数据的基础上补插连续函数，使得这条连续曲线通过全部给定的离散数据点。</strong></p> 
<p><strong>==L1范数损失函数，也被称为最小绝对值偏差;L2范数损失函数，也被称为最小平方误差；<mark>Huber损失，平滑的平均绝对误差，Huber损失对数据中的异常点没有平方误差损失那么敏感。本质上，Huber损失是绝对误差，只是在误差很小时，就变为平方误差。</mark></strong></p> 
<h4><a id="_192"></a>核心误差构建</h4> 
<p><strong>Pl ̈ucker Coordinates<mark>普吕克坐标系</mark></strong></p> 
<p>这里提到了核心误差构建，可能大家会有疑问：</p> 
<p><strong>直接看总误差</strong><br> <img src="https://images2.imgbox.com/5f/55/BCxf1T1I_o.jpg" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a7/a2/mPLeMuAw_o.jpg" alt="在这里插入图片描述"></p> 
<p><u><strong>上文我们一直在讲光度误差，也就是第一部分。而现在开始讲的就是第二部分误差，跟线有关的核心误差。</strong></u></p> 
<p><strong>首先简单说一下普吕克坐标系 是将空间中的一个3D直线用两个在该直线上的3D点进行了表示。</strong></p> 
<p>一条3D直线可以用Pl ucker坐标表示。给定3D直线上的两点p1和p2, 3D直线的Pl ucker坐标系表示为（3）这个公式。其中×表示叉乘。Pl ucker坐标是齐次坐标。这里我们将d归一化，因为这样可以在接下来的描述中得到更简洁的公式。</p> 
<p>在普吕克坐标系表示下 点到该直线的距离为：（4）<br> <img src="https://images2.imgbox.com/bb/09/Ss3sFGXb_o.jpg" alt="在这里插入图片描述"><br> 前文提到在普吕克坐标系下 一个3D直线可以用线上的两个3D点进行表示，并且我们对深度进行了归一化。</p> 
<p>这里再解释一下投影函数：这篇论文是将直接法的残差计算从点扩展到了线段，<strong>用Π表示投影的函数，也就是用像素坐标和内参矩阵以及深度信息，投影出点的空间坐标，反之Π-1表示的是将空间坐标投影到像素平面上。由此可以定义特征点的投影过程：</strong><br> <img src="https://images2.imgbox.com/0e/2c/mo3HHBkc_o.png" alt="在这里插入图片描述"></p> 
<p>利用这个式子，我们可以得到前一帧上特征点投影到当前帧上的位置，用这个位置，就可以计算直接法中第一项的光度误差。</p> 
<p>再补充讲一下普吕克坐标：</p> 
<p>不同之处在于，这篇文章中的普朗克坐标做了一下<strong>归一化处理</strong>，**原本的普朗克坐标是两个向量组成：线段的方向向量以及光心、线段组成平面的法向量，**在这篇论文中对普朗克坐标的定义稍微改变了一下：<br> <img src="https://images2.imgbox.com/ad/42/tyyfJBDd_o.jpg" alt="在这里插入图片描述"></p> 
<p>而一般的普朗克坐标定义是：d=p2−p1,m=p1×p2，<strong>也就是完全依赖于线段的两个端点</strong>，不过论文中的定义并不妨碍我们用普朗克坐标去对线段进行表示，经过验算其实这两种表达方法都是一样的。在普朗克坐标的基础上，**可以用点的坐标和线段的普朗克坐标直接计算出点到空间线段的距离：<strong>公式（4）</strong><br> <img src="https://images2.imgbox.com/a2/aa/HMJB22SG_o.jpg" alt="在这里插入图片描述"></p> 
<p>那这个公式怎么理解：</p> 
<p>这个式子其实用<strong>面积</strong>取解释是最好理解的，组成普朗克坐标的两个向量，<strong>d表示线段的方向，m的大小为围成三角形的面积的两倍，d的方向是围成平面的法向量方向。我们假设相机光心为o，线段的两个端点为p1p2，点为p</strong>，那么有下面的图<br> <img src="https://images2.imgbox.com/ea/d2/MkvbBrMi_o.jpg" alt="在这里插入图片描述"></p> 
<p>对于这个图来说，L也就是空间线段P1P2，m的大小表示的是三角形OP1P2的面积的两倍，也就是图中矮一点的平行四边形的面积，而p也就是向量OP，其与d叉乘的，也就是OP与P1P2叉乘，其大小表示的是图中较高一点的平行四边形的面积，而这两个面积，底的大小是一样的，差别就是高，而这个高，反应的就是p到p1p2的距离。这里可能存在一些式子细节上的理解问题，但是个人感觉这样去解释是最方便的。<br> <strong>这个距离也就是误差，累加和求最小，后面第三个误差项会用到。</strong></p> 
<h5><a id="_236"></a>线段上任一点的空间表示方法</h5> 
<p><strong>参数化共线2D点的3D点</strong></p> 
<p>接下来我们就是要<strong>表示这两个空间3D点</strong>：</p> 
<p>在本节中，我们证明了<strong>2D直线上2D点的3D点可以由2D直线的两个端点的反深度来确定</strong>，如图3 (a)所示。形式上，假设x1和x2是2D线段l的端点，其深度分别为逆深度d1和d2。利用(1)中引入的反投影函数Π−1c, x1和x2的三维点具有形式<br> <img src="https://images2.imgbox.com/d0/99/lf2tkRGM_o.jpg" alt="在这里插入图片描述"><br> x 1和x 2 为2d线的两个端点 d 1和d 2 分别为这两个三维点的逆深度 那个像π似的是反投影矩阵 <strong>得到了这条线上的两个三维点坐标</strong>（上面已经讲过反投影的作用）<br> 然后结合上面的普吕克坐标表示的直线 就可以用上面两个三维点来表示直线了，也就是公式（6）。</p> 
<p><strong>线上任意一点的空间表示方法</strong></p> 
<p>对于一帧图像上的一条线段，两个端点的坐标可以用投影和逆深度来表示（论文没有提逆深度是怎么得到的，应该是用<strong>三角化</strong>）之前在vslam中提到三角化恢复深度信息。</p> 
<p>上文解释了3D点坐标，以及利用这两个点，可以计算出线段的普朗克坐标。</p> 
<p>那么对于2D线段上的任何一点，定义点的方向向量为：<br> <img src="https://images2.imgbox.com/0a/57/hP2VbrFc_o.jpg" alt="在这里插入图片描述"></p> 
<p>这个定义看起来比较麻烦，从图里来看直接就是相机光心发出，经过点x的向量：<br> <img src="https://images2.imgbox.com/12/bf/23VwRrhN_o.jpg" alt="在这里插入图片描述"></p> 
<p>在定义这些量之后，我们可以得出下面的式子：</p> 
<p><strong>定理1:2D直线上2D点x的3D点px由d1和d2确定，形式为:</strong><br> <img src="https://images2.imgbox.com/2a/2a/De16vLa4_o.jpg" alt="在这里插入图片描述"></p> 
<p>先说结论，通过推导，这个式子表明<strong>线段上任何一个点的空间坐标依赖于线段端点的逆深度</strong>。推导的过程在论文的附录里面，<strong>对于点的空间坐标px，由于px必然在线段p1p2上，所以点到线段的距离一定是0，此外p1px和p1p2一定是共线的，由此可以写出下面两个式子：</strong><br> <img src="https://images2.imgbox.com/73/64/xWXlEI1y_o.png" alt="在这里插入图片描述"><br> 这一块是根据上面的图来的。</p> 
<h5><a id="_271"></a>线段的光度误差计算</h5> 
<p>上文公式(2)中引入的光度误差只是对<mark>点</mark>定义的。在本节中，我们展示了<mark>线</mark>可以很容易地纳入光度误差(2)，如图3 (a)所示（还是上面那张图）。假设一条3D线L被参考图像Ii和目标图像Ij观察到，L在Ii中的图像为L。我们从L中采样一些点，如在[2]中所做的那样。<strong>根据定理1，很明显，给定d1和d2，只确定了内点的三维点。因此，我们使用不同的公式来计算端点和l内部点的光度误差。对于一个内部点x，我们首先用(8)来计算对应的3D点px。然后把px投影到Ij中。这个过程可以表述为</strong><br> <img src="https://images2.imgbox.com/9a/4e/Hmcov40S_o.jpg" alt="在这里插入图片描述"></p> 
<p><strong>线段的光度误差可以分为两部分：端点的光度误差和线段上采样点的光度误差。</strong></p> 
<p>假设l的N个内点被采样，形成集合x，我们将二维直线l的光度误差表示为</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-NrNH0quo-1668338297619)(EDPLVO.assets/15.jpg)]<img src="https://images2.imgbox.com/05/19/Oa7JEnnp_o.jpg" alt="在这里插入图片描述"></p> 
<p><strong><mark>注意</mark>，Elj只依赖于x1和x2的逆深度，无论从l中采样了多少个点。</strong></p> 
<p>由于我们在上一节最后证明了，线段上任何一点的光度误差只取决于端点的逆深度，所以对于上面这个式子，其结果只取决于两个端点的逆深度。</p> 
<p>看过DPLVO之后才发现，这个线段的光度误差是在DPLVO的基础上增加的，这也是改进点之一，<strong>原本在DPLVO里面只有点的重投影误差和共线约束两部分，这里又增加了线的重投影误差，其实本质上应该算是采样点的重投影误差，由于采样点的2d位置确定后，其3d位置只取决于起点终点的深度，所以在不增加参量的情况下，给残差增加了新的约束。</strong>（后文再讲这个DPLVO）</p> 
<h5><a id="_291"></a>线段的共线约束</h5> 
<p>他文章太绕了，我在这里简单讲一下：</p> 
<p>本文采用LSD算法进行线路检测。由于噪声、量化误差和运动模糊，LSD算法检测到的每条线都与一个线支撑区域相关联。设ρi表示li的支撑区域的宽度。ρi可以反映li端点的不确定度，进而影响三维端点qi,1和qi,2的不确定度。ρi越大，通常意味着3D端点上的不确定度越高。因此，我们采用1ρi来衡量共线约束。利用公式(4)（<strong>点到线的距离</strong>）和上述符号，我们得到L的共线约束为:<br> <img src="https://images2.imgbox.com/5b/b6/5CmYweab_o.jpg" alt="在这里插入图片描述"></p> 
<p><strong>简单来说，论文中使用的线段提取方法是LSD，而LSD本身存在的问题是受到运动模糊或者取整的影响，线段提取本身存在一定的不确定性，不确定性反应在线段的宽度上。这里个人理解是线段可能并不准确，会由于抖动、模糊等原因，让坐标不准确，也就是说提取出来的一条线段，其周围一定范围可能仍然属于这条线段。基于此，<mark>论文对线段增加了一个支持域，支持域宽度用ρ表示，以此来衡量线段的不确定性，ρ越大，说明线段提取结果周围很大的范围都可能是线段的一部分，所以不确定性大。</mark></strong></p> 
<p><strong>所谓共线约束，实际上就是指一条线段在所有观测到的图像上检测的2d线段，反过来投影其端点应该保持共线，利用这一点，可以写出公式（11）</strong><mark>这个误差就是第三种误差</mark></p> 
<h4><a id="_303"></a>完整的误差公式</h4> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-BKGpKzPa-1668338297621)(EDPLVO.assets/18.jpg)]<img src="https://images2.imgbox.com/75/5c/mnx7iEec_o.jpg" alt="在这里插入图片描述"></p> 
<p><strong>完整的残差公式被定义为上式，可以简单分为三部分：特征点的光度误差、线段的光度误差以及线段的共线约束。</strong></p> 
<h4><a id="_310"></a>基于滑动窗口的优化策略</h4> 
<p>这篇论文采用了VINS同样的<strong>滑动窗口机制，目的是为了提高残差公式的优化效率以及准确性，优化的目标包括位姿和点线坐标</strong>(针对公式12的)。优化的细节论文没有明说，是直接参考了另一篇论文的内容。</p> 
<p><img src="https://images2.imgbox.com/39/0f/qJWh3nVF_o.jpg" alt="在这里插入图片描述"></p> 
<p>一条3D直线和大小为3的滑动窗口的算法示意图(A)和不同方法的Hessian矩阵(b)-(e)。</p> 
<p>从每条2D线上采样4个点。与在DPLVO[2]中所做的一样，活动3D线的不可见点在以下优化中被固定为先验(演示为(a)中的灰点)。</p> 
<ul><li> <p>(b)和©分别演示了我们的两步最小化算法和直接最小化(12)的Hessian矩阵。对于我们的算法，无论从一条2D直线上采样多少个点，都只优化了两个端点的逆深度，因为2D直线上其他点的3D点是由它们决定的，如图3 (a)所示。</p> </li><li> <p>(d)和(e)分别演示了DSO[1]和DPLVO[2]的Hessian矩阵。DSO只考虑各点的光度误差，DPLVO对共线点施加共线约束。这两种方法都为每个像素引入了相反的深度。</p> <p>很明显，我们的<strong>两步优化算法会得到最小的黑森矩阵</strong>。</p> </li></ul> 
<h4><a id="_327"></a>两步优化方法（最小化）</h4> 
<p><img src="https://images2.imgbox.com/0a/ab/awJgXprp_o.jpg" alt="在这里插入图片描述"></p> 
<p>线条扩大了未知的数量，并与点和姿势相关联。因此，联合优化点和姿态线将增加计算量。我们引入了两步最小化方法(12)。</p> 
<ul><li>首先关注(12)的最后一项。点、线和姿势在这个术语中是相关的。给定x相机姿态和二维端点的逆深度，这个问题等价于将3D直线拟合为点集，即:{delil | delil = arg minL EL, L∈L}，其中EL的定义见(11)。</li><li>另一方面，如果在delil中的线参数是固定的，得到的位姿和反深度的代价函数E可以像只考虑光度误差一样有效地最小化。</li></ul> 
<p>我们可以重复这两个步骤。该算法称为两步最小化，在算法1中进行了总结。</p> 
<p><strong>注意，一个Levenberg-Marquardt (LM)步骤可能包括多个迭代，以减少成本。两步最小化算法总是收敛的。</strong></p> 
<p><strong>个人理解是将变量拆成两部分做优化，先优化线的坐标，之后固定线的坐标，优化位姿和逆深度信息。</strong></p> 
<h4><a id="_342"></a>前端</h4> 
<p>这一章论文介绍了3D线段的初始化以及2D线段的合并。</p> 
<p><strong>线段的初始化部分</strong></p> 
<p>由于采用的是直接法，所以对于当前帧并没有进行特征提取的过程，所以使用的是一种跟踪策略，具体来说是先对线段上的点进行采样，每个采样点进行投影，投影得到的点拟合一条2D线段，之后利用两条线段的反向延长曲面，相交得到一个3D线段，从而完成初始化的过程。但是问题在于，这个初始化执行的时机，从这个初始化的过程可以看出，跟踪准确的前提是要一个准确的位姿，否则投影拟合出来的线段是不准确的，但是前面计算位姿的时候明明是有线段的对应关系的，这二者相互冲突了。（具体这篇文章没说怎么给这个初始位姿）</p> 
<p><strong>2D线段的合并</strong></p> 
<p>论文说LSD提取出来的线段有时候许多断掉的小线段，如果直接合并，可能会让本来的两条线段误合并成一条，这显然是不合适的。<br> <img src="https://images2.imgbox.com/93/ea/TIeNyHz5_o.jpg" alt="在这里插入图片描述"></p> 
<p>在本例中，被合并线可能位于梯度较小的地区。<strong>因此，当被合并线的投影重叠或两条线的平均梯度之间的角度很大时，我们不合并两条线</strong>。如图所示，合并线可以通过低梯度区域。因此，原始段被投影到合并的直线上。二维点分别从投影线段中采样。**如果近似平行线彼此过于接近，我们将沿着这条线的梯度的大小相加，并保留累积幅度最大的一条。<strong>我们还尝试将新检测到的线与现有的线合并，以避免引入新的参数</strong>。（具体计算方法没给）</p> 
<p><mark><strong>到此这篇论文基本结束了</strong></mark></p> 
<p>参考文献：<br> <mark>因为论文较新，没有太多可以参考的，可能理解有误，欢迎讨论交流。这里是我参考的两篇文章，大家可以去看看</mark><br> https://blog.csdn.net/weixin_45485946/article/details/125965966<br> https://blog.csdn.net/weixin_43849505/article/details/127719623</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ea49ed8a8bdde9d2680d99fdcbb5e17e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Quartus18.1 lite免费教育版下载及安装</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2c4bbeee5df801e8e6ab11843e856b7a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">为什么科来找不到网络适配器呢</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>