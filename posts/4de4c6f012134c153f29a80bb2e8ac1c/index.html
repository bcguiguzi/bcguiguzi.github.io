<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python入门网络爬虫之精华版，赶快收藏 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python入门网络爬虫之精华版，赶快收藏" />
<meta property="og:description" content="相关文件 关注小编，私信小编领取哟！
当然别忘了一件三连哟~~
公众号：Python日志
前言 Python学习网络爬虫主要分3个大的版块：抓取，分析，存储
另外，比较常用的爬虫框架Scrapy，这里最后也详细介绍一下。
举例子 当我们在浏览器中输入一个url后回车，后台会发生什么？
简单来说这段过程发生了以下四个步骤：
查找域名对应的IP地址。向IP对应的服务器发送请求。服务器响应请求，发回网页内容。浏览器解析网页内容。 网络爬虫要做的，简单来说，就是实现浏览器的功能。通过指定url，直接返回给用户所需要的数据，而不需要一步步人工去操纵浏览器获取。
抓取 这一步，你要明确要得到的内容是什么？是HTML源码，还是Json格式的字符串等。
1. 最基本的抓取 抓取大多数情况属于get请求，即直接从对方服务器上获取数据。
首先，Python中自带urllib及urllib2这两个模块，基本上能满足一般的页面抓取。另外，requests也是非常有用的包，与此类似的，还有httplib2等等。
Requests： import requests response = requests.get(url) content = requests.get(url).content print &#34;response headers:&#34;, response.headers print &#34;content:&#34;, content Urllib2： import urllib2 response = urllib2.urlopen(url) content = urllib2.urlopen(url).read() print &#34;response headers:&#34;, response.headers print &#34;content:&#34;, content Httplib2： import httplib2 http = httplib2.Http() response_headers, content = http.request(url, &#39;GET&#39;) print &#34;response headers:&#34;, response_headers print &#34;content:&#34;, content 此外，对于带有查询字段的url，get请求一般会将来请求的数据附在url之后，以?分割url和传输数据，多个参数用&amp;连接。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/4de4c6f012134c153f29a80bb2e8ac1c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-22T21:46:16+08:00" />
<meta property="article:modified_time" content="2023-05-22T21:46:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python入门网络爬虫之精华版，赶快收藏</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>相关文件</h2> 
<p>关注小编，私信小编领取哟！<br> 当然别忘了一件三连哟~~<br> 公众号：Python日志</p> 
<h2><a id="_4"></a>前言</h2> 
<p>Python学习网络爬虫主要分3个大的版块：抓取，分析，存储<br> 另外，比较常用的爬虫框架Scrapy，这里最后也详细介绍一下。</p> 
<h2><a id="_9"></a>举例子</h2> 
<p>当我们在浏览器中输入一个url后回车，后台会发生什么？<br> 简单来说这段过程发生了以下四个步骤：</p> 
<ul><li>查找域名对应的IP地址。</li><li>向IP对应的服务器发送请求。</li><li>服务器响应请求，发回网页内容。</li><li>浏览器解析网页内容。</li></ul> 
<p>网络爬虫要做的，简单来说，就是实现浏览器的功能。通过指定url，直接返回给用户所需要的数据，而不需要一步步人工去操纵浏览器获取。</p> 
<h2><a id="_20"></a>抓取</h2> 
<p>这一步，你要明确要得到的内容是什么？是HTML源码，还是Json格式的字符串等。</p> 
<h3><a id="1__22"></a>1. 最基本的抓取</h3> 
<p>抓取大多数情况属于get请求，即直接从对方服务器上获取数据。<br> 首先，Python中自带urllib及urllib2这两个模块，基本上能满足一般的页面抓取。另外，requests也是非常有用的包，与此类似的，还有httplib2等等。</p> 
<pre><code class="prism language-python">Requests：
	<span class="token keyword">import</span> requests
	response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
	content <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">.</span>content
	<span class="token keyword">print</span> <span class="token string">"response headers:"</span><span class="token punctuation">,</span> response<span class="token punctuation">.</span>headers
	<span class="token keyword">print</span> <span class="token string">"content:"</span><span class="token punctuation">,</span> content
Urllib2：
	<span class="token keyword">import</span> urllib2
	response <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
	content <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
	<span class="token keyword">print</span> <span class="token string">"response headers:"</span><span class="token punctuation">,</span> response<span class="token punctuation">.</span>headers
	<span class="token keyword">print</span> <span class="token string">"content:"</span><span class="token punctuation">,</span> content
Httplib2：
	<span class="token keyword">import</span> httplib2
	http <span class="token operator">=</span> httplib2<span class="token punctuation">.</span>Http<span class="token punctuation">(</span><span class="token punctuation">)</span>
	response_headers<span class="token punctuation">,</span> content <span class="token operator">=</span> http<span class="token punctuation">.</span>request<span class="token punctuation">(</span>url<span class="token punctuation">,</span> <span class="token string">'GET'</span><span class="token punctuation">)</span>
	<span class="token keyword">print</span> <span class="token string">"response headers:"</span><span class="token punctuation">,</span> response_headers
	<span class="token keyword">print</span> <span class="token string">"content:"</span><span class="token punctuation">,</span> content
</code></pre> 
<p>此外，对于带有查询字段的url，get请求一般会将来请求的数据附在url之后，以?分割url和传输数据，多个参数用&amp;连接。</p> 
<pre><code class="prism language-python">data <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'data1'</span><span class="token punctuation">:</span><span class="token string">'XXXXX'</span><span class="token punctuation">,</span> <span class="token string">'data2'</span><span class="token punctuation">:</span><span class="token string">'XXXXX'</span><span class="token punctuation">}</span>
Requests：data为<span class="token builtin">dict</span>，json
	<span class="token keyword">import</span> requests
	response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> params<span class="token operator">=</span>data<span class="token punctuation">)</span>
Urllib2：data为string
	<span class="token keyword">import</span> urllib<span class="token punctuation">,</span> urllib2    
	data <span class="token operator">=</span> urllib<span class="token punctuation">.</span>urlencode<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
	full_url <span class="token operator">=</span> url<span class="token operator">+</span><span class="token string">'?'</span><span class="token operator">+</span>data
	response <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span>full_url<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="2__59"></a>2. 对于登陆情况的处理</h3> 
<h4><a id="21__60"></a>2.1 使用表单登陆</h4> 
<p>这种情况属于post请求，即先向服务器发送表单数据，服务器再将返回的cookie存入本地。</p> 
<pre><code class="prism language-python">data <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'data1'</span><span class="token punctuation">:</span><span class="token string">'XXXXX'</span><span class="token punctuation">,</span> <span class="token string">'data2'</span><span class="token punctuation">:</span><span class="token string">'XXXXX'</span><span class="token punctuation">}</span>
Requests：data为<span class="token builtin">dict</span>，json
	<span class="token keyword">import</span> requests
	response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> data<span class="token operator">=</span>data<span class="token punctuation">)</span>
Urllib2：data为string
	<span class="token keyword">import</span> urllib<span class="token punctuation">,</span> urllib2    
	data <span class="token operator">=</span> urllib<span class="token punctuation">.</span>urlencode<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
	req <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> data<span class="token operator">=</span>data<span class="token punctuation">)</span>
	response <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span>req<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="22_cookie_74"></a>2.2 使用cookie登陆</h4> 
<p>使用cookie登陆，服务器会认为你是一个已登陆的用户，所以就会返回给你一个已登陆的内容。因此，需要验证码的情况可以使用带验证码登陆的cookie解决。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> requests			
requests_session <span class="token operator">=</span> requests<span class="token punctuation">.</span>session<span class="token punctuation">(</span><span class="token punctuation">)</span> 
response <span class="token operator">=</span> requests_session<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token operator">=</span>url_login<span class="token punctuation">,</span> data<span class="token operator">=</span>data<span class="token punctuation">)</span> 
</code></pre> 
<p>若存在验证码，此时采用response = requests_session.post(url=url_login, data=data)是不行的，做法应该如下：</p> 
<pre><code class="prism language-python">response_captcha <span class="token operator">=</span> requests_session<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token operator">=</span>url_login<span class="token punctuation">,</span> cookies<span class="token operator">=</span>cookies<span class="token punctuation">)</span>
response1 <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url_login<span class="token punctuation">)</span> <span class="token comment"># 未登陆</span>
response2 <span class="token operator">=</span> requests_session<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url_login<span class="token punctuation">)</span> <span class="token comment"># 已登陆，因为之前拿到了Response Cookie！</span>
response3 <span class="token operator">=</span> requests_session<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url_results<span class="token punctuation">)</span> <span class="token comment"># 已登陆，因为之前拿到了Response Cookie！</span>
</code></pre> 
<h3><a id="3__90"></a>3. 对于反爬虫机制的处理</h3> 
<h4><a id="31__91"></a>3.1 使用代理</h4> 
<p>适用情况：限制IP地址情况，也可解决由于“频繁点击”而需要输入验证码登陆的情况。</p> 
<p>这种情况最好的办法就是维护一个代理IP池，网上有很多免费的代理IP，良莠不齐，可以通过筛选找到能用的。对于“频繁点击”的情况，我们还可以通过限制爬虫访问网站的频率来避免被网站禁掉。</p> 
<pre><code class="prism language-python">proxies <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'http'</span><span class="token punctuation">:</span><span class="token string">'http://XX.XX.XX.XX:XXXX'</span><span class="token punctuation">}</span>
Requests：
	<span class="token keyword">import</span> requests
	response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> proxies<span class="token operator">=</span>proxies<span class="token punctuation">)</span>
Urllib2：
	<span class="token keyword">import</span> urllib2
	proxy_support <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>ProxyHandler<span class="token punctuation">(</span>proxies<span class="token punctuation">)</span>
	opener <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>build_opener<span class="token punctuation">(</span>proxy_support<span class="token punctuation">,</span> urllib2<span class="token punctuation">.</span>HTTPHandler<span class="token punctuation">)</span>
	urllib2<span class="token punctuation">.</span>install_opener<span class="token punctuation">(</span>opener<span class="token punctuation">)</span> <span class="token comment"># 安装opener，此后调用urlopen()时都会使用安装过的opener对象</span>
	response <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="32__108"></a>3.2 时间设置</h4> 
<p>适用情况：限制频率情况。</p> 
<p>Requests，Urllib2都可以使用time库的sleep()函数：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> time
time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="33__118"></a>3.3 伪装成浏览器，或者反“反盗链”</h4> 
<p>有些网站会检查你是不是真的浏览器访问，还是机器自动访问的。这种情况，加上User-Agent，表明你是浏览器访问即可。有时还会检查是否带Referer信息还会检查你的Referer是否合法，一般再加上Referer。</p> 
<pre><code class="prism language-python">headers <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'User-Agent'</span><span class="token punctuation">:</span><span class="token string">'XXXXX'</span><span class="token punctuation">}</span> <span class="token comment"># 伪装成浏览器访问，适用于拒绝爬虫的网站</span>
headers <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'Referer'</span><span class="token punctuation">:</span><span class="token string">'XXXXX'</span><span class="token punctuation">}</span>
headers <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'User-Agent'</span><span class="token punctuation">:</span><span class="token string">'XXXXX'</span><span class="token punctuation">,</span> <span class="token string">'Referer'</span><span class="token punctuation">:</span><span class="token string">'XXXXX'</span><span class="token punctuation">}</span>
Requests：
	response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">)</span>
Urllib2：
	<span class="token keyword">import</span> urllib<span class="token punctuation">,</span> urllib2   
	req <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">)</span>
	response <span class="token operator">=</span> urllib2<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span>req<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="4__133"></a>4. 对于断线重连</h3> 
<p>不多说。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">multi_session</span><span class="token punctuation">(</span>session<span class="token punctuation">,</span> <span class="token operator">*</span>arg<span class="token punctuation">)</span><span class="token punctuation">:</span>
	retryTimes <span class="token operator">=</span> <span class="token number">20</span>
	<span class="token keyword">while</span> retryTimes<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">:</span>
		<span class="token keyword">try</span><span class="token punctuation">:</span>
			<span class="token keyword">return</span> session<span class="token punctuation">.</span>post<span class="token punctuation">(</span><span class="token operator">*</span>arg<span class="token punctuation">)</span>
		<span class="token keyword">except</span><span class="token punctuation">:</span>
			<span class="token keyword">print</span> <span class="token string">'.'</span><span class="token punctuation">,</span>
			retryTimes <span class="token operator">-=</span> <span class="token number">1</span>
</code></pre> 
<p>或者</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">multi_open</span><span class="token punctuation">(</span>opener<span class="token punctuation">,</span> <span class="token operator">*</span>arg<span class="token punctuation">)</span><span class="token punctuation">:</span>
	retryTimes <span class="token operator">=</span> <span class="token number">20</span>
	<span class="token keyword">while</span> retryTimes<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">:</span>
		<span class="token keyword">try</span><span class="token punctuation">:</span>
			<span class="token keyword">return</span> opener<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token operator">*</span>arg<span class="token punctuation">)</span>
		<span class="token keyword">except</span><span class="token punctuation">:</span>
			<span class="token keyword">print</span> <span class="token string">'.'</span><span class="token punctuation">,</span>
			retryTimes <span class="token operator">-=</span> <span class="token number">1</span>
</code></pre> 
<p>这样我们就可以使用multi_session或multi_open对爬虫抓取的session或opener进行保持。</p> 
<h3><a id="5_Ajax_160"></a>5. 对于Ajax请求的处理</h3> 
<p>对于“加载更多”情况，使用Ajax来传输很多数据。</p> 
<p>它的工作原理是：从网页的url加载网页的源代码之后，会在浏览器里执行JavaScript程序。这些程序会加载更多的内容，“填充”到网页里。这就是为什么如果你直接去爬网页本身的url，你会找不到页面的实际内容。</p> 
<p>这里，若使用Google Chrome分析”请求“对应的链接(方法：右键→审查元素→Network→清空，点击”加载更多“，出现对应的GET链接寻找Type为text/html的，点击，查看get参数或者复制Request URL)，循环过程。</p> 
<ul><li>如果“请求”之前有页面，依据上一步的网址进行分析推导第1页。以此类推，抓取抓Ajax地址的数据。</li><li>对返回的json格式数据(str)进行正则匹配。json格式数据中，需从’\uxxxx’形式的unicode_escape编码转换成u’\uxxxx’的unicode编码。</li></ul> 
<h3><a id="6_Selenium_170"></a>6. 自动化测试工具Selenium</h3> 
<p>Selenium是一款自动化测试工具。它能实现操纵浏览器，包括字符填充、鼠标点击、获取元素、页面切换等一系列操作。总之，凡是浏览器能做的事，Selenium都能够做到。</p> 
<h3><a id="7__172"></a>7. 验证码识别</h3> 
<p>对于网站有验证码的情况，我们有三种办法：</p> 
<ul><li>使用代理，更新IP。</li><li>使用cookie登陆。</li><li>验证码识别。</li></ul> 
<p>使用代理和使用cookie登陆之前已经讲过，下面讲一下验证码识别。</p> 
<p>可以利用开源的Tesseract-OCR系统进行验证码图片的下载及识别，将识别的字符传到爬虫系统进行模拟登陆。当然也可以将验证码图片上传到打码平台上进行识别。如果不成功，可以再次更新验证码识别，直到成功为止。</p> 
<p>爬取有两个需要注意的问题：</p> 
<ul><li>如何监控一系列网站的更新情况，也就是说，如何进行增量式爬取？</li><li>对于海量数据，如何实现分布式爬取？</li></ul> 
<h2><a id="_188"></a>分析</h2> 
<p>抓取之后就是对抓取的内容进行分析，你需要什么内容，就从中提炼出相关的内容来。<br> 常见的分析工具有正则表达式，BeautifulSoup，lxml等等。</p> 
<h2><a id="_192"></a>存储</h2> 
<p>分析出我们需要的内容之后，接下来就是存储了。</p> 
<p>我们可以选择存入文本文件，也可以选择存入MySQL或MongoDB数据库等。</p> 
<p>存储有两个需要注意的问题：</p> 
<ul><li>如何进行网页去重？</li><li>内容以什么形式存储？</li></ul> 
<h2><a id="Scrapy_202"></a>Scrapy</h2> 
<p>Scrapy是一个基于Twisted的开源的Python爬虫框架，在工业中应用非常广泛。</p> 
<h2><a id="Robots_205"></a>Robots协议</h2> 
<p>好的网络爬虫，首先需要遵守Robots协议。Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。</p> 
<p>在网站根目录下放一个robots.txt文本文件（如 https://www.taobao.com/robots.txt ），里面可以指定不同的网络爬虫能访问的页面和禁止访问的页面，指定的页面由正则表达式表示。网络爬虫在采集这个网站之前，首先获取到这个robots.txt文本文件，然后解析到其中的规则，然后根据规则来采集网站的数据。</p> 
<h3><a id="1_Robots_210"></a>1. Robots协议规则</h3> 
<pre><code class="prism language-python">User<span class="token operator">-</span>agent<span class="token punctuation">:</span> 指定对哪些爬虫生效
Disallow<span class="token punctuation">:</span> 指定不允许访问的网址
Allow<span class="token punctuation">:</span> 指定允许访问的网址
</code></pre> 
<p>注意: 一个英文要大写，冒号是英文状态下，冒号后面有一个空格，"/"代表整个网站</p> 
<h3><a id="2_Robots_218"></a>2. Robots协议举例</h3> 
<pre><code class="prism language-python">禁止所有机器人访问
	User<span class="token operator">-</span>agent<span class="token punctuation">:</span> <span class="token operator">*</span>
	Disallow<span class="token punctuation">:</span> <span class="token operator">/</span>
允许所有机器人访问
	User<span class="token operator">-</span>agent<span class="token punctuation">:</span> <span class="token operator">*</span>
	Disallow<span class="token punctuation">:</span> 
禁止特定机器人访问
	User<span class="token operator">-</span>agent<span class="token punctuation">:</span> BadBot
	Disallow<span class="token punctuation">:</span> <span class="token operator">/</span>
允许特定机器人访问
	User<span class="token operator">-</span>agent<span class="token punctuation">:</span> GoodBot
	Disallow<span class="token punctuation">:</span> 
禁止访问特定目录
	User<span class="token operator">-</span>agent<span class="token punctuation">:</span> <span class="token operator">*</span>
	Disallow<span class="token punctuation">:</span> <span class="token operator">/</span>images<span class="token operator">/</span>
仅允许访问特定目录
	User<span class="token operator">-</span>agent<span class="token punctuation">:</span> <span class="token operator">*</span>
	Allow<span class="token punctuation">:</span> <span class="token operator">/</span>images<span class="token operator">/</span>
	Disallow<span class="token punctuation">:</span> <span class="token operator">/</span>
禁止访问特定文件
	User<span class="token operator">-</span>agent<span class="token punctuation">:</span> <span class="token operator">*</span>
	Disallow<span class="token punctuation">:</span> <span class="token operator">/</span><span class="token operator">*</span><span class="token punctuation">.</span>html$
仅允许访问特定文件
	User<span class="token operator">-</span>agent<span class="token punctuation">:</span> <span class="token operator">*</span>
	Allow<span class="token punctuation">:</span> <span class="token operator">/</span><span class="token operator">*</span><span class="token punctuation">.</span>html$
	Disallow<span class="token punctuation">:</span> <span class="token operator">/</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e72197965b222e2dfaa15648a658eb2c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">TensorFlow 实现车牌识别</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1279ae45bab9660ae34a75ee516e29e6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端页面用户选择静态资源布置的图片，想拿url却拿到base64编码？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>