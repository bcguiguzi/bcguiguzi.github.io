<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>了解语言模型Model Language，NLP必备 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="了解语言模型Model Language，NLP必备" />
<meta property="og:description" content="目录
目录
1. 统计语言模型
2. n-gram 模型
2.1 n-gram语言模型的稀疏性问题
2.2 n-gram 语言模型的存储问题
3. 基于窗口的神经语言模型
4. 语言模型的评估指标:困惑度
语言模型是预测接下来出现什么词的任务。理论上说，您还可以将语言模型视为为一段文本分配概率的系统。 语言模型的预测任务 实践上说，您每天都在使用语言模型。当你在网页搜索上输入的文字时，当你输入今天，手机就会自动计算概率分布，并且弹出 是什么日子，是农历几月几日单词等等供你选择。
还有你的输入法也会根据目前你输入的字计算概率，有多个选择供你选。
1. 统计语言模型 统计语言模型(Statistical Language Model)是所有 NLP 的基础。在语音识别系统中,对于给定的语音段 Voice,需要找到一个使概率 p(Text|Voice)最大的文本段 Text，利用 Bayes 公式，有:
p(Voice|Text)为声学模型，p(Voice|Text)为语言模型。
统计语言模型是用来计算一个句子的概率的概率模型。假设 = ，示由 T 个词 按顺序构成的一个句子,则的联合概率为:
就是这个句子的概率。利用 Bayes 公式，上式可以被链式地分解为:
其中的(条件)概率 就是语言模型的参数。
参数 的近似计算，利用 Bayes 公式，有:
其中 和 分别为单词 和在语料中出现的次数。
2. n-gram 模型 模型的基本思想：基于Markov,一个词出现的概率只与它前面n-1个词相关，即：
这一简化不仅使单个参数的统计变得更容易(统计时需要匹配的词串更短)，也使参数的总数变少了。
模型参数的量级是 N 的指数函数，显然 n 不能取得太大，实用中最多的是采用 n=3 的三元模型。 从理论上看，n 越大效果越好。但是，当 n 大到一定程度时，模型效果的提升幅度会变小。例如，当 n 从 1 到 2，再从 2 到 3 时，模型的效果上升显著，而从 3 到 4 时，效果的提升就不显著了。参数越多，可区别性越好，但同时单个参数的实例变少从而降低了可靠性，因此需要在可靠性和可区别性之间进行折中。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/b33c74b4769581cee53ccd591ef87df2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-23T20:43:34+08:00" />
<meta property="article:modified_time" content="2022-11-23T20:43:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">了解语言模型Model Language，NLP必备</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="1.%20%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;"><a href="#1.%20%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" rel="nofollow">1. 统计语言模型</a></p> 
<p id="2.%20n-gram%20%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;"><a href="#2.%20n-gram%20%E6%A8%A1%E5%9E%8B" rel="nofollow">2. n-gram 模型</a></p> 
<p id="2.1%20n-gram%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%A8%80%E7%96%8F%E6%80%A7%E9%97%AE%E9%A2%98-toc" style="margin-left:80px;"><a href="#2.1%20n-gram%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%A8%80%E7%96%8F%E6%80%A7%E9%97%AE%E9%A2%98" rel="nofollow">2.1 n-gram语言模型的稀疏性问题</a></p> 
<p id="2.2%C2%A0n-gram%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AD%98%E5%82%A8%E9%97%AE%E9%A2%98-toc" style="margin-left:80px;"><a href="#2.2%C2%A0n-gram%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AD%98%E5%82%A8%E9%97%AE%E9%A2%98" rel="nofollow">2.2 n-gram 语言模型的存储问题</a></p> 
<p id="3.%20%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;"><a href="#3.%20%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" rel="nofollow">3. 基于窗口的神经语言模型</a></p> 
<p id="4.%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%3A%E5%9B%B0%E6%83%91%E5%BA%A6-toc" style="margin-left:40px;"><a href="#4.%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%3A%E5%9B%B0%E6%83%91%E5%BA%A6" rel="nofollow">4. 语言模型的评估指标:困惑度</a></p> 
<hr id="hr-toc"> 
<pre>语言模型是预测接下来出现什么词的任务。理论上说，您还可以将语言模型视为为一段文本分配概率的系统。
</pre> 
<figure class="image"> 
 <img alt="" src="https://images2.imgbox.com/1c/aa/CroEHnfI_o.png"> 
 <figcaption>
   语言模型的预测任务 
 </figcaption> 
</figure> 
<p>实践上说，您每天都在使用语言模型。当你在网页搜索上输入的文字时，当你输入今天，手机就会自动计算概率分布，并且弹出 是什么日子，是农历几月几日单词等等供你选择。</p> 
<p>还有你的输入法也会根据目前你输入的字计算概率，有多个选择供你选。</p> 
<p><img alt="" src="https://images2.imgbox.com/74/b9/nXqT3iEa_o.png"></p> 
<p></p> 
<p></p> 
<p></p> 
<h3 id="1.%20%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">1. 统计语言模型</h3> 
<p>统计语言模型(Statistical Language Model)是所有 NLP 的基础。在语音识别系统中,对于给定的语音段 Voice,需要找到一个使概率 p(Text|Voice)最大的文本段 Text，利用 Bayes 公式，有:</p> 
<p><img alt="p(Text|Voice) = \frac {p(Voice|Text)p(Text)} {p(Voice) }" class="mathcode" src="https://images2.imgbox.com/04/c4/VWMEK2Ic_o.png"></p> 
<p>p(Voice|Text)为<strong>声学模型</strong>，p(Voice|Text)为<strong>语言模型</strong>。</p> 
<p>统计语言模型是用来计算一个句子的概率的概率模型。假设 <img alt="W = w_1^T" class="mathcode" src="https://images2.imgbox.com/86/86/2tgBghjF_o.png">= <img alt="w_1, w_2, \ldots, w_T" class="mathcode" src="https://images2.imgbox.com/79/c5/1yEPS8TR_o.png">，示由 T 个词 <img alt="w_1, w_2, \ldots, w_T" class="mathcode" src="https://images2.imgbox.com/91/76/mUUWe0Xk_o.png">按顺序构成的一个句子,则<img alt="w_1, w_2, \ldots, w_T" class="mathcode" src="https://images2.imgbox.com/62/98/2tS0CNBn_o.png">的<span style="color:#1a439c;">联合概率</span>为:</p> 
<p><img alt="p(W) =P(w_1^T)=" class="mathcode" src="https://images2.imgbox.com/f0/03/WMdQ9xUJ_o.png"> <img alt="P(w_1, w_2, \ldots, w_T)" class="mathcode" src="https://images2.imgbox.com/95/db/nsdFITT5_o.png"></p> 
<p>就是这个句子的概率。利用 Bayes 公式，上式可以被链式地分解为:</p> 
<p><img alt="P(W)=P(w_1^T) = P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1^2) \cdots P(w_T \mid w_1^{T-1})" class="mathcode" src="https://images2.imgbox.com/82/8c/zfKaSBa9_o.png"></p> 
<p>其中的(条件)概率<img alt="P(w_1), P(w_2 \mid w_1) ,P(w_3 \mid w_1^2) ,\cdots, P(w_T \mid w_1^{T-1})" class="mathcode" src="https://images2.imgbox.com/2c/4a/dWiDD0ck_o.png"> 就是语言模型的参数。</p> 
<p>参数 <img alt="P(w_k \mid w_1^{k-1})(k&gt;1)" class="mathcode" src="https://images2.imgbox.com/78/cf/5F6BemC3_o.png"> 的近似计算，利用 <span style="color:#1a439c;">Bayes 公式</span>，有:</p> 
<p><img alt="P(w_k \mid w_1^{k-1})= \frac {P(w_1^k )}{ P(w_1^{k-1}) }\approx \frac {count(w_1^k )}{count (w_1^{k-1})}" class="mathcode" src="https://images2.imgbox.com/3d/6d/7QWmX5tf_o.png"></p> 
<p>其中<img alt="{count(w_1^k )}" class="mathcode" src="https://images2.imgbox.com/09/c2/C341E0TG_o.png"> 和<img alt="{count(w_1^{k-1 })}" class="mathcode" src="https://images2.imgbox.com/82/9e/9KcafyDI_o.png"> 分别为单词 <img alt="w_1^{k }" class="mathcode" src="https://images2.imgbox.com/14/ab/rDUh3Wjr_o.png">和<img alt="w_1^{k-1 }" class="mathcode" src="https://images2.imgbox.com/17/31/H001vBZc_o.png">在语料中出现的次数。</p> 
<p></p> 
<h3 id="2.%20n-gram%20%E6%A8%A1%E5%9E%8B">2. n-gram 模型</h3> 
<p>模型的基本思想：<span style="color:#1a439c;">基于Markov,一个词出现的概率只与它前面n-1个词相关</span>，即：</p> 
<p><img alt="P(w_k \mid w_1^{k-1}) \approx P(w_k \mid w_{k-n+1}^{k-1}) \approx \frac {count(w_{k-n+1}^{k} )}{count (w_{k-n+1}^{k-1})}" class="mathcode" src="https://images2.imgbox.com/4e/91/FhmO7LDW_o.png"></p> 
<p>这一简化不仅使单个参数的统计变得更容易(统计时需要匹配的词串更短)，也使参数的总数变少了。</p> 
<p>模型参数的量级是 N 的指数函数<img alt="O(N^n)" class="mathcode" src="https://images2.imgbox.com/23/27/zfxYe5Q8_o.png">，显然 n 不能取得太大，<span style="color:#1a439c;">实用中最多的是采用 n=3 的三元模型</span>。 从理论上看，n 越大效果越好。但是，当 n 大到一定程度时，模型效果的提升幅度会变小。例如，当 n 从 1 到 2，再从 2 到 3 时，模型的效果上升显著，而从 3 到 4 时，效果的提升就不显著了。参数越多，可区别性越好，但同时单个参数的实例变少从而降低了可靠性，因此需要在可靠性和可区别性之间进行折中。</p> 
<p></p> 
<p><strong>例如</strong>，我们学习一个4-gram语言模型：</p> 
<figure class="image"> 
 <img alt="" src="https://images2.imgbox.com/ef/26/0m09BTBk_o.png"> 
 <figcaption>
   4-gram 语言模型的一个例子 
 </figcaption> 
</figure> 
<p style="text-align:center;"></p> 
<p>假设在语料库中“students opened their”发生了 1000 次，</p> 
<p>若:“students opened their <span style="color:#1a439c;">books</span>”发生了 400 次，则:P(<span style="color:#1a439c;">books</span> | students opened their) = 0.4;</p> 
<p>若:“students opened their <span style="color:#1a439c;">exams</span>”发生了 100 次，则:P(<span style="color:#1a439c;">exams </span>| students opened their)= 0.1;</p> 
<p>但是</p> 
<ol><li>若在语料库中“students opened their w ”从未出现在数据中怎么办？也就是<strong><strong><img alt="count (w_{k-n+1}^{k}) = 0" class="mathcode" height="19" src="https://images2.imgbox.com/01/8e/Ff27N5EH_o.png" width="121"></strong></strong>，就认为<strong><strong><img alt="P(w_k \mid w_1^{k-1})" class="mathcode" height="18" src="https://images2.imgbox.com/68/1f/m5KJbhaB_o.png" width="87"></strong></strong>概率为零吗？</li><li>若“students opened their”未出现在数据中那有怎么办？也就是<img alt="" height="15" src="https://images2.imgbox.com/53/4f/qClyVarV_o.png" width="74">=0。 </li><li>若在语料库中“students opened their w ”出现的次数和和“students opened their”出现的次数相同，也就是<strong><strong><img alt="count (w_{k-n+1}^{k}) = count (w_{k-n+1}^{k-1})" class="mathcode" height="15" src="https://images2.imgbox.com/2e/1b/VTpiNw3r_o.png" width="155"></strong></strong>，那么能否认为<strong><strong><img alt="P(w_k \mid w_1^{k-1})" class="mathcode" height="15" src="https://images2.imgbox.com/8f/55/0oZq1uHv_o.png" width="71"></strong></strong>就等于 1 呢?</li></ol> 
<p>那么，在这种情况下，我们应该丢弃“proctor”的上下文(图中划红线部分)吗? 这就引出了 n-gram 语言模型的两个主要问题:稀疏性和存储。</p> 
<p></p> 
<h4 id="2.1%20n-gram%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%A8%80%E7%96%8F%E6%80%A7%E9%97%AE%E9%A2%98">2.1 n-gram语言模型的稀疏性问题</h4> 
<p>些模型的稀疏性问题是由两个情况引起的</p> 
<p style="text-align:center;"><img alt="" height="291" src="https://images2.imgbox.com/6d/68/lw3hS9PN_o.png" width="657"></p> 
<p></p> 
<p></p> 
<h4 id="2.2%C2%A0n-gram%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AD%98%E5%82%A8%E9%97%AE%E9%A2%98">2.2 n-gram 语言模型的存储问题</h4> 
<p><img alt="" height="200" src="https://images2.imgbox.com/1b/15/4TWx5rqZ_o.png" width="581"></p> 
<p>我们知道需要存储在语料库中看到的所有 n-gram 的统计数。随着 n 的增加(或语料库大小的增加)， 模型的大小也会增加。</p> 
<p></p> 
<h3 id="3.%20%E7%A5%9E%E7%BB%8F%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">3. 基于窗口的神经语言模型</h3> 
<p>在机器学习中有一种通用模型:对所考虑的问题建模后，首先为其构造一个目标函数，其次对这个目 标函数进行优化，然后求得一组最优的参数，最后利用这组最优参数对应的模型来进行预测。</p> 
<p><img alt="L = \sum_{w\in C} log p(w|Context (w)) (3.1)" class="mathcode" src="https://images2.imgbox.com/da/53/LooIVNNH_o.png"></p> 
<p>由此可见，概率<img alt="p(w|Context (w)" class="mathcode" src="https://images2.imgbox.com/ff/25/Tlx7fYnk_o.png">已被视为关于w和context(w)的函数：</p> 
<p><img alt="p(w\mid Context (w)) = F(w, Context (w), \theta )(3.2)" class="mathcode" src="https://images2.imgbox.com/c3/d3/m8Ylevo5_o.png"></p> 
<p>其中:θ为待定参数集。因此，一旦对公式3.1进行优化得到最优参数集θ*后，F 也就唯一被确定了， 以后任何概率 p(w|Context(w))就可以通过函数 F(w，Context(w)，θ)来计算了。</p> 
<p>下面介绍一种通过神经网络来构造 F 的方法，即 Bengio《A neural probabilistic language model. Journal of Machine Learning Research》(2003)的神经概率语言模型。其中首次解决了上面所说的“<span style="color:#1a439c;">维度灾难</span>”，这篇论 文提出一个自然语言处理的大规模的深度学习模型，这个模型能够通过学习单词的分布式表示，以及用这些表示来表示单词的概率函数。图3.1 给出了该模型的示意图，它包括四个层: 输入(Input)层、投影(Projection)层、隐藏(Hidden)层和输出(Output)层。</p> 
<figure class="image"> 
 <img alt="" height="320" src="https://images2.imgbox.com/d6/99/z8d19rGO_o.png" width="674"> 
 <figcaption>
   图3.1：首个 NLP 神经网络架构模型(Bengio 模型)             图3.2:对 Bengio 模型的简化表示 
 </figcaption> 
</figure> 
<p style="text-align:center;"></p> 
<p>这个模型的简化版本如图3.2所示，其中蓝色的层表示输入单词的embedding拼接:e=[e(1);e(2);e(3);e(4)]， 红色的层表示隐藏层:<img alt="h=f(We+b_1)" class="mathcode" src="https://images2.imgbox.com/40/82/wLZkSYdj_o.png">，绿色的输出分布是对词表的一个 softmax 概率分布:<img alt="y = softmax(Uh+b_2)" class="mathcode" src="https://images2.imgbox.com/f7/11/ZPMgWrX9_o.png"></p> 
<p></p> 
<p>例如构建一个基于固定窗口的神经语言模型。 首先，了解一下固定窗口的案例，见图:</p> 
<figure class="image"> 
 <img alt="" src="https://images2.imgbox.com/b1/e0/ZP2jySte_o.png"> 
 <figcaption>
   固定窗口案例 
 </figcaption> 
</figure> 
<p> 然后，建立一个固定窗口神经语言模型,如图：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a0/7c/Vzyz0mFX_o.png"></p> 
<p></p> 
<p>Bengio 模型对 n-gram LM 的改进:</p> 
<ul><li> 没有稀疏问题。</li><li> 没有存储问题:不需要存储所有观察到的 n-gram。</li></ul> 
<p>遗留问题:</p> 
<ul><li> 固定窗口太小</li><li> 放大窗口放大 W</li><li> 窗口永远不够大!</li><li>x(1) 和 x(2) 乘以 W 中完全不同的权重。输入的处理方式不对称。</li></ul> 
<pre>因此，我们需要一个可以处理任何长度输入的神经架构。
</pre> 
<h3 id="4.%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%3A%E5%9B%B0%E6%83%91%E5%BA%A6">4. 语言模型的评估指标:困惑度</h3> 
<p><img alt="" height="113" src="https://images2.imgbox.com/03/ff/J2szSa8Y_o.png" width="663"></p> 
<p> 其中分母项（<img alt="P_{LM}" class="mathcode" src="https://images2.imgbox.com/e9/e9/OawVRBDe_o.png">）是指在语言模型在语料库中的逆概率。指数项（1/T）是指用单词数量做归一化。最后的式子中，表示困惑度等于交叉熵损失J(θ)的指数。</p> 
<p>语言模型的困惑度越低越好！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/72e9ef4c4dbe552b9d94f0a29f145dd4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android Studio 中添加图片为什么不出来解决方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b16ee4179d064cc0e9da77affe480620/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">用js实现滚动加载动画效果</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>