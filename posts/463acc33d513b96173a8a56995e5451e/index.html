<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>半监督深度学习小结：类协同训练和一致性正则化 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="半监督深度学习小结：类协同训练和一致性正则化" />
<meta property="og:description" content="作者丨陈家铭
学校丨中山大学硕士生
研究方向丨半监督深度图像分类
协同训练 Co-training Co-training 是基于分歧的方法，其假设每个数据可以从不同的角度（view）进行分类，不同角度可以训练出不同的分类器，然后用这些从不同角度训练出来的分类器对无标签样本进行分类，再选出认为可信的无标签样本加入训练集中。
由于这些分类器从不同角度训练出来的，可以形成一种互补，而提高分类精度；就如同从不同角度可以更好地理解事物一样。
该方法虽然理论很强，但却有一定的假设条件（事实上半监督学习都是在一定的假设条件下进行的），引用周志华大大《基于分歧的半监督学习》中的描述：
协同训练法要求数据具有两个充分冗余且满足条件独立性的视图：
1. “充分（Sufficient）” 是指每个视图都包含足够产生最优学习器的信息, 此时对其中任一视图来说，另一个视图则是“冗余（Redundant）” 的；
2. 对类别标记来说这两个视图条件独立。
这个数据假设就很强力了，既要求数据信息充分还冗余，你还要找到两个独立互补的视图。但是，在一定程度上满足条件的情况下，co-training 的效果也是非常给力。 那么，在半监督深度学习里，co-training 会以什么方式呈现呢？问题的关键自然在于，如何去构建两个（或多个）近似代表充分独立的视图的深度模型，两个比较直观的方法就是：
使用不同的网络架构（据我看过的论文 [1] [2] 中指出，哪怕是对同一个数据集，不同的网络架构学习到的预测分布是不一样的）；
使用特殊的训练方法来得到多样化的深度模型。
注：以下工作的推荐，是我自认为其思想类似于 co-training，因此称为类协同训练，不是说下面的工作就一定是 co-training。
这是一篇 ECCV 2018 的文章，论文首先指出，直接对同一个数据集训练两个网络，会有两个弊端：
对同一个数据集训练两个网络，并不能保证两个网络具有不同的视图，更不能保证具有不同且互补的信息；
协同训练会使得两个网络在训练过程中趋于一致，会导致 collapsed neural networks，进而使得协同训练失效。
为了解决上述问题，论文主要做了两个工作：
1. 提出了一个新的代价函数，进行协同训练，其形式如下：
信息论学得不好请多见谅，大概就是均匀分布的熵最大，当两个预测分布不一致时，这两个预测分布求和取平均会使得熵增大。相反，如果预测一致熵就不会增加多少。
论文中还明确强调，该代价函数只用在无标签数据上，因为有标签数据的监督代价函数（论文用的交叉熵）已经使得预测趋于一致（趋于真实标签），用在有标签数据上没有必要，但是好像没说明也用在有标签数据上的后果。
2. （关键）提出了 View Difference Constraint。其思路是：我们只有一个数据集 D，但我们不能再同一个数据集上训练两个网络，因此需要从 D 中派生出另一个数据集 D&#39;，而这个派生方法就是计算 D 的对抗样本。具体即使用对方的对抗样本来训练自己：
其中， g1(x) 表示网络 p1 的对抗样本， H(·) 是某种代价函数（KL 散度），该约束的设计期望是使得两个网络具备不同却互补信息。我个人觉得这是一个 very amazing 的想法。尽管我不知道理论上是否能保证两个网络不同却互补，但直观上……
实验效果非常好，两个（或多个）网络使用的是同一种架构，就是感觉没有和集成方法比较感觉有点遗憾，毕竟论文用了多个网络。
Tri-net 是 IJCAI 2018 的论文，挂着周志华大大的名字，用的思想也是周志华提出来的 tri-training，可谓阵容豪华。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/463acc33d513b96173a8a56995e5451e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-12-24T12:44:53+08:00" />
<meta property="article:modified_time" content="2018-12-24T12:44:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">半监督深度学习小结：类协同训练和一致性正则化</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="rich_media_content" id="js_content"> 
 <p style="margin-left:.5em;text-align:center;"><img src="https://images2.imgbox.com/cc/8c/uYlWmi4m_o.gif" alt="640"></p> 
 <p style="margin-left:0em;background-color:rgb(255,255,255);letter-spacing:.5px;"><span style="color:rgb(133,118,106);font-size:12.6316px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;background-color:rgb(255,255,255);letter-spacing:.5px;"><span style="color:rgb(133,118,106);font-size:12.6316px;text-align:justify;">作者丨陈家铭</span></p> 
 <p style="margin-left:.5em;background-color:rgb(255,255,255);letter-spacing:.5px;"><span style="color:rgb(133,118,106);font-size:12.6316px;text-align:justify;">学校丨中山大学硕士生</span></p> 
 <p style="margin-left:.5em;background-color:rgb(255,255,255);letter-spacing:.5px;"><span style="color:rgb(133,118,106);font-size:12.6316px;text-align:justify;">研究方向丨半监督深度图像分类</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:normal;"><br></p> 
 <h2 style="font-weight:bold;background-color:rgb(255,255,255);line-height:1.2;border-left-color:rgb(16,142,233);font-size:20px;border-left-width:6px;border-left-style:solid;letter-spacing:1px;word-spacing:1px;"><span style="letter-spacing:.5px;">协同训练 Co-training</span></h2> 
 <p style="margin-left:.5em;text-align:justify;line-height:normal;"><br></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">Co-training 是基于分歧的方法，其假设每个数据可以从不同的角度（view）进行分类，不同角度可以训练出不同的分类器，然后用这些从不同角度训练出来的分类器对无标签样本进行分类，再选出认为可信的无标签样本加入训练集中。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">由于这些分类器从不同角度训练出来的，可以形成一种互补，而提高分类精度；就如同从不同角度可以更好地理解事物一样。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">该方法虽然理论很强，但却有一定的假设条件（事实上半监督学习都是在一定的假设条件下进行的），引用周志华大大《基于分歧的半监督学习》中的描述：</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <blockquote> 
  <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="font-size:12px;"><strong><span style="letter-spacing:.5px;color:rgb(136,136,136);">协同训练法要求数据具有两个充分冗余且满足条件独立性的视图：</span></strong></span></p> 
  <p style="margin-left:.5em;text-align:justify;line-height:normal;"><span style="letter-spacing:.5px;color:rgb(136,136,136);font-size:12px;"><br></span></p> 
  <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="letter-spacing:.5px;color:rgb(136,136,136);font-size:12px;">1. “充分（Sufficient）” 是指每个视图都包含足够产生最优学习器的信息, 此时对其中任一视图来说，另一个视图则是“冗余（Redundant）” 的；</span></p> 
  <p style="margin-left:.5em;text-align:justify;line-height:normal;"><span style="letter-spacing:.5px;color:rgb(136,136,136);font-size:12px;"><br></span></p> 
  <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="letter-spacing:.5px;color:rgb(136,136,136);font-size:12px;">2. 对类别标记来说这两个视图条件独立。</span></p> 
 </blockquote> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><br></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">这个数据假设就很强力了，既要求数据信息充分还冗余，你还要找到两个独立互补的视图。但是，在一定程度上满足条件的情况下，co-training 的效果也是非常给力。 </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">那么，在半监督深度学习里，co-training 会以什么方式呈现呢？问题的关键自然在于，如何去构建两个（或多个）<strong>近似</strong>代表充分独立的视图的深度模型，两个比较直观的方法就是：</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <ul class="list-paddingleft-2" style="list-style-type:disc;"><li><p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">使用不同的网络架构（据我看过的论文 </span><span style="font-size:15px;letter-spacing:.5px;color:rgb(136,136,136);">[1] [2]</span><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"> 中指出，哪怕是对同一个数据集，不同的网络架构学习到的预测分布是不一样的）；</span></p><p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"></span></p></li><li><p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">使用特殊的训练方法来得到多样化的深度模型。</span></p></li></ul> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">注：以下工作的推荐，是我自认为其思想类似于 co-training，因此称为类协同训练，不是说下面的工作就一定是 co-training。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/d3/39/1AexJ5HQ_o.png" alt="640"></p> 
 <p style="text-align:center;margin-left:.5em;"><br></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/04/ab/slZan1CJ_o.png" alt="640"></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">这是一篇 ECCV 2018 的文章，论文首先指出，<strong>直接对同一个数据集训练两个网络，会有两个弊端：</strong></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <ul class="list-paddingleft-2" style="list-style-type:disc;"><li><p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">对同一个数据集训练两个网络，并不能保证两个网络具有不同的视图，更不能保证具有不同且互补的信息；</span></p><p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"></span></p></li><li><p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">协同训练会使得两个网络在训练过程中趋于一致，会导致 collapsed neural networks，进而使得协同训练失效。</span></p></li></ul> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">为了解决上述问题，论文主要做了两个工作：</span></strong><br></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">1. 提出了一个新的代价函数，进行协同训练，其形式如下：</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/4b/e3/r41Sgu7a_o.png" alt="640"></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">信息论学得不好请多见谅，大概就是均匀分布的熵最大，当两个预测分布不一致时，这两个预测分布求和取平均会使得熵增大。相反，如果预测一致熵就不会增加多少。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">论文中还明确强调，该代价函数只用在无标签数据上，因为有标签数据的监督代价函数（论文用的交叉熵）已经使得预测趋于一致（趋于真实标签），用在有标签数据上没有必要，但是好像没说明也用在有标签数据上的后果。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">2. （关键）提出了 View Difference Constraint。其思路是：我们只有一个数据集 D，但我们不能再同一个数据集上训练两个网络，因此需要从 D 中派生出另一个数据集 D'，而这个派生方法就是计算 D 的对抗样本。具体即使用对方的对抗样本来训练自己：</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/86/7e/QGBdPDeF_o.png" alt="640"></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">其中， g1(x) 表示网络 p1 的对抗样本， H(·) 是某种代价函数（KL 散度），该约束的设计期望是使得两个网络具备不同却互补信息。我个人觉得这是一个 very amazing 的想法。尽管我不知道理论上是否能保证两个网络不同却互补，但直观上……</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">实验效果非常好，两个（或多个）网络使用的是同一种架构，就是感觉没有和集成方法比较感觉有点遗憾，毕竟论文用了多个网络。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/c9/e3/mHnHY3aa_o.png" alt="640"></p> 
 <p style="text-align:center;margin-left:.5em;"><br></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/a1/12/p0UdEepX_o.png" alt="640"></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><br></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">Tri-net 是 IJCAI 2018 的论文，挂着周志华大大的名字，用的思想也是周志华提出来的 tri-training，可谓阵容豪华。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">论文称类协同训练为 disagreement-based semi-supervised learning，其关键在于训练多个分类器，已经对不同视图上的不一致性的探索。为此，论文从三个方面改进：model initialization，diversity augmentation 和 pseudo-label editing。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">在展开这三个贡献点之前，我们先来看看 Tri-net 的网络架构：</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/80/95/PxLAD3Cc_o.png" alt="640"></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"></span><br></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">按照把网络架构分为低层抽象和高层抽象部分，Tri-net 架构中的 Ms 是一个共享的低层抽象部分，而 Tri-net 的高层抽象部分是用三个不同的网络架构组成。 </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">刚开始我看到这个架构时，一脸懵逼，但后来想想，这样设计也有一定道理。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">首先，根据我了解的迁移学习理论，低层抽象部分确实是可以共用，用预训练模型进行迁移学习时，低层抽象部分是建议使用很小的学习率或者不学习的。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">其次，不同的高层部分对低层抽象还是有一定程度不同的影响，作为共享低层部分，能学习到满足不同高层部分的低层抽象，即更具泛化性的低层抽象（当然这是我猜的）。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">然后其训练过程主要分为两部分：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"> </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">1. 使用 Output Smearing 处理的有标签数据初始化网络，可以使不同的高层抽象具有不同的预测分布；</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">2. Tri-training 网络：如果有两个高层抽象对一个样本具有一致预测，且该预测可信且稳定，则把该样本作为加入到第三个高层抽象的训练数据中。同时，为了防止 collapsed neural networks 问题（Deep Co-Training 提到），还会在某些 epoch 中继续使用 Output Smearing 训练网络，以增加不同高层抽象的 diversity。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">现在来看看论文的三个贡献点： </span></strong></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">1. Output Smearing：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">同上文的 Deep Co-Training 的论文一样，该论文也认为不能直接在同一个数据集 D 上直接训练网络，Output Smearing 通过向有标签数据的添加随机噪声来构造不同的数据集 D1，D2，D3，分别用来初始化对应的高层抽象部分 M1，M2，M3，这样就能使用高层抽象部分多样化（即代表不同视图）。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">2. Diversity Augmentation：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">为了解决 collapsed neural networks 问题，在特定的 epoch 继续使用 output smearing 数据集来 fine-tune 网络，以继续增强多样化；</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">3. Pseudo-Label Editing：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">Tri-training 训练时，<strong>需要挑选可靠且稳定的无标签样本加入训练集</strong>，如何确定预测的样本是可靠且稳定？</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">本身 Tri-training 这种类协同训练挑选的样本就具有很强的可靠，论文提出一个的方法 Pseudo-label editing：通过 Dropout 的随机性，对样本进行多次预测，如果多次预测的结果都几乎一样，则认为是稳定的。 </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">模型的缺点也相对明显，训练过程相对复杂，感觉靠经验设计；Pseudo-label editing 需要进行多次预测，估计计算负担也挺重的。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/c9/5f/zlfsIXns_o.png" alt="640"></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:normal;"><br></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/91/27/PynRfbtU_o.png" alt="640"></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">这篇 ECCV 2018 的论文是做人脸识别的，主要提出了一个 Consensus-driven propagation 的算法</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">，该算法主要由三部分：Committee，Mediator 和 Pseudo Label Propagation 组成。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">1. Commitee：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">需要用有标签数据训练一个 base model 和 N 个 committee models，而且这些 models 都需要使用不同的网络架构，以保证 diversity （信息的多样性）。然后使用这些 models 提取深度特征，构建 KNN 图，并进一步获取三种信息： </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">Relationship：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">0-1 邻接图，若两个节点在所有的 committee models 的 KNN图中都邻接，则为 1（为什么要排除 base model？）。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">Affinity：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">节点间的相似性信息，论文用余弦相似度。 </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">Local structures w.r.t each node：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">一个节点与其他所有节点的相似性信息，论文用余弦相似度。 </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">2. Mediator：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">一个全连接网络，用来融合上面的多视图信息，来判断正的样本对，和负的样本对。 </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">3. Pseudo Label Propagation：</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">一个对无标签数据赋 pseudo label 的算法，通过 mediator 给出的正样本对的概率作为相似度量，貌似通过一种类似聚类的方法，给样本赋 pseudo label。 </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">论文里面提到的一些概念和想法挺有意思，实验上也很充分，一定程度上佐证了我的一些思考。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:normal;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <h2 style="font-weight:bold;background-color:rgb(255,255,255);line-height:1.2;border-left-color:rgb(16,142,233);font-size:20px;border-left-width:6px;border-left-style:solid;letter-spacing:1px;word-spacing:1px;"><span style="letter-spacing:.5px;">小结</span></h2> 
 <p style="margin-left:.5em;text-align:justify;line-height:normal;"><br></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">最后，我们来总结一下类协同训练在深度学习中应用的三个关键点：</span></strong></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">1. 怎么训练具有不同视图信息的分类器？</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">目前看到的方法有二：1）构建不同的数据集；2）使用不同的网络架构。看起来两种方法一起用效果会更好；</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">2. 如何解决 collapsed neural networks 问题</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">，即如何保持分类器的 diversity，这问题非常重要。 </span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">3. 如何训练“好”的无标签样本加入训练集？</span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">虽然协同训练本身通过一致性原则选择的样本就具有一定的可靠性，但是否有很好的挑选方法？如稳定性。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">最后的最后，再安利另一篇文章，是中大凌亮老师团队的 <em><strong>Deep Co-Space: Sample Mining Across Feature Transformation for Semi-Supervised Learning</strong></em>，利用样本间关系的时序差异，来挑选“好”的无标签样本。这想法非常赞，值得研读。</span></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:normal;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <h2 style="font-weight:bold;background-color:rgb(255,255,255);line-height:1.2;border-left-color:rgb(16,142,233);font-size:20px;border-left-width:6px;border-left-style:solid;letter-spacing:1px;word-spacing:1px;"><span style="letter-spacing:.5px;">Consistency Regularization</span></h2> 
 <p style="text-align:justify;margin-left:.5em;line-height:normal;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;line-height:1.75em;margin-left:.5em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">说到一致（Consistency），其实很多代价都有这个内涵，如 MSE 代价，最小化预测与标签的差值，也就是希望预测与标签能够一致。其他的代价，如 KL 散度、交叉熵代价也类似。所以一致性，是一种非常内在而本质的目标，可以让深度网络进行有效学习。 </span></p> 
 <p style="text-align:justify;line-height:1.75em;margin-left:.5em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;line-height:1.75em;margin-left:.5em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">但是在半监督中，无标签数据并没有标签，因而勤劳而美丽的科研工作者们就想出了各种无需标签信息的 Consistency Regularization，随着 Consistency Regularizaton 的不断发展，一度成为半监督深度学习发展史上耀眼的 SOTA。 </span></p> 
 <p style="text-align:justify;line-height:1.75em;margin-left:.5em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;line-height:1.75em;margin-left:.5em;"><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">Consistency Regularization 的主要思想是：对于一个输入，即使受到微小干扰，其预测都应该是一致的。 </span></strong></p> 
 <p style="text-align:justify;line-height:1.75em;margin-left:.5em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;line-height:1.75em;margin-left:.5em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">例如，某人的裸照（干净的输入）和其有穿衣服的照片（受到干扰的照片），你也应该能知道这是同一个人（一致性）。 当然，这个干扰不能太大（例如衣服把整个人都遮住了）。</span></p> 
 <p style="text-align:justify;line-height:1.75em;margin-left:.5em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;line-height:1.75em;margin-left:.5em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">Consistency Regularization 虽然做法简单，但是却具有很多良好的作用，将会在下面的文章中阐述。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/5a/06/kfWpsHOe_o.png" alt="640"></p> 
 <p style="text-align:center;margin-left:.5em;"><br></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/08/56/RWKPBV3L_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"></span><br></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">这是 NIPS 2014 年发表的工作，其提出了一个概念：pseudo-ensemble，一个 pseudo-ensemble 是一系列子模型（child model），这些子模型通过某种噪声过程（noise process）扰动父模型（parent model）得到。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">Pseudo-ensemble 与其他的有关扰动的方法的区别在于：其他的方法只考虑在输入空间的扰动，而 pseudo-ensemble 还考虑在模型空间（model space）上的扰动。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">一个典型的 pseudo-ensemble 就是 Dropout。 但是，除了dropout 以外，我没怎么想到其他的模型空间上的扰动，看论文的公式貌似是在网络的中间表示添加噪声？论文有代码，但我没怎么看，有不同意见的同学可以评论里提出。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">其有监督代价函数如下：</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/42/e3/k74Rhw6F_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">其中 θ 是网络参数，ξ 表示某种噪声过程，该有监督代价函数就是让扰动得到的子模型与标签 y 一致。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">论文中还提出其半监督形式：<strong>The Pseudo-Ensemble Agreement regularizer (PEA)</strong>，其形式如下：</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0c/64/EWzTvdZS_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">其中 d 是网络的层数，其含义应该是把父模型的每一层中间表示，与子模型的进行一致正则， V 是某种惩罚函数，如 MSE 代价（注：最后一层的中间表示即网络的预测）<span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">。</span> </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">现在回顾一下 Consistency regularization 的思想：对于一个输入，受扰微小扰动后，其预测应该是一致的。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">PEA 的含义我认为就是，对于一个输入，受到扰动后，其所有的中间表示，都应该一致。其实根据后面更多的论文，这个约束可能强力些。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">PEA 的目的是，使得模型对扰动具有鲁棒性，因为鲁棒的模型泛化性能更好，同时还能学习数据的内在不变性（<strong>作用 1</strong>）。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/46/31/uArfSo0G_o.png" alt="640"></p> 
 <p style="text-align:center;margin-left:.5em;"><br></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/de/e3/0GgBEe9X_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">这是 NIPS 2016 年发表的工作。随机性在大部分的学习系统中起到重要的作用，深度学习系统也如此。一些随机技术，如随机数据增强、Dropout、随机最大池化等，可以使得使用 SGD 训练的分类器具有更好的泛化性和鲁棒性。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">而且这种不确定性的存在，使得模型对同一个样本的多次预测结果可能不同。因此论文提出一个无监督代价（即我说的半监督正则），其通过最小化对同一个样本的多次预测，利用这种随机性来达到更好的泛化性能。该无监督代价形式如下：</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0b/51/PILkHMfy_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">其中<img border="0" style="font-family:Helvetica, Arial, sans-serif;text-align:center;width:20px;" title="" width="20" src="https://images2.imgbox.com/21/2c/T0K3uxuc_o.png" alt="640"></span><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">代表对输入<img border="0" style="font-family:Helvetica, Arial, sans-serif;text-align:center;width:20px;" title="" width="20" src="https://images2.imgbox.com/49/86/fuBETozy_o.png" alt="640"></span><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">的第 j 次预测， T 表示某种数据变换。除了对样本做变换，在网络内也使用类似 Dropout 或随机池化等技术产生随机性。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">似泥？Pseudo-Ensemble！对不起，你俩有点像。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">虽然感觉很类似，但是这两篇论文很值得一读啊，论文里提出许多的观点和想法，一直延续至今，信息量挺大。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/62/ff/FiHVTCbK_o.png" alt="640"></p> 
 <p style="text-align:center;margin-left:.5em;"><br></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/42/de/Fc4reAXu_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">这篇在 ICLR 2017 年的工作提出了一个我称之为 <strong>peer-consistency</strong> 的正则项，即 π 模型，也是我最开始对 Consistency Regularization 的认知的由来。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">π 模型认为，同一个输入，在不同的正则和数据增强条件下，网络对其预测应该是一致的。其无监督代价部分如下：</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c3/d4/7BLK7ia5_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">其中 zi 是网络的一个预测，而<img border="0" style="font-family:Helvetica, Arial, sans-serif;text-align:center;width:20px;" title="" width="20" src="https://images2.imgbox.com/61/fb/SW6p3fZZ_o.png" alt="640"></span><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">是网络对同一个样本在不同的正则和数据增强条件下的一个预测，然后让着两个预测一致。看起来很像前面两篇文章的简化版，但是效果好啊，这也是我说前面的约束太强的原因。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">w(t) 是权重函数，是迭代次数的函数。由于在网络的初始阶段，网络的预测十分不准（尤其是半监督中有标签数据有限的情况），这时的网络预测靠不住的，因此这无监督代价在初始时的权重应该设置得比较小，到后期再慢慢增大。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">w(t) 非常关键，论文中使用了一个高斯爬升函数，具体可以看论文。我的理解是，这种 peer-consistency 鼓励一个样本点的扰动不变性，其实鼓励了预测函数（即网络）对样本的邻域具有光滑性（<strong>作用 2</strong>）<span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">。</span></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">而且把 peer-consistency 看做是一种标签正则，可以从最大熵模型来理解 peer-consistency，具体可参考 <em><strong>Regularizing Neural Networks by Penalizing Confident Output Distributions</strong></em><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"></span></strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">,</span><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"></span></strong><em><strong><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"> </span></strong></em>ICLR 17。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">论文中还提出了另一种更强大的方法，但由于不在该主题下，不讲，可能会另开一篇文章讲吧，同学可以自己看论文。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/4a/8a/Q29z2BWK_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.5em;"><br></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/a9/4b/qnrH1lgG_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">这篇 NIPS 2017 的工作提出了一个 peer-consistency 的升级版，Teacher-student Consistency。 其除了 consistency 这个思想外，还有“模型成功的关键在于 target 的质量”这个动机。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">具体想法就是，我从当前的模型（Student model），构造出一个比 Student model 更好一些的 Teather model，然后用这个 Teacher model 的预测来训练 Student model（即 Consistency Regularization）。 其无监督代价部分如下：</span></p> 
 <p style="text-align:center;margin-left:.5em;"><br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/89/bc/sjpsypGu_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">其中 f(x, θ', η') 是 Teacher model 的预测， <span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">f(x, θ, η)</span> 是 Student model 的预测。但问题是，怎么去构造这个更好一些的 Teacher model？ </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">论文提出的方法是，对 Student model 每次更新的模型做移动平均，移动平均后的模型就是 Teacher model，Teacher model 也不用反向传播更新，就参数的移动平均足以。其移动平均公式如下：</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e6/05/vgBKgKSj_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">有没有一种好神奇的感觉，效果还非常地好。论文还用实验说明，其能形成一个良好的循环，得到一个 Student model，构造一个更好的 Teacher model，然后这个 Teacher model 又能用来升级 Student model，然后又……</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">当然，由于 Student model 刚开始只用有限的有标签数据训练，其性能并不好，因此该无监督代价同样需要乘一个权重（函数），这个权重函数和 <em><strong>Temporal Ensembling for Semi-supervised Learning</strong></em> 一样。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1c/eb/Hj6fFWS6_o.png" alt="640"></p> 
 <p style="text-align:center;"><br></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/b1/6f/IP9I1rgm_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">这是 CVPR 2018 的工作，该工作指出，前面的 consistency 都是只作用在单个数据点上，即都是对同一个数据点的增强。因此，<strong>论文提出一种 features consistency，其认为属于同一个类别的数据特征（分类层前面的中间表示）应该具有一致性。 </strong></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">但是，无监督数据并没有标签，我怎么知道是不是属于同一类，论文使用前面的半监督方法的预测作为样本的标签，即 pseudo label，来构造这种属于同一类别的特征一致性。 </span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;">虽然感觉这是取了巧，但是实验效果还不错，而且想法也很对，还是分享一下。其特征一致性的正则如下：</span></p> 
 <p style="text-align:center;margin-left:.5em;line-height:1.75em;"><br></p> 
 <p style="text-align:center;margin-left:.5em;"><img src="https://images2.imgbox.com/b4/55/uWCMl1xE_o.png" alt="640"></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">其中，若预测的为属于同一类，则 Wij=1，不属于同一类则为 0。 h(xi) 表示样本的（深度）特征。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">当然，这个代价函数本身是 1994 年 NIPS 一个工作提出来的，被这篇论文重新活用了，还是那句话，想法对，效果也好。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:normal;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:normal;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">还有一篇 Consistency Regularization 的算是 <em><strong>Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning</strong></em>，虽然论文讲的是如何计算对抗噪声，但我看了代码觉得其实和 consistency 很像，只不过是普通扰动变成了对抗扰动。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;">另一篇类似 CVPR 2018 SNTG 的论文是 ECCV 2018 的 <em><strong>Transductive Semi-Supervised Deep Learning using Min-Max Features</strong></em>，不过大部分概念我说过了，论文自己去看就行，思想差不多。</span></p> 
 <p style="text-align:justify;margin-left:.5em;line-height:normal;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"> <br></span></p> 
 <h2 style="font-weight:bold;background-color:rgb(255,255,255);line-height:1.2;border-left-color:rgb(16,142,233);font-size:20px;border-left-width:6px;border-left-style:solid;letter-spacing:1px;word-spacing:1px;"><span style="letter-spacing:.5px;">参考文献</span></h2> 
 <p><br></p> 
 <p style="text-align:left;margin-left:.5em;line-height:normal;"><span style="letter-spacing:.5px;text-align:justify;font-size:12px;color:rgb(136,136,136);">[1] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li. Bag of Tricks for Image Classification with Convolutional Neural Networks. ArXiv:1812.01187.</span></p> 
 <p style="text-align:left;margin-left:.5em;line-height:normal;"><span style="color:rgb(136,136,136);font-size:12px;letter-spacing:.5px;text-align:justify;">[2]Wei Han, </span><span style="color:rgb(136,136,136);font-size:12px;letter-spacing:.5px;text-align:justify;">Ruyi Feng, Lizhe Wang, Yafan Cheng. A semi-supervised generative framework with deep learning features for high-resolution remote sensing image scene classification. ISPRS Journal of Photogrammetry and Remote Sensing.</span><br></p> 
 <p style="margin-left:.5em;text-align:justify;line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:.5px;text-align:justify;"><br></span></p> 
 <p style="margin-left:0em;color:rgb(51,51,51);text-align:center;line-height:2em;"><img border="0" title="" width="60" style="font-size:16px;color:rgb(51,51,51);letter-spacing:.5px;background-color:rgb(255,255,255);text-align:justify;" src="https://images2.imgbox.com/5c/a4/GU3m57Yo_o.png" alt="640?"></p> 
 <p style="margin-left:0em;color:rgb(51,51,51);text-align:center;line-height:normal;"><br></p> 
 <p style="margin-left:0em;color:rgb(51,51,51);text-align:center;line-height:normal;"><br></p> 
 <p style="letter-spacing:.5px;color:rgb(51,51,51);background-color:rgb(255,255,255);text-align:justify;line-height:normal;"><strong style="color:rgb(62,62,62);"><span style="font-size:12px;">点击以下标题查看作者其他文章：</span></strong> <br></p> 
 <p style="letter-spacing:.5px;color:rgb(51,51,51);text-align:center;background-color:rgb(255,255,255);"><br></p> 
 <ul class="list-paddingleft-2" style="list-style-type:square;"><li><p style="margin-left:0em;letter-spacing:0px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247492654&amp;idx=1&amp;sn=b9047d5cca7657f02dc7f6685ef04037&amp;chksm=96ea3baea19db2b8dc1c1267801d0c585b3cf072531af86abdeb73c6fb4c07dc3325c2d13d57&amp;scene=21#wechat_redirect" rel="nofollow" style="color:rgb(87,107,149);text-decoration:underline;font-size:12px;">自动机器学习（AutoML）最新综述</a></p></li><li><p style="margin-left:0em;letter-spacing:0px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247492317&amp;idx=1&amp;sn=e823a75d9463257ed9ea7b3e4677c1ae&amp;chksm=96ea3d5da19db44be0872ff4e29043aa72c7a624a116196bfeeca092a15f9209d7cf8ce46eb5&amp;scene=21#wechat_redirect" rel="nofollow" style="color:rgb(87,107,149);text-decoration:underline;font-size:12px;">自然语言处理中的语言模型预训练方法</a></p></li><li><p style="margin-left:0em;letter-spacing:0px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247491082&amp;idx=1&amp;sn=d7c1cb39c3be43154c658ca5a791eb4c&amp;chksm=96e9c18aa19e489c32fe36671e4208ce42bf200e3a7adeda200fa2785462d16f85c58bb455b4&amp;scene=21#wechat_redirect" rel="nofollow" style="color:rgb(87,107,149);text-decoration:underline;font-size:12px;">从傅里叶分析角度解读深度学习的泛化能力</a></p></li><li><p style="margin-left:0em;letter-spacing:0px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247493033&amp;idx=1&amp;sn=1ae1cd347126b10d6a857cd9bba7b601&amp;chksm=96ea3a29a19db33f3c07723ed6e5ecbb8d2ff1b1617f1cf0d39cb3cc1e6e9c325cc29147d58d&amp;scene=21#wechat_redirect" rel="nofollow" style="color:rgb(87,107,149);text-decoration:underline;font-size:12px;">两行代码玩转Google BERT句向量词向量</a></p></li><li><p style="margin-left:0em;letter-spacing:0px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247493627&amp;idx=1&amp;sn=33e2f7c787fa9f14cef581f10b7dd2f7&amp;chksm=96ea387ba19db16dc97620e28e6a7c8605b396b53f21e3eff6cf9553762a1dbc5233c580cc53&amp;scene=21#wechat_redirect" rel="nofollow" style="color:rgb(87,107,149);text-decoration:underline;font-size:12px;">近期知识图谱顶会论文推荐，你都读过哪几篇？</a></p></li><li><p style="margin-left:0em;letter-spacing:0px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247492746&amp;idx=1&amp;sn=921d7315a973b85dd4e802cb5fd456fb&amp;chksm=96ea3b0aa19db21c48841ddcee38592a3c086ae8fa1a9893cf46ff974f0f38fb350bcd528265&amp;scene=21#wechat_redirect" rel="nofollow" style="color:rgb(87,107,149);text-decoration:underline;font-size:12px;">TensorSpace：超酷炫3D神经网络可视化框架</a></p></li><li><p style="margin-left:0em;letter-spacing:0px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247493520&amp;idx=1&amp;sn=2b04c009ef75291ef3d19e8fe673aa36&amp;chksm=96ea3810a19db10621e7a661974c796e8adeffc31625a769f8db1d87ba803cd58a30d40ad7ce&amp;scene=21#wechat_redirect" rel="nofollow" style="color:rgb(87,107,149);text-decoration:underline;font-size:12px;">深度长文：NLP的巨人肩膀（上）</a></p></li><li><p style="margin-left:0em;letter-spacing:0px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247493731&amp;idx=1&amp;sn=51206e4ca3983548436d889590ab5347&amp;chksm=96ea37e3a19dbef5b6db3143eb9df822915126d3d8f61fe73ddb9f8fa329d568ec79a662acb1&amp;scene=21#wechat_redirect" rel="nofollow" style="color:rgb(87,107,149);text-decoration:underline;font-size:12px;">NLP的巨人肩膀（下）：从CoVe到BERT</a></p></li></ul> 
 <p style="margin-left:0em;color:rgb(51,51,51);text-align:center;line-height:1.75em;"><br></p> 
 <p style="font-size:16px;color:rgb(62,62,62);text-align:center;letter-spacing:1px;font-family:'Optima-Regular', 'PingFangTC-light';line-height:1.75em;"><br></p> 
 <p style="text-align:center;letter-spacing:1px;"><br></p> 
 <p style="text-align:center;letter-spacing:1px;"><span style="font-size:12px;color:rgb(178,178,178);"><strong style="color:rgb(51,51,51);letter-spacing:.544px;background-color:rgb(255,255,255);font-family:'Optima-Regular', 'PingFangTC-light';font-size:14px;"><img class="__bg_gif" src="https://images2.imgbox.com/6b/50/iYmqUl0z_o.gif" alt="640?"></strong><span style="color:rgb(51,51,51);letter-spacing:.544px;background-color:rgb(255,255,255);font-family:'Optima-Regular', 'PingFangTC-light';font-size:14px;"><strong>#</strong></span><span style="letter-spacing:.544px;background-color:rgb(255,255,255);font-family:'Optima-Regular', 'PingFangTC-light';font-size:14px;color:rgb(214,168,65);"><strong>投 稿 通 道<span style="color:rgb(0,0,0);">#</span></strong></span></span></p> 
 <p style="text-align:center;letter-spacing:1px;"><span style="letter-spacing:.544px;background-color:rgb(255,255,255);font-family:'Optima-Regular', 'PingFangTC-light';color:rgb(178,178,178);font-size:11px;"><strong><span style="color:rgb(255,255,255);letter-spacing:.5px;background-color:rgb(0,0,0);"> 让你的论文被更多人看到 </span></strong></span></p> 
 <p><br></p> 
 <br> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="color:rgb(178,178,178);font-size:12px;letter-spacing:.5px;"><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';letter-spacing:2px;text-align:justify;">如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？ <strong>答案就是：你不认识的人。</strong></span></span></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><br></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="color:rgb(178,178,178);font-size:12px;letter-spacing:.5px;"><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';letter-spacing:2px;text-align:justify;">总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 </span></span></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><br></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="color:rgb(178,178,178);font-size:12px;letter-spacing:.5px;"><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';letter-spacing:2px;text-align:justify;">PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<strong>最新论文解读</strong>，也可以是<strong>学习心得</strong>或<strong>技术干货</strong>。我们的目的只有一个，让知识真正流动起来。</span></span></p> 
 <p><br></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="font-size:12px;">? <strong>来稿标准：</strong></span></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';font-size:12px;letter-spacing:2px;text-align:justify;">• 稿件确系个人<strong>原创作品</strong>，来稿需注明作者个人信息（姓名+学校/工作单位+学历/职位+研究方向） </span></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';font-size:12px;letter-spacing:2px;text-align:justify;">• 如果文章并非首发，请在投稿时提醒并附上所有已发布链接 </span></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';font-size:12px;letter-spacing:2px;text-align:justify;">• PaperWeekly 默认每篇文章都是首发，均会添加“原创”标志</span></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><br></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="font-size:12px;"><strong>? 投稿邮箱：</strong></span></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';font-size:12px;letter-spacing:2px;text-align:justify;">• 投稿邮箱：</span><span style="font-family:'Optima-Regular', 'PingFangTC-light';font-size:12px;letter-spacing:2px;text-align:justify;text-decoration:underline;color:rgb(0,82,255);">hr@paperweekly.site</span><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';font-size:12px;letter-spacing:2px;text-align:justify;"> </span></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';font-size:12px;letter-spacing:2px;text-align:justify;">• 所有文章配图，请单独在附件中发送 </span></p> 
 <p style="margin-left:8px;letter-spacing:1px;"><span style="color:rgb(136,136,136);font-family:'Optima-Regular', 'PingFangTC-light';font-size:12px;letter-spacing:2px;text-align:justify;">• 请留下即时联系方式（微信或手机），以便我们在编辑发布时和作者沟通</span></p> 
 <p style="margin-left:8px;"><br></p> 
 <p style="letter-spacing:.5px;"><br></p> 
 <p style="color:rgb(51,51,51);font-size:17px;letter-spacing:.544px;text-align:center;font-family:'Optima-Regular', 'PingFangTC-light';"><br></p> 
 <p style="color:rgb(51,51,51);font-size:17px;letter-spacing:.544px;text-align:center;font-family:'Optima-Regular', 'PingFangTC-light';"><span style="font-family:arial, '宋体', sans-serif;font-size:14px;letter-spacing:2px;">?</span></p> 
 <p style="color:rgb(51,51,51);font-size:17px;letter-spacing:.544px;text-align:center;font-family:'Optima-Regular', 'PingFangTC-light';"><br></p> 
 <p style="color:rgb(51,51,51);font-size:17px;letter-spacing:.544px;text-align:center;font-family:'Optima-Regular', 'PingFangTC-light';"><span style="text-align:justify;color:rgb(0,0,0);font-size:13px;letter-spacing:2px;">现在，在<strong>「知乎」</strong>也能找到我们了</span></p> 
 <p style="color:rgb(51,51,51);font-size:17px;letter-spacing:.544px;text-align:center;font-family:'Optima-Regular', 'PingFangTC-light';"><span style="text-align:justify;color:rgb(0,0,0);font-size:13px;letter-spacing:2px;">进入知乎首页搜索<strong>「PaperWeekly」</strong></span></p> 
 <p style="color:rgb(51,51,51);font-size:17px;letter-spacing:.544px;text-align:center;font-family:'Optima-Regular', 'PingFangTC-light';"><span style="text-align:justify;color:rgb(0,0,0);font-size:13px;letter-spacing:2px;">点击<strong>「关注」</strong>订阅我们的专栏吧</span></p> 
 <p style="margin-left:0em;letter-spacing:.5px;color:rgb(51,51,51);text-align:center;line-height:normal;"><br></p> 
 <p style="margin-left:0em;letter-spacing:.5px;color:rgb(51,51,51);text-align:center;line-height:2em;"><br></p> 
 <p style="margin-left:0em;letter-spacing:.5px;"><span style="color:rgb(71,168,218);"><strong><span style="font-size:14px;">关于PaperWeekly</span></strong><br></span></p> 
 <p style="margin-left:0em;letter-spacing:.5px;text-align:justify;background-color:rgb(255,255,255);line-height:normal;"><br></p> 
 <p style="margin-left:0em;letter-spacing:.5px;text-align:justify;background-color:rgb(255,255,255);line-height:1.5em;"><span style="color:rgb(136,136,136);font-size:14px;">PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击<strong>「交流群」</strong>，小助手将把你带入 PaperWeekly 的交流群里。</span></p> 
 <p style="margin-left:0em;letter-spacing:.5px;text-align:justify;background-color:rgb(255,255,255);line-height:1.75em;"><br></p> 
 <p style="margin-left:0em;letter-spacing:.5px;text-align:center;"><img class="__bg_gif" src="https://images2.imgbox.com/52/b7/OrQdTsJG_o.gif" alt="640?"></p> 
 <p style="margin-left:0em;letter-spacing:.5px;text-align:justify;"><span style="letter-spacing:1px;font-size:13px;color:rgb(63,63,63);">▽ 点击 | </span><span style="letter-spacing:1px;font-size:13px;"><span style="letter-spacing:.5px;color:rgb(171,25,66);">阅读原文</span> <span style="letter-spacing:.5px;color:rgb(63,63,63);">| 获取最新论文推荐</span></span></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5c902f2075877fe9e5b8b02cbaaa1af0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">浅显易懂的GCC使用教程——初级篇</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/383535768803df05a3ad1b5789367497/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">opencv python 轮廓特征/凸包/外接矩形/外接圆/拟合矩形/拟合直线/拟合圆</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>