<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【自然语言处理六-最重要的模型-transformer-上】 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【自然语言处理六-最重要的模型-transformer-上】" />
<meta property="og:description" content="自然语言处理六-最重要的模型-transformer-上 什么是transformer模型transformer 模型在自然语言处理领域的应用transformer 架构encoderinput处理部分（词嵌入和postional encoding）attention部分addNorm Feedforward &amp; add &amp;&amp; NormFeedforword，实际上就是两层全连接层，中间有激活函数等add &amp; Norm 最终encoder的输出 什么是transformer模型 它是编码器和解码器的架构，来处理一个序列对，这个跟seq2seq的架构是一样的。
如果没接触过seq2seq架构，可以通俗的理解，编码器用来处理输入，解码器用来输出但与seq2seq的架构不同的是，transformer是纯基于注意力的。
之前花了几篇的篇幅讲注意力，也是在为后面讲解这个模型打基础。 transformer模型无疑是近几年最重要的模型，目前的大模型几乎都以它为基础发展，很多模型的名字都带有缩写T，正是transformer的缩写。
当然transfomer不仅仅用于自然语言处理领域，归集于自然语言处理模块下面来讲，是因为它在自然语言处理领域的应用非常广泛，下面就讲它的几种应用。
transformer 模型在自然语言处理领域的应用 编码器和解码器架构，比较擅长处理QA类的问题，但这个QA不仅仅是一个问题、一个答案的形式，许多的自然语言处理，都可以理解为QA类问题，比如：
真实的QA类问题。比如：机器人问答。机器翻译。比如中英翻译摘要提取。输入文章，提取摘要情感分析。输入评价，输出正面/负面评价
等等 下面来介绍transformer的架构，看什么样子的架构能实现上面的这些功能
transformer 架构 它出自经典论文《attention is all you need》，论文地址是： http://arxiv.org/abs/1706.03762，本文中的诸多图片都是取自该论文，下面的架构图也是出自论文
从上图就可以看出，transfomer的架构包括左边encoder和右边decoder，下面先来讲encoder部分
encoder 左侧的encoder部分，输入一排input vector向量，输出一排向量，忽略中间的细节来看，是如下的的架构：
中间encoer部分，如果是seq2seq架构，就是RNN，transformer就相对复杂一些：
下面分部分介绍encoder的各个部分：
input处理部分（词嵌入和postional encoding） 输入一排，经过词嵌入input Embedding，再加上位置信息，Postional Encoding （这部分可以在 位置编码有介绍），生成一排向量。
然后进入attention计算
attention部分 transformer最重要的attention部分，这部分是多头注意力。值得注意的是，这部分的输出并不会直接丢给全连接层，还需要在额外经过residual add和layer norm
add add的操作:
执行residual 残差连接，将attention的输入加到self-attention后的输出
Norm 残差后的输出进行层归一化，层归一化的操作：
不考虑batch，将输入中同一个feature，同一个sample，不同的dimension 计算均值和标准差，然后如下计算
这个操作，用能听得懂的话翻译一下就是，是对每个样本里面的元素进行归一化
整个过程如下：
最终上述部分的输出作为全连接层的输入
Feedforward &amp; add &amp;&amp; Norm 上一部分的输出，输入到本部分
Feedforword，实际上就是两层全连接层，中间有激活函数等 当然中间的卷积，可以换成线性层Linear" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/d5ad2a1ce5a0732d615c1775bd9a3713/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-04T14:24:48+08:00" />
<meta property="article:modified_time" content="2024-03-04T14:24:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【自然语言处理六-最重要的模型-transformer-上】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>自然语言处理六-最重要的模型-transformer-上</h4> 
 <ul><li><a href="#transformer_1" rel="nofollow">什么是transformer模型</a></li><li><a href="#transformer__11" rel="nofollow">transformer 模型在自然语言处理领域的应用</a></li><li><a href="#transformer__22" rel="nofollow">transformer 架构</a></li><li><ul><li><a href="#encoder_28" rel="nofollow">encoder</a></li><li><ul><li><a href="#inputpostional__encoding_34" rel="nofollow">input处理部分（词嵌入和postional encoding）</a></li><li><a href="#attention_39" rel="nofollow">attention部分</a></li><li><ul><li><a href="#add_41" rel="nofollow">add</a></li><li><a href="#Norm_45" rel="nofollow">Norm</a></li></ul> 
    </li><li><a href="#Feedforward__add__Norm_54" rel="nofollow">Feedforward &amp; add &amp;&amp; Norm</a></li><li><ul><li><a href="#Feedforword_56" rel="nofollow">Feedforword，实际上就是两层全连接层，中间有激活函数等</a></li><li><a href="#add__Norm_62" rel="nofollow">add &amp; Norm</a></li></ul> 
    </li><li><a href="#encoder_64" rel="nofollow">最终encoder的输出</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="transformer_1"></a>什么是transformer模型</h2> 
<ul><li>它是编码器和解码器的架构，来处理一个序列对，这个跟seq2seq的架构是一样的。<br> 如果没接触过seq2seq架构，可以通俗的理解，编码器用来处理输入，解码器用来输出</li><li>但与seq2seq的架构不同的是，<strong>transformer是纯基于注意力的</strong>。<br> 之前花了几篇的篇幅讲注意力，也是在为后面讲解这个模型打基础。</li></ul> 
<p>transformer模型无疑是近几年最重要的模型，目前的大模型几乎都以它为基础发展，很多模型的名字都带有缩写T，正是transformer的缩写。<br> 当然transfomer不仅仅用于自然语言处理领域，归集于自然语言处理模块下面来讲，是因为它在自然语言处理领域的应用非常广泛，下面就讲它的几种应用。</p> 
<h2><a id="transformer__11"></a>transformer 模型在自然语言处理领域的应用</h2> 
<p>编码器和解码器架构，比较擅长处理QA类的问题，但这个QA不仅仅是一个问题、一个答案的形式，许多的自然语言处理，都可以理解为QA类问题，比如：</p> 
<ol><li>真实的QA类问题。比如：机器人问答。</li><li>机器翻译。比如中英翻译</li><li>摘要提取。输入文章，提取摘要</li><li>情感分析。输入评价，输出正面/负面评价<br> 等等</li></ol> 
<p>下面来介绍transformer的架构，看什么样子的架构能实现上面的这些功能</p> 
<h2><a id="transformer__22"></a>transformer 架构</h2> 
<p>它出自经典论文《attention is all you need》，论文地址是： http://arxiv.org/abs/1706.03762，本文中的诸多图片都是取自该论文，下面的架构图也是出自论文</p> 
<p><img src="https://images2.imgbox.com/e2/71/or0OFH9j_o.png" alt="在这里插入图片描述"><br> 从上图就可以看出，transfomer的架构包括左边encoder和右边decoder，下面先来讲encoder部分</p> 
<h3><a id="encoder_28"></a>encoder</h3> 
<p>左侧的encoder部分，输入一排input vector向量，输出一排向量，忽略中间的细节来看，是如下的的架构：<br> <img src="https://images2.imgbox.com/30/bc/6Z9GM5En_o.png" alt="在这里插入图片描述"><br> 中间encoer部分，如果是seq2seq架构，就是RNN，transformer就相对复杂一些：<br> 下面分部分介绍encoder的各个部分：</p> 
<h4><a id="inputpostional__encoding_34"></a>input处理部分（词嵌入和postional encoding）</h4> 
<p><img src="https://images2.imgbox.com/47/03/cH770Gwr_o.png" alt="在这里插入图片描述"><br> 输入一排，经过词嵌入input Embedding，再加上位置信息，Postional Encoding （这部分可以在 <a href="https://blog.csdn.net/zishuijing_dd/article/details/136299892?spm=1001.2014.3001.5501">位置编码</a>有介绍），生成一排向量。<br> 然后进入attention计算</p> 
<h4><a id="attention_39"></a>attention部分</h4> 
<p>transformer最重要的attention部分，这部分是多头注意力。值得注意的是，这部分的输出并不会直接丢给全连接层，还需要在额外经过residual add和layer norm</p> 
<h5><a id="add_41"></a>add</h5> 
<p>add的操作:<br> 执行residual 残差连接，将attention的<strong>输入</strong>加到<strong>self-attention后的输出</strong></p> 
<h5><a id="Norm_45"></a>Norm</h5> 
<p>残差后的输出进行层归一化，层归一化的操作：<br> 不考虑batch，将输入中同一个feature，同一个sample，不同的dimension 计算均值和标准差，然后如下计算<br> <img src="https://images2.imgbox.com/9f/67/mshph8s1_o.png" alt="在这里插入图片描述"><br> 这个操作，用能听得懂的话翻译一下就是，是对每个样本里面的元素进行归一化<br> 整个过程如下：<br> <img src="https://images2.imgbox.com/36/7b/cwD8w4oe_o.png" alt="在这里插入图片描述"><br> 最终上述部分的输出作为全连接层的输入</p> 
<h4><a id="Feedforward__add__Norm_54"></a>Feedforward &amp; add &amp;&amp; Norm</h4> 
<p>上一部分的输出，输入到本部分</p> 
<h5><a id="Feedforword_56"></a>Feedforword，实际上就是两层全连接层，中间有激活函数等</h5> 
<p><img src="https://images2.imgbox.com/42/8c/6YHNdmK4_o.png" alt="在这里插入图片描述"><br> 当然中间的卷积，可以换成线性层Linear</p> 
<p>经过这个全连接层的输出之后，依然要经过残差add 和层归一化norm，然后输出.</p> 
<h5><a id="add__Norm_62"></a>add &amp; Norm</h5> 
<p>这部分 同attention 那一层的操作，此处不赘述</p> 
<h4><a id="encoder_64"></a>最终encoder的输出</h4> 
<p>在encoder中，上面这三个步骤是可以重复多次的，所以看到架构图中表示了*N操作。<br> 最终的输出才是encoder的输出。</p> 
<p>篇幅所限，下一篇文章继续 transformer的decoder部分 <a href="https://blog.csdn.net/zishuijing_dd/article/details/136427404?spm=1001.2014.3001.5501">自然语言处理六-最重要的模型-transformer-下</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/150bb21c00fac13f352fb83b4d0e54f4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【自然语言处理六-最重要的模型-transformer-下】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/94ce2bc704eaf20f4f94515f7839b5b2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">LM358ADR运算放大器芯片中文资料规格书产品文档PDF数据手册引脚图图片</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>