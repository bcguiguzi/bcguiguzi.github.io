<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大模型之SORA技术学习 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大模型之SORA技术学习" />
<meta property="og:description" content="文章目录 sora的技术原理文字生成视频过程sora的技术优势量大质优的视频预训练库算力多，采样步骤多，更精细。GPT解释力更强，提示词(Prompt）表现更好 使用场景参考 Sora改变AI认知方式，开启走向【世界模拟器】的史诗级的漫漫征途，才是未来暴风眼，真正的重点。 但Sora并没有理解整个世界和诸多物理细节，常识及定律，而是根据GPT语义解释能力、丰富的联想和丰富度，基于海量的视频库，抄写视频片段并猜视频下一帧，并使用Diffusion、GAN（对抗式生成网络技术）将多个视频片段混合在一起，产生稳定且连续的短视频。
Yann LeCun（ACM图灵奖得主，纽约大学教授。Meta首席人工智能科学家）针对Sora的评论。大意是：
让我在这里澄清一个巨大的误解。从提示文字，生成看起来相当逼真的视频，并不意味着，这个系统理解物理世界。生成一个视频，和基于世界模型的因果预测，大不相同。目前这种生成式的方向，代价高昂，可能还有更好的法子。
sora的技术原理 结合了 Diffusion model（扩散模型）和 Transformer 技术，以确保视频内容与文本描述紧密相连
Sora的官方技术报告详见：
Sora官方的技术报告：Video generation models as world simulators
主要功能和目标： 目前，Sora 的主要功能是将文本输入转换成视频输出。这包括但不限于将故事、说明或命令转化为相应的视频。Sora 的目标是创建一个能够理解复杂文本描述并将其转化为高质量视频内容的系统。官方的声明指出，Sora 的最终目标是向一个“通用物理世界模拟器”的方向迈进，即成为一个能够模拟真实世界的复杂互动和动态环境的强大工具。 训练流程包括以下步骤： 收集视频数据与标注信息。训练图片字幕模型。利用 GPT-4 丰富视频描述。切分视频为 Patches。应用视频压缩模型。在潜在空间中处理视频数据。应用扩散模型与 Transformer 进行训练。最终恢复高清视频。 核心模块： Transformer 结构：这是一种深度学习模型的架构，最初用于自然语言处理，现在被广泛应用于各种领域，包括 Sora 的几个组件。Diffusion model：用于逐步去除视频中的噪声，从而生成清晰的图像场景。 文字生成视频过程 提供一段文字：依靠GPT语义解释能力、丰富的联想和丰富度，产生针对视频内容详细的描述。如文本是：“散步在夜晚东京街道上”，GPT发挥想象力，联想出一堆词和关联“高楼”、“繁华夜景”等等。它联想力越丰富，Sora能关联到的时空碎块就越多越准。Diffusion：作为一个画师，根据关键词特征值对应的可能性概率，在海量视频库到处翻，看看抄哪一个碎块比较像，看哪个像，就猜对应的下一笔要落在什么地方。重复很多步通过Diffusion和Transformer共同联想，死记硬背，从巨大视频库里生拉硬拽，配合GAN（对抗式生成网络技术），把这些一张张碎块拼成图，再拼接成一个序列，每秒播放几十张，视频就出来了
sora的技术优势 Sora实际上对于Pika等，只是量变，都是差不多的技术和原理，没有质变。但几乎达到了近似质变的效果了。原因在于：
量大质优的视频预训练库 大力出奇迹，是OpenAI的基因。
到底花了多少钱在高质量的视频素材上，搜集了多少的视频库，只有OpenAI自己知道。但可以肯定的是，远远不是Pika等创业团队所能比的。
记得多，才能抄的好，混得妙。
甚至，现在的视频量已经不能满足OpenAI的需求了。已经被爆料，OpenAI的视频库，大量使用了UE5生成的视频来做补充和训练。我们看到的赛车那个视频就是。
Pika、Runway、Stable Video和Sora有时候会撞车：都使用了同一个素材加到库中。那么使用一样类似的关键词，可能就能调出一模一样的元素。
算力多，采样步骤多，更精细。 不同采样和计算步骤后，通过同一个视频库“猜”的步骤越多，加的东西越细，效果越好。做32倍运算的效果，就明显好于4倍的效果。
还是大力出奇迹，OpenAI不变的配方和味道。
那么请问，Pika等创业公司能有多少张GPU卡呢?
Sora能土豪的用32倍，1080p，渲染1分钟的视频。创业公司能用多少，4倍，360p，4-8秒，已经足够把钱烧光了…
画面精致度怎么比？时长怎么比？不公平。
Diffusion的不稳定性通病，在Pika等产品中已经表现无疑。
Sora和他们完全不在一个层次上，稳定性很好。已经不能完全用数据和算力来解释了。一定是采用了GAN（对抗式生成网络技术）这个增强连续性的技术。
生成的视频效果比较见下图：
GPT解释力更强，提示词(Prompt）表现更好 对一段提示词或提示句子，GPT能展开的联想和丰富度，是决定了Sora抄什么，能猜多准的。
OpenAI的GPT能力天下第一，开源模型无能撼动。
所以，不幸的是，Pika等创业公司大多还是要依靠OpenAI的GPT能力。
那么，问题来了，亲儿子能用的，一定胜过外部客户能用到的深度和广度。
第二个点，就是外部公司的视频库和GPT联想能力不能首尾配对；但是Sora可以啊，GPT是自家的，视频库也是自家的，两者直接****关联的精准度以及调取的效率，完全是外部客户不能比的。
使用场景 生成创意素材。通过剪辑和局部使用，做出自己的成品。对于自媒体行业是一大利好。生成概念片和内部讨论稿，极大的加快创意沟通的效率。利用它的连续性，结合一些3D工具，快速建模。已经有人在做了，但效果还待改进。推荐大家看看B站UP主设计师的AI工具箱的Sora建模实践, 调试的好，是个路子。 参考 3原理&#43;1揭秘，将Sora拉下神坛" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/c7e50652fa3c2b57e50e47ff0e69c515/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-03T12:54:47+08:00" />
<meta property="article:modified_time" content="2024-03-03T12:54:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大模型之SORA技术学习</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#sora_8" rel="nofollow">sora的技术原理</a></li><li><a href="#_29" rel="nofollow">文字生成视频过程</a></li><li><a href="#sora_35" rel="nofollow">sora的技术优势</a></li><li><ul><li><a href="#_37" rel="nofollow">量大质优的视频预训练库</a></li><li><a href="#_47" rel="nofollow">算力多，采样步骤多，更精细。</a></li><li><a href="#GPTPrompt_61" rel="nofollow">GPT解释力更强，提示词(Prompt）表现更好</a></li></ul> 
  </li><li><a href="#_68" rel="nofollow">使用场景</a></li><li><a href="#_73" rel="nofollow">参考</a></li></ul> 
</div> 
<br> Sora改变AI认知方式，开启走向【世界模拟器】的史诗级的漫漫征途，才是未来暴风眼，真正的重点。 
<p></p> 
<p>但<font color="blue">Sora并没有理解整个世界和诸多物理细节，常识及定律，而是根据GPT语义解释能力、丰富的联想和丰富度，基于海量的视频库，抄写视频片段并猜视频下一帧，并使用Diffusion、GAN（对抗式生成网络技术）将多个视频片段混合在一起，产生稳定且连续的短视频。</font></p> 
<p>Yann LeCun（ACM图灵奖得主，纽约大学教授。Meta首席人工智能科学家）针对Sora的评论。大意是：</p> 
<blockquote> 
 <p>让我在这里<strong>澄清一个巨大的误解</strong>。从提示文字，生成看起来相当逼真的视频，<strong>并不意味着，这个系统理解物理世界</strong>。生成一个视频，和基于世界模型的因果预测，大不相同。目前这种生成式的方向，代价高昂，可能还有更好的法子。<br> <img src="https://images2.imgbox.com/f8/f3/8Mq4Of2X_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<h2><a id="sora_8"></a>sora的技术原理</h2> 
<p>结合了 Diffusion model（扩散模型）和 Transformer 技术，以确保视频内容与文本描述紧密相连<br> <strong>Sora的官方技术报告</strong>详见：<br> <a href="https://openai.com/research/video-generation-models-as-world-simulators" rel="nofollow">Sora官方的技术报告：Video generation models as world simulators</a></p> 
<ol><li><strong>主要功能和目标</strong>： 
  <ul><li>目前，<strong>Sora</strong> 的主要功能是将文本输入转换成视频输出。这包括但不限于将故事、说明或命令转化为相应的视频。</li><li><strong>Sora</strong> 的目标是创建一个能够理解复杂文本描述并将其转化为高质量视频内容的系统。</li><li>官方的声明指出，<strong>Sora</strong> 的最终目标是向一个“通用物理世界模拟器”的方向迈进，即成为一个能够模拟真实世界的复杂互动和动态环境的强大工具。</li></ul> </li><li><strong>训练流程</strong>包括以下步骤： 
  <ul><li>收集视频数据与标注信息。</li><li>训练图片字幕模型。</li><li>利用 <strong>GPT-4</strong> 丰富视频描述。</li><li>切分视频为 Patches。</li><li>应用视频压缩模型。</li><li>在潜在空间中处理视频数据。</li><li>应用扩散模型与 <strong>Transformer</strong> 进行训练。</li><li>最终恢复高清视频。</li></ul> </li><li><strong>核心模块</strong>： 
  <ul><li><strong>Transformer 结构</strong>：这是一种深度学习模型的架构，最初用于自然语言处理，现在被广泛应用于各种领域，包括 <strong>Sora</strong> 的几个组件。</li><li><strong>Diffusion model</strong>：用于逐步去除视频中的噪声，从而生成清晰的图像场景。</li></ul> </li></ol> 
<h2><a id="_29"></a>文字生成视频过程</h2> 
<ul><li>提供一段文字：依靠GPT语义解释能力、丰富的联想和丰富度，产生针对视频内容详细的描述。如文本是：“散步在夜晚东京街道上”，GPT发挥想象力，联想出一堆词和关联“高楼”、“繁华夜景”等等。它联想力越丰富，Sora能关联到的时空碎块就越多越准。</li><li>Diffusion：作为一个画师，根据关键词特征值对应的可能性概率，在海量视频库到处翻，看看抄哪一个碎块比较像，看哪个像，就猜对应的下一笔要落在什么地方。重复很多步</li><li>通过Diffusion和Transformer共同联想，死记硬背，从巨大视频库里生拉硬拽，配合GAN（对抗式生成网络技术），把这些一张张碎块拼成图，再拼接成一个序列，每秒播放几十张，视频就出来了<br> <img src="https://images2.imgbox.com/51/f0/yv6yN1wC_o.png" alt="在这里插入图片描述"></li></ul> 
<h2><a id="sora_35"></a>sora的技术优势</h2> 
<p>Sora实际上对于Pika等，只是量变，都是差不多的技术和原理，没有质变。但几乎达到了近似质变的效果了。原因在于：</p> 
<h3><a id="_37"></a>量大质优的视频预训练库</h3> 
<p><strong>大力出奇迹</strong>，是OpenAI的基因。</p> 
<p>到底花了多少钱在高质量的视频素材上，搜集了多少的视频库，只有OpenAI自己知道。但可以肯定的是，远远不是Pika等创业团队所能比的。</p> 
<p><strong>记得多，才能抄的好，混得妙。</strong></p> 
<p>甚至，现在的视频量已经不能满足OpenAI的需求了。已经被爆料，OpenAI的视频库，大量使用了UE5生成的视频来做补充和训练。我们看到的赛车那个视频就是。</p> 
<p>Pika、Runway、Stable Video和Sora有时候会撞车：<code>都使用了同一个素材加到库中。那么使用一样类似的关键词，可能就能调出一模一样的元素。</code></p> 
<h3><a id="_47"></a>算力多，采样步骤多，更精细。</h3> 
<p>不同采样和计算步骤后，通过同一个视频库“猜”的步骤越多，加的东西越细，效果越好。做32倍运算的效果，就明显好于4倍的效果。<br> 还是<strong>大力出奇迹</strong>，OpenAI不变的配方和味道。</p> 
<p>那么请问，Pika等创业公司能有多少张GPU卡呢?<br> Sora能土豪的用32倍，1080p，渲染1分钟的视频。创业公司能用多少，4倍，360p，4-8秒，已经足够把钱烧光了…<br> 画面精致度怎么比？时长怎么比？<strong>不公平。</strong></p> 
<p>Diffusion的不稳定性通病，在Pika等产品中已经表现无疑。<br> Sora和他们完全不在一个层次上，稳定性很好。已经不能完全用数据和算力来解释了。一定是采用了GAN（对抗式生成网络技术）这个增强连续性的技术。</p> 
<p>生成的视频效果比较见下图：<br> <img src="https://images2.imgbox.com/b7/eb/A3mLfit8_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="GPTPrompt_61"></a>GPT解释力更强，提示词(Prompt）表现更好</h3> 
<p>对一段提示词或提示句子，GPT能展开的联想和丰富度，是决定了Sora抄什么，能猜多准的。<br> OpenAI的GPT能力天下第一，开源模型无能撼动。<br> 所以，不幸的是，Pika等创业公司大多还是要依靠OpenAI的<strong>GPT能力</strong>。</p> 
<p>那么，问题来了，亲儿子能用的，一定胜过外部客户能用到的<strong>深度和广度</strong>。<br> 第二个点，就是外部公司的视频库和GPT联想能力不能首尾配对；但是Sora可以啊，GPT是自家的，视频库也是自家的，两者<strong>直接****关联的精准度以及调取的效率</strong>，完全是外部客户不能比的。</p> 
<h2><a id="_68"></a>使用场景</h2> 
<ul><li>生成<strong>创意素材</strong>。通过剪辑和局部使用，做出自己的成品。对于自媒体行业是一大利好。</li><li>生成<strong>概念片和内部讨论稿</strong>，极大的加快创意沟通的效率。</li><li>利用它的<strong>连续性</strong>，结合一些3D工具，<strong>快速建模</strong>。已经有人在做了，但效果还待改进。推荐大家看看B站UP主设计师的AI工具箱的Sora建模实践, 调试的好，是个路子。</li></ul> 
<h2><a id="_73"></a>参考</h2> 
<p><a href="https://mp.weixin.qq.com/s/iGxhKxNkNBoPIH6iYmSKyw" rel="nofollow">3原理+1揭秘，将Sora拉下神坛</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/af2a59aae0628cf8e837b599866d3dcb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">大模型（LLM）的训练语料信息汇总</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/68689810c6cf3c9dad09425b747e2f57/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端实现排序数组的合并</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>