<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【自然语言处理四-从矩阵操作角度看 自注意self attention】 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【自然语言处理四-从矩阵操作角度看 自注意self attention】" />
<meta property="og:description" content="自然语言处理四-从矩阵操作角度看 自注意self attention 从矩阵角度看self attention获取Q K V矩阵注意力分数softmax注意力的输出再来分析整体的attention的矩阵操作过程从矩阵操作角度看，self attention如何解决问题的？W^q^ W^k^ W^v^这三个矩阵怎么获得？ 从矩阵角度看self attention 上一篇文章，【自然语言处理三-自注意self attention】介绍了如何实现selft attention，但没有介绍，为何自注意力就能解决参数扩张、无法并行等问题，仅仅用语言描述太过干涩，从矩阵操作的角度则可以清晰的了解，self attention的运作机制以及它如何解决这些问题的。
首先，还是先给出self attention的整体流程图
中间这个attention层，从输入到attention层的输出，就是我们是实现的目标，下面是一个简单的图示：
下面我们就从矩阵操作的角度来描述，具体如何实现中间这个self attention层。
获取Q K V矩阵 首先是根据输入乘上矩阵,获取qi,ki,vi
当我们将(ai,…an)整合成一个矩阵的时候，实际上这个操作是这样的：
这样我们的Q K V矩阵就是针对整个输入的了。
注意力分数 a1对于ai的注意力分数，是q1和ki的点乘，当然这个点乘操作在上文介绍过，可以有别的方法。
这个过程同样可以合并成一个矩阵操作，如下图：A矩阵中的每一列，就是ai对于其他输入的注意力分数
softmax 上述获取的A矩阵执行softmax操作
注意力的输出 softmax后的注意力分数，与其他输入的vi做乘法操作，获取最终注意力层的一个输出。
这个过程同样可以合并矩阵操作，如下：
最终的的这个O矩阵就是注意力的输出。
再来分析整体的attention的矩阵操作过程 这个总体的过程，可以用下面更简略的图来表示：
从矩阵操作角度看，self attention如何解决问题的？ 1.解决参数可能急剧扩张的问题
我们从上面整体的矩阵操作过程来看，实际上只有三个矩阵Wq Wk Wv的参数需要学习，其他都是经过矩阵运算。
参数不会出现剧增
2.解决无法并行的问题
矩阵对于每个输入的操作，是并行的，不再像seq2seq架构一样，是按照时间步，一步步操作。
3.解决记忆能力的问题
attention的分数是基于全体输入的，且没有经过时间步的传播，因此记忆是基于全句子的，且信息没有丢失
Wq Wk Wv这三个矩阵怎么获得？ 从整体流程来看，要实现attention，最关键的就是找到合适的Wq Wk Wv矩阵，那么这三个矩阵是怎么获得的呢？
它们是靠学习获得的，初始化后，经过模型输出，然后经过反向传播，通过调整误差，一步步的精确化了这三个矩阵" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/934bf03c69f3508a3fa6db7f5bc4a9ef/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-26T11:24:52+08:00" />
<meta property="article:modified_time" content="2024-02-26T11:24:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【自然语言处理四-从矩阵操作角度看 自注意self attention】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>自然语言处理四-从矩阵操作角度看 自注意self attention</h4> 
 <ul><li><a href="#self_attention_1" rel="nofollow">从矩阵角度看self attention</a></li><li><ul><li><a href="#Q_K_V_13" rel="nofollow">获取Q K V矩阵</a></li><li><a href="#_20" rel="nofollow">注意力分数</a></li><li><a href="#softmax_26" rel="nofollow">softmax</a></li><li><a href="#_30" rel="nofollow">注意力的输出</a></li><li><a href="#attention_38" rel="nofollow">再来分析整体的attention的矩阵操作过程</a></li><li><a href="#self_attention_42" rel="nofollow">从矩阵操作角度看，self attention如何解决问题的？</a></li><li><a href="#Wq_Wk_Wv_51" rel="nofollow">W^q^ W^k^ W^v^这三个矩阵怎么获得？</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="self_attention_1"></a>从矩阵角度看self attention</h2> 
<p>上一篇文章，<a href="https://blog.csdn.net/zishuijing_dd/article/details/136276435?spm=1001.2014.3001.5501">【自然语言处理三-自注意self attention】</a>介绍了如何实现selft attention，但没有介绍，为何自注意力就能解决参数扩张、无法并行等问题，仅仅用语言描述太过干涩，从矩阵操作的角度则可以清晰的了解，self attention的运作机制以及它如何解决这些问题的。</p> 
<p>首先，还是先给出self attention的整体流程图</p> 
<p><img src="https://images2.imgbox.com/bb/6a/Tdm6lAfC_o.png" alt="在这里插入图片描述"></p> 
<p>中间这个attention层，从输入到attention层的输出，就是我们是实现的目标，下面是一个简单的图示：<br> <img src="https://images2.imgbox.com/75/e1/oSNzeZ6k_o.png" alt="在这里插入图片描述"></p> 
<p>下面我们就从矩阵操作的角度来描述，具体如何实现中间这个self attention层。</p> 
<h3><a id="Q_K_V_13"></a>获取Q K V矩阵</h3> 
<p>首先是根据输入乘上矩阵,获取q<sup>i</sup>,k<sup>i</sup>,v<sup>i</sup><br> <img src="https://images2.imgbox.com/7f/c8/ynXDzDsr_o.png" alt="在这里插入图片描述"><br> 当我们将(a<sup>i</sup>,…a<sup>n</sup>)整合成一个矩阵的时候，实际上这个操作是这样的：<br> <img src="https://images2.imgbox.com/0c/00/nd7vFcl4_o.png" alt="在这里插入图片描述"><br> 这样我们的Q K V矩阵就是针对整个输入的了。</p> 
<h3><a id="_20"></a>注意力分数</h3> 
<p>a<sup>1</sup>对于a<sup>i</sup>的注意力分数，是q<sup>1</sup>和k<sup>i</sup>的点乘，当然这个点乘操作在上文介绍过，可以有别的方法。</p> 
<p>这个过程同样可以合并成一个矩阵操作，如下图：A矩阵中的每一列，就是a<sup>i</sup>对于其他输入的注意力分数<br> <img src="https://images2.imgbox.com/dc/57/cJoC9rsZ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="softmax_26"></a>softmax</h3> 
<p>上述获取的A矩阵执行softmax操作<br> <img src="https://images2.imgbox.com/56/f6/VCuH526M_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_30"></a>注意力的输出</h3> 
<p><img src="https://images2.imgbox.com/52/9f/qTkG035H_o.png" alt="在这里插入图片描述"></p> 
<p>softmax后的注意力分数，与其他输入的v<sup>i</sup>做乘法操作，获取最终注意力层的一个输出。<br> 这个过程同样可以合并矩阵操作，如下：<br> <img src="https://images2.imgbox.com/87/79/mF1rdADE_o.png" alt="在这里插入图片描述"><br> 最终的的这个O矩阵就是注意力的输出。</p> 
<h3><a id="attention_38"></a>再来分析整体的attention的矩阵操作过程</h3> 
<p>这个总体的过程，可以用下面更简略的图来表示：<br> <img src="https://images2.imgbox.com/7f/9d/WsaTWWIZ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="self_attention_42"></a>从矩阵操作角度看，self attention如何解决问题的？</h3> 
<p>1.解决参数可能急剧扩张的问题<br> 我们从上面整体的矩阵操作过程来看，实际上只有三个矩阵W<sup>q</sup> W<sup>k</sup> W<sup>v</sup>的参数需要学习，其他都是经过矩阵运算。<br> 参数不会出现剧增<br> 2.解决无法并行的问题<br> 矩阵对于每个输入的操作，是并行的，不再像seq2seq架构一样，是按照时间步，一步步操作。<br> 3.解决记忆能力的问题<br> attention的分数是基于全体输入的，且没有经过时间步的传播，因此记忆是基于全句子的，且信息没有丢失</p> 
<h3><a id="Wq_Wk_Wv_51"></a>W<sup>q</sup> W<sup>k</sup> W<sup>v</sup>这三个矩阵怎么获得？</h3> 
<p>从整体流程来看，要实现attention，最关键的就是找到合适的W<sup>q</sup> W<sup>k</sup> W<sup>v</sup>矩阵，那么这三个矩阵是怎么获得的呢？<br> 它们是靠学习获得的，初始化后，经过模型输出，然后经过反向传播，通过调整误差，一步步的精确化了这三个矩阵</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b9ef0298108606191c72c0444e7d148a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Redis探秘：十大最佳应用场景揭示</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a51ac6426fc7303ca503d1dd0dd7e168/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">安装淘宝镜像cnpm报错</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>