<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【复现笔记】Iterative Corresponding Geometry - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【复现笔记】Iterative Corresponding Geometry" />
<meta property="og:description" content="文献：
M. Stoiber, M. Sundermeyer and R. Triebel, &#34;Iterative Corresponding Geometry: Fusing Region and Depth for Highly Efficient 3D Tracking of Textureless Objects,&#34; 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 6845-6855, doi: 10.1109/CVPR52688.2022.00673.
论文网址：
Iterative Corresponding Geometry: Fusing Region and Depth for Highly Efficient 3D Tracking of Textureless Objects | IEEE Conference Publication | IEEE Xplore
作者提供代码：
3DObjectTracking/ICG at master · DLR-RM/3DObjectTracking · GitHub" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/4098117335bd777623e7c7497f7ff4dd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-12T19:15:01+08:00" />
<meta property="article:modified_time" content="2023-01-12T19:15:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【复现笔记】Iterative Corresponding Geometry</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>文献：</p> 
<p>M. Stoiber, M. Sundermeyer and R. Triebel, "Iterative Corresponding Geometry: Fusing Region and Depth for Highly Efficient 3D Tracking of Textureless Objects," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 6845-6855, doi: 10.1109/CVPR52688.2022.00673.</p> 
<p>论文网址：</p> 
<p><a href="https://ieeexplore.ieee.org/document/9879565" rel="nofollow" title="Iterative Corresponding Geometry: Fusing Region and Depth for Highly Efficient 3D Tracking of Textureless Objects | IEEE Conference Publication | IEEE Xplore">Iterative Corresponding Geometry: Fusing Region and Depth for Highly Efficient 3D Tracking of Textureless Objects | IEEE Conference Publication | IEEE Xplore</a></p> 
<p>作者提供代码：</p> 
<p><a href="https://github.com/DLR-RM/3DObjectTracking/tree/master/ICG" title="3DObjectTracking/ICG at master · DLR-RM/3DObjectTracking · GitHub">3DObjectTracking/ICG at master · DLR-RM/3DObjectTracking · GitHub</a></p> 
<p></p> 
<p><strong>1、依赖</strong></p> 
<blockquote> 
 <p>The following dependencies are required: <a href="https://eigen.tuxfamily.org/index.php?title=Main_Page" rel="nofollow" title="Eigen 3">Eigen 3</a>, <a href="http://glew.sourceforge.net/" rel="nofollow" title="GLEW">GLEW</a>, <a href="https://www.glfw.org/" rel="nofollow" title="GLFW 3">GLFW 3</a>, and <a href="https://opencv.org/" rel="nofollow" title="OpenCV 4">OpenCV 4</a>. In addition, images from an Azure Kinect or RealSense camera can be streamed using the <a href="https://github.com/microsoft/Azure-Kinect-Sensor-SDK" title="K4A">K4A</a> and <a href="https://github.com/IntelRealSense/librealsense" title="realsense2">realsense2</a> libraries.</p> 
</blockquote> 
<p>必须：</p> 
<p>（1）Eigen 3：<a href="https://blog.csdn.net/weixin_54470372/article/details/127448876?spm=1001.2014.3001.5501" title="Eigen3+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客">Eigen3+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客</a></p> 
<p>（2）GLEW：<a href="https://blog.csdn.net/weixin_54470372/article/details/127451007?spm=1001.2014.3001.5501" title="GLEW+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客">GLEW+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客</a></p> 
<p>（3）GLFW 3：<a href="https://blog.csdn.net/weixin_54470372/article/details/127450496?spm=1001.2014.3001.5501" title="GLFW+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客">GLFW+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客</a></p> 
<p>（4）OpenCV 4：<a href="https://blog.csdn.net/weixin_54470372/article/details/127452721?spm=1001.2014.3001.5501" title="OpenCV+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客">OpenCV+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客</a></p> 
<p>非必须：</p> 
<p>（5）kinect或realsense的库，本次未安装</p> 
<p>（6）OpenMP，未安装</p> 
<p>（7）Doxygen：<a href="https://blog.csdn.net/weixin_54470372/article/details/127461189?spm=1001.2014.3001.5502" title="Doxygen+Graphviz+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客">Doxygen+Graphviz+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客</a></p> 
<p>（8）Graphviz：<a href="https://blog.csdn.net/weixin_54470372/article/details/127461189?spm=1001.2014.3001.5502" title="Doxygen+Graphviz+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客">Doxygen+Graphviz+Ubuntu20.04安装_weixin_54470372的博客-CSDN博客</a></p> 
<pre><code>~/3dTracking$ git clone https://github.com/DLR-RM/3DObjectTracking.git
Cloning into '3DObjectTracking'...
remote: Enumerating objects: 325, done.
remote: Counting objects: 100% (325/325), done.
remote: Compressing objects: 100% (230/230), done.
remote: Total 325 (delta 159), reused 233 (delta 81), pack-reused 0
Receiving objects: 100% (325/325), 4.76 MiB | 1.49 MiB/s, done.
Resolving deltas: 100% (159/159), done.</code></pre> 
<p><strong>2、</strong><strong>源代码的文件结构</strong></p> 
<p>有5个文件夹：</p> 
<blockquote> 
 <ul><li><code>include/</code>: header files of the <em>ICG</em> library</li></ul> 
 <p>                                ICG库的头文件</p> 
 <ul><li><code>src/</code>: source files of the <em>ICG</em> library</li></ul> 
 <p>                                ICG库的源文件</p> 
 <ul><li><code>third_party/</code>: external header-only libraries</li></ul> 
 <p>                                只包含头文件的外部库</p> 
 <ul><li><code>examples/</code>: example files for tracking as well as for evaluation on different datasets</li></ul> 
 <p>                                用于跟踪和评估不同数据集的示例文件</p> 
 <ul><li><code>doc/</code>: files for documentation</li></ul> 
 <p>                                文档</p> 
</blockquote> 
<p><strong>3、CMAKE &amp; BUILD</strong></p> 
<blockquote> 
 <p>Use <code>CMake</code> to build the library from source. The following dependencies are required: <a href="https://eigen.tuxfamily.org/index.php?title=Main_Page" rel="nofollow" title="Eigen 3">Eigen 3</a>, <a href="http://glew.sourceforge.net/" rel="nofollow" title="GLEW">GLEW</a>, <a href="https://www.glfw.org/" rel="nofollow" title="GLFW 3">GLFW 3</a>, and <a href="https://opencv.org/" rel="nofollow" title="OpenCV 4">OpenCV 4</a>. In addition, images from an Azure Kinect or RealSense camera can be streamed using the <a href="https://github.com/microsoft/Azure-Kinect-Sensor-SDK" title="K4A">K4A</a> and <a href="https://github.com/IntelRealSense/librealsense" title="realsense2">realsense2</a> libraries. Both libraries are optional and can be disabled using the <em>CMake</em> flags <code>USE_AZURE_KINECT</code>, and <code>USE_REALSENSE</code>. If <em>CMake</em> finds <a href="https://www.openmp.org/" rel="nofollow" title="OpenMP">OpenMP</a>, the code is compiled using multithreading and vectorization for some functions. Finally, the documentation is built if <a href="https://www.doxygen.nl/index.html" rel="nofollow" title="Doxygen">Doxygen</a> with <em>dot</em> is detected. Note that links to classes that are embedded in this readme only work in the generated documentation.</p> 
</blockquote> 
<blockquote> 
 <p>翻译：使用CMake从源代码构建库。需要以下依赖项:特征3、GLEW、GLFW 3和OpenCV 4。此外，来自Azure Kinect或RealSense相机的图像可以使用K4A和realsense2库进行流媒体传输。这两个库都是可选的，可以使用CMake标志USE_AZURE_KINECT和USE_REALSENSE来禁用。如果CMake找到了OpenMP，则使用多线程和对某些函数进行向量化来编译代码。最后，如果检测到Doxygen内嵌dot，则构建文档。注意，到这个自述文件中嵌入的类的链接只能在生成的文档中使用。</p> 
</blockquote> 
<p>（1）cmake</p> 
<p>首先，切换到目录3DObjectTracking/ICG目录，因为没有安装kinect和realsense的libraries，所以需要设置：USE_AZURE_KINECT和USE_REALSENSE两个options，所以输入命令：</p> 
<pre><code>cmake -DUSE_AZURE_KINECT=OFF -DUSE_REALSENSE=OFF</code></pre> 
<p>输出如下：</p> 
<pre><code>$ cmake -DUSE_AZURE_KINECT=OFF -DUSE_REALSENSE=OFF
CMake Warning:
  No source or binary directory provided.  Both will be assumed to be the
  same as the current working directory, but note that this warning will
  become a fatal error in future CMake releases.


CMake Warning (dev) at /usr/local/share/cmake-3.25/Modules/FindOpenGL.cmake:315 (message):
  Policy CMP0072 is not set: FindOpenGL prefers GLVND by default when
  available.  Run "cmake --help-policy CMP0072" for policy details.  Use the
  cmake_policy command to set the policy and suppress this warning.

  FindOpenGL found both a legacy GL library:

    OPENGL_gl_LIBRARY: /usr/lib/x86_64-linux-gnu/libGL.so

  and GLVND libraries for OpenGL and GLX:

    OPENGL_opengl_LIBRARY: /usr/lib/x86_64-linux-gnu/libOpenGL.so
    OPENGL_glx_LIBRARY: /usr/lib/x86_64-linux-gnu/libGLX.so

  OpenGL_GL_PREFERENCE has not been set to "GLVND" or "LEGACY", so for
  compatibility with CMake 3.10 and below the legacy GL library will be used.
Call Stack (most recent call first):
  CMakeLists.txt:22 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found Doxygen: /usr/bin/doxygen (found version "1.8.17") found components: doxygen dot 
-- Configuring done
-- Generating done
-- Build files have been written to: /home/r****/3dTracking/3DObjectTracking/ICG</code></pre> 
<p>（2）make</p> 
<pre><code>make</code></pre> 
<p>最后几行输出：</p> 
<pre><code>Patching output file 35/35
lookup cache used 1814/65536 hits=11252 misses=1888
finished...
[100%] Built target doc_doxygen</code></pre> 
<p><strong>5、USAGE</strong></p> 
<blockquote> 
 <p>As explained previously, <em>ICG</em> is a library that supports a wide variety of tracking scenarios. As a consequence, to start tracking, one has to first configure the tracker. For this, two options exist:</p> 
 <ul><li> <p>One option is to use <em>C++</em> programming to set up and configure all objects according to ones scenario. An example that allows running the tracker on a sequence streamed from an AzureKinect is shown in <code>examples/run_on_camera_sequence.cpp</code>. The executable thereby takes the path to a directory and names of multiple bodies. The directory has to contain <code>Body</code> and <code>StaticDetector</code> metafiles that are called <code>&lt;BODY_NAME&gt;.yaml</code> file and <code>&lt;BODY_NAME&gt;_detector.yaml</code>. Similarly, <code>examples/run_on_recorded_sequence.cpp</code> allows to run the tracker on a sequence that was recorded using <code>record_camera_sequence.cpp</code>. The executable allows the tracking of a single body that is detected using a <code>ManualDetector</code>. It requires the metafiles for a <code>LoaderColorCamera</code>, <code>Body</code>, and <code>ManualDetector</code>, as well as the path to a temporary directory in which generated model files are stored.</p> </li><li> <p>In addition to the usage as a library in combination with <em>C++</em> programming, the tracker can also be configured using a generator function together with a YAML file that defines the overall configuration. A detailed description on how to set up the YAML file is given in <a href="https://github.com/DLR-RM/3DObjectTracking/blob/master/ICG/generator.html" title="Generator Configfile">Generator Configfile</a>. An example that shows how to use a generator is shown in <code>examples/run_generated_tracker.cpp</code>. The executable requires a YAML file that is parsed by the <code>GenerateConfiguredTracker()</code> function to generate a <code>Tracker</code> object. The main YAML file thereby defines how individual objects are combined and allows to specify YAML metafiles for individual components that do not use default parameters. An example of a YAML file is given in <code>examples\generator_example\config.yaml</code>.</p> </li></ul> 
</blockquote> 
<blockquote> 
 <p>翻译：</p> 
 <p>正如前面所解释的，ICG是一个支持多种跟踪场景的库。因此，要开始跟踪，首先必须配置跟踪器。对此，有两种选择:</p> 
 <ul><li>一种选择是使用c++编程来根据一个场景设置和配置所有对象。示例/run_on_camera_sequence.cpp中显示了一个允许在来自AzureKinect的序列流上运行跟踪器的例子。因此，可执行文件采用一个目录的路径和多个主体的名称。该目录必须包含名为&lt;body_name&gt;.yaml的Body和StaticDetector的meatafiles和&lt;body_name&gt;_detect .yaml文件。类似地，examples/run_on_recorded_sequence.cpp允许在使用了record_camera_sequence.cpp进行记录的序列上运行跟踪器。可执行文件允许跟踪使用ManualDetector检测到的单个主体。它需要LoaderColorCamera、Body和ManualDetector的metafiles，以及存储生成的模型文件的临时目录的路径。</li><li>除了将跟踪器作为库与c++编程结合使用外，还可以使用生成器函数和定义总体配置的YAML文件来配置跟踪器。Generator Configfile中给出了关于如何设置YAML文件的详细描述。在examples/run_generated_tracker.cpp中显示了如何使用生成器的示例。可执行文件需要一个由GenerateConfiguredTracker()函数解析的YAML文件，以生成一个Tracker对象。因此，主YAML文件定义了如何组合单个对象，并允许为不使用默认参数的单个组件指定YAML元文件。在examples\generator_example\config.yaml中给出了一个YAML文件的例子。</li></ul> 
</blockquote> 
<p>本次不使用kinect或realsense，所以暂时不需要</p> 
<p><strong>6、评估</strong></p> 
<blockquote> 
 <p>The code in examples/evaluate_&lt;DATASET_NAME&gt;_dataset.cpp and examples/parameters_study_&lt;DATASET_NAME&gt;.cpp contains everything for the evaluation on the YCB-Video, OPT, Choi, and RBOT datasets. For the evaluation, please download the YCB-Video, OPT, Choi, or RBOT dataset and adjust the dataset_directory in the source code. Note that model files (e.g. 002_master_chef_can_depth_model.bin, 002_master_chef_can_region_model.bin, ...) will be created automatically and are stored in the specified external_directory. For the evaluation of the YCB-Video dataset, please unzip poses_ycb-video.zip and store its content in the respective external_directory. For the Choi dataset, the Matlab script in examples/dataset_converter/convert_choi_dataset.m has to be executed to convert .pcd files into .png images. Also, using a program such as MeshLab, all model files have to be converted from .ply to .obj files and stored in the folder external_directory/models. Both the OPT and RBOT datasets work without any manual changes.</p> 
</blockquote> 
<blockquote> 
 <p>翻译：</p> 
 <p>（1）examples/evaluate_&lt;dataset_name&gt; _dataset .cpp和examples/parameters_study_&lt;dataset_name&gt;.cpp中的代码包含了对YCB-Video、OPT、Choi和RBOT数据集的所有评估。对于评估，请下载YCB-Video、OPT、Choi或RBOT数据集，并修改源代码中的dataset_directory。</p> 
 <p>（2）模型文件(如002_master_chef_can_depth_model.bin, 002_master_chef_can_region_model.bin，…)将自动创建，并存储在指定的external_directory中。</p> 
 <p>（3）数据集处理：对于YCB-Video数据集的评估，请解压缩poses_ycb-video.zip并将其内容存储在相应的external_目录中。对于Choi数据集，在examples/dataset_converter/convert_choi_dataset中的Matlab脚本。必须执行M将.pcd文件转换为.png图像。此外，使用像MeshLab这样的程序，所有的模型文件必须从.ply转换为.obj文件，并存储在external_directory/models文件夹中。OPT和RBOT数据集都不需要任何手动更改即可工作。</p> 
</blockquote> 
<p>（1）数据集</p> 
<blockquote> 
 <p>（1）使用的数据集：YCB-Video, OPT, Choi, RBOT</p> 
 <p>（2）对于YCB-Video数据集的评估，请解压缩poses_ycb-video.zip并将其内容存储在相应的external_目录中。</p> 
 <p>（3）对于Choi数据集，在examples/dataset_converter/convert_choi_dataset中的Matlab脚本。必须执行M将.pcd文件转换为.png图像。</p> 
 <p>（4）OPT和RBOT数据集都不需要任何手动更改即可工作。</p> 
</blockquote> 
<ul><li>YCB：<a href="https://rse-lab.cs.washington.edu/projects/posecnn/" rel="nofollow" title="PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes – UW Robotics and State Estimation Lab">PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes – UW Robotics and State Estimation Lab</a></li><li>OPT：<a href="http://media.ee.ntu.edu.tw/research/OPT/" rel="nofollow" title="OPT Dataset">OPT Dataset</a></li><li>Choi：<a href="http://people.ece.umn.edu/~cchoi/research_rgbdtracking.html" rel="nofollow" title="Changhyun Choi &gt; Research &gt; RGB-D Tracking">Changhyun Choi &gt; Research &gt; RGB-D Tracking</a></li><li>RBOT：<a href="http://cvmr.info/research/RBOT/" rel="nofollow" title="Computer Vision and Mixed Reality Group | RBOT | Computer Vision and Mixed Reality Group">Computer Vision and Mixed Reality Group | RBOT | Computer Vision and Mixed Reality Group</a></li></ul> 
<p>（2） 针对OPT数据集的评估</p> 
<p>修改代码中的directory：</p> 
<p>examples/evaluate_opt_dataset.cpp需要修改dataset、external和result_path</p> 
<p>examples/parameters_study_opt_dataset.cpp需要修改dataset、external_path</p> 
<blockquote> 
 <p>dataset_path：存放数据集的目录</p> 
 <p>external_path：将要存放模型文件的目录</p> 
 <p>result_path：将要存放结果的目录</p> 
</blockquote> 
<p>以下是我设置的目录：</p> 
<pre><code>  std::filesystem::path dataset_directory{"/home/r*/3dTracking/datasetP/OPT/Model3D/"};
  std::filesystem::path external_directory{"/home/r*/3dTracking/3DObjectTracking/ICG/examples/externalP/"};
  std::filesystem::path result_path{"/home/r*/3dTracking/3DObjectTracking/ICG/examples/resultP/"};</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f17f709275b2267aef14d5832c99dce6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Zabbix监控服务详解&#43;实战</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f4198d868ec398f8db5cad6d9cbf5930/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">还不会用Yakit&amp;Bp?来，我教你</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>