<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>OpenMMLab重磅升级！为计算机视觉不同方向建立的统一代码库 - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="OpenMMLab重磅升级！为计算机视觉不同方向建立的统一代码库" />
<meta property="og:description" content="关注“迈微AI研习社”，内容首发于公众号
来源｜陈恺@知乎，https://zhuanlan.zhihu.com/p/160966882
7 月 10 号 OpenMMLab 在 WAIC 2020 世界人工智能大会上发布了重磅升级，给大家带来了更丰富的 OpenMMLab 大礼包，1 个架构，10&#43; 个研究方向，100&#43; 种算法，600&#43; 预训练模型，是深度学习时代最完整的计算机视觉算法开源体系。
总体介绍 链接：https://open-mmlab.github.io/
从2018年年中开始，MMLab 和商汤科技就联合启动了 OpenMMLab 计划，为计算机视觉的不同方向建立统一而开放的代码库。这两年内，我们陆续开源了多个算法框架，包括：
MMDetection（物体检测）：在一个统一而灵活的架构上，高效实现了20多种典型检测算法。(https://github.com/open-mmlab/mmdetection)
MMAction（行为理解）：支持视频行为理解中的动作识别、时序检测、和时空检测等多种基本任务，复现了多种流行的算法，并支持常见的各种数据集。(https://github.com/open-mmlab/mmaction)
MMSkeleton（基于人体骨骼的理解）：以人体骨骼为核心的视频理解框架，基于时空图模型（ST-GCN）支持行为理解、姿态估计、动作生成等任务。(https://github.com/open-mmlab/mmskeleton)
MMSR（图像与视频超分辨率）：在统一的架构上，实现了一系列先进的超分辨率算法。
MMFashion（时尚分析）：专注于时尚服饰领域的视觉分析，覆盖识别、检索、属性预测、检测、分割、推荐等主流任务。(https://github.com/open-mmlab/mmfashion)
这些框架为学术社区提供了丰富而高质量的算法实现，开创了 OpenMMLab 的生态。
经过小伙伴们的加班加点，我们这个月初开源了 MMSegmentation、MMDetection3D、MMEditing、MMAction2、MMClassification、和 MMPose 这6个新的算法框架，并对 MMCV和 MMDetection 进行了全面更新。上述的这些框架都是基于 MMCV 的统一架构支持。它们在算法规模、训练效率和模型精度上都达到了开源社区的优秀水平，并且具有以下特性：
完善的文档和入门教程，丰富的代码注释，不再对着代码一头雾水
规范的开发流程，严谨的代码审核和单元测试，不再忍受 bug 缠身之痛
分布式训练框架，高效算子和逻辑实现，不再为跑不满的GPU干着急
有道是自从 star 了 OpenMMLab，一不愁算法复现挠秃头，二不愁从头造轮子几春秋，三不愁新方向框架难上手。
框架特点 MMCV（基础支持）
https://github.com/open-mmlab/mmcv
更完善的训练流程支持，文件读取多后端支持，图片处理多后端支持，更丰富的 CNN 模块，20 种常用算子的高效 CUDA 实现。
MMDetection（目标检测）
https://github.com/open-mmlab/mmdetection
支持算法多达 40 个，300&#43; 预训练模型，速度和精度相比 1.0 版本有很大提升。通过更细粒度的模块化设计，MMDetection 的任务拓展性大大增强，成为了检测相关项目的基础平台。
MMSegmentation（语义分割）
https://github.com/open-mmlab/mmsegmentation" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/93d385a5f6ae6d984af4500b852b8978/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-29T19:30:00+08:00" />
<meta property="article:modified_time" content="2020-07-29T19:30:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">OpenMMLab重磅升级！为计算机视觉不同方向建立的统一代码库</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p>关注“<strong>迈微AI研习社</strong>”，内容首发于公众号</p> 
 <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/b7/c1/4DL2N8BW_o.png"></p> 
 <p>来源｜陈恺@知乎，https://zhuanlan.zhihu.com/p/160966882</p> 
 <p>7 月 10 号 OpenMMLab 在 WAIC 2020 世界人工智能大会上发布了重磅升级，给大家带来了更丰富的 OpenMMLab 大礼包，1 个架构，10+ 个研究方向，100+ 种算法，600+ 预训练模型，是深度学习时代最完整的计算机视觉算法开源体系。</p> 
 <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/1e/ce/MwLNtMfR_o.png"></p> 
 <h3>总体介绍</h3> 
 <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/1b/a1/8jgDSF4X_o.png"></p> 
 <p>链接：https://open-mmlab.github.io/</p> 
 <p>从2018年年中开始，MMLab 和商汤科技就联合启动了 OpenMMLab 计划，为计算机视觉的不同方向建立统一而开放的代码库。这两年内，我们陆续开源了多个算法框架，包括：</p> 
 <ul><li> <p><strong>MMDetection（物体检测）</strong>：在一个统一而灵活的架构上，高效实现了20多种典型检测算法。(https://github.com/open-mmlab/mmdetection)</p> </li><li> <p><strong>MMAction（行为理解）</strong>：支持视频行为理解中的动作识别、时序检测、和时空检测等多种基本任务，复现了多种流行的算法，并支持常见的各种数据集。(https://github.com/open-mmlab/mmaction)</p> </li><li> <p><strong>MMSkeleton（基于人体骨骼的理解）</strong>：以人体骨骼为核心的视频理解框架，基于时空图模型（ST-GCN）支持行为理解、姿态估计、动作生成等任务。(https://github.com/open-mmlab/mmskeleton)</p> </li><li> <p><strong>MMSR（图像与视频超分辨率）</strong>：在统一的架构上，实现了一系列先进的超分辨率算法。</p> </li><li> <p><strong>MMFashion（时尚分析）</strong>：专注于时尚服饰领域的视觉分析，覆盖识别、检索、属性预测、检测、分割、推荐等主流任务。(https://github.com/open-mmlab/mmfashion)</p> </li></ul> 
 <p>这些框架为学术社区提供了丰富而高质量的算法实现，开创了 OpenMMLab 的生态。</p> 
 <p>经过小伙伴们的加班加点，我们这个月初开源了 MMSegmentation、MMDetection3D、MMEditing、MMAction2、MMClassification、和 MMPose 这6个新的算法框架，并对 MMCV和 MMDetection 进行了全面更新。上述的这些框架都是基于 MMCV 的统一架构支持。它们在算法规模、训练效率和模型精度上都达到了开源社区的优秀水平，并且具有以下特性：</p> 
 <ul><li> <p>完善的文档和入门教程，丰富的代码注释，不再对着代码一头雾水</p> </li><li> <p>规范的开发流程，严谨的代码审核和单元测试，不再忍受 bug 缠身之痛</p> </li><li> <p>分布式训练框架，高效算子和逻辑实现，不再为跑不满的GPU干着急</p> </li></ul> 
 <p><em>有道是自从 star 了 OpenMMLab，一不愁算法复现挠秃头，二不愁从头造轮子几春秋，三不愁新方向框架难上手。</em></p> 
 <h3>框架特点</h3> 
 <ul><li> <p><strong>MMCV（基础支持）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/mmcv</p> 
 <blockquote> 
  <p>更完善的训练流程支持，文件读取多后端支持，图片处理多后端支持，更丰富的 CNN 模块，20 种常用算子的高效 CUDA 实现。</p> 
 </blockquote> 
 <ul><li> <p><strong>MMDetection（目标检测）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/mmdetection</p> 
 <blockquote> 
  <p>支持算法多达 40 个，300+ 预训练模型，速度和精度相比 1.0 版本有很大提升。通过更细粒度的模块化设计，MMDetection 的任务拓展性大大增强，成为了检测相关项目的基础平台。</p> 
 </blockquote> 
 <ul><li> <p><strong>MMSegmentation（语义分割）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/mmsegmentation</p> 
 <blockquote> 
  <p>在统一的设定下，对 10+ 种语义分割算法的进行了统一 benchmark，并且达到了更高的精度。开源了 200+ 预训练模型和丰富的配置，方便进行横向纵向比较。支持丰富的训练和测试 trick，可以应对多样的使用场景。支持混合精度训练，节省显存 40%以上。</p> 
 </blockquote> 
 <ul><li> <p><strong>MMDetection3D（3D目标检测）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/mmdetection3d</p> 
 <blockquote> 
  <p>MMDetection 的 3D 扩展，训练速度领先其他3D点云检测代码库。任务拓展性强，可作为3D检测相关任务的基础平台，支持单模态和多模态 3D 检测，同时支持室内和室外场景数据集SOTA。无缝使用 MMDetection 中的全部已有算法和模型，尽享丝滑用户体验。</p> 
 </blockquote> 
 <ul><li> <p><strong>MMEditing（图像和视频编辑）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/mmediting</p> 
 <blockquote> 
  <p>设计了统一的框架同时支持超分、修复、抠图、生成四大方向，方便用户在一个框架中调用不同的算法和模型。提供了丰富的底层视觉算法的高效算子，拥有高效的训练和测试速度，复现了多个未开源算法，并首次公开实现了基于 PyTorch 的 GAN 的分布式训练。</p> 
 </blockquote> 
 <ul><li> <p><strong>MMAction2（动作识别）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/mmaction2</p> 
 <blockquote> 
  <p>支持 8 种模型和 8 个数据集，提供完善的数据处理脚本，支持多种解码器进行高效的在线视频解码，训练速度快，模型精度高。更细粒度的模块化设计，易于拓展。</p> 
 </blockquote> 
 <ul><li> <p><strong>MMClassification（图像分类）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/mmclassification</p> 
 <blockquote> 
  <p>丰富的训练配置，支持常见网络的复现，并提供相应的预训练模型。</p> 
 </blockquote> 
 <ul><li> <p><strong>MMPose（人体关键点检测）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/mmpose</p> 
 <blockquote> 
  <p>首个同时支持 top-down 和 bottom-up 类型算法的开源人体姿态估计框架，并实现了目前学术界的最高训练精度和最快训练速度。基于模块化的设计，易于拓展和修改。后续将支持多人3D姿态估计、密集人群的姿态估计、人脸关键点等更多模块。</p> 
 </blockquote> 
 <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ee/ef/ecXkgVqq_o.png"></p> 
 <p><strong>我们将在后续文章里对这些葫芦娃进行详细的介绍，敬请（快）期（关）待（注）！</strong></p> 
 <p>除了上述项目外，OpenMMLab 也加入了一些创新型项目，例如 OpenSelfSup，OpenPCDet，OpenUnReID 等等。这些项目选择了不同的架构方式，也是对 AI 社区非常有价值的贡献。它们和 MM 系列框架一起，共同构成了繁荣的 OpenMMLab 社区，给大家更多选择。</p> 
 <ul><li> <p><strong>OpenSelfSup（自监督学习）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/OpenSelfSup</p> 
 <blockquote> 
  <p>以统一的框架支持基于分类、聚类、memory bank、contrastive learning 的多种自监督学习算法，支持最前沿的 SimCLR, MoCo, BYOL 等算法，在多个 benchmark 上支持标准化的评测方案。</p> 
 </blockquote> 
 <ul><li> <p><strong>OpenPCDet（点云 3D 目标检测）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/OpenPCDet</p> 
 <blockquote> 
  <p>清晰简洁的代码框架设计，基于统一规范化的 3D 坐标系，方便地支持各种数据集，代码采用模块化设计，集成各种 3D 检测模型，包含 PV-RCNN 等多个排行榜 top 排名的高性能3D检测方法。</p> 
 </blockquote> 
 <ul><li> <p><strong>OpenUnReID)（无监督目标重识别）</strong></p> </li></ul> 
 <p>https://github.com/open-mmlab/OpenUnReID</p> 
 <blockquote> 
  <p>第一个针对无监督及领域自适应的目标重识别框架，同时支持基于伪标签和域转换的算法，速度和精度相对其他框架均更优，且具有较强的可拓展性。</p> 
 </blockquote> 
 <h3>OpenMMLab 代码架构简介</h3> 
 <p>在 MM 系列项目的整体架构中，我们突出两个特点：模块化和简洁化。模块化让用户可以更灵活地进行排列组合，以及开发新的方法和模块；简洁化让用户不用过多关注和核心逻辑无关的事情，将辅助功能交由框架自行处理。</p> 
 <p>我们通过 Registry、Config和 Builder 实现模块化的基础支持，然后在模块层面将数据、模型等进行拆解和抽象，进行模块化的实现，在最顶层通过 Runner 来实现流程的简洁化。</p> 
 <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/60/aa/EG2mzEld_o.png"></p> 
 <p><strong>注册器和构建器</strong></p> 
 <p>注册器提供了在不修改核心代码的前提下进行模块拓展的能力。通过管理字符串到类的映射，可以实现从配置文件直接构建模块。在 OpenMMLab 的项目中，不论是模型结构，数据预处理组件，还是优化器，甚至顶层的训练流程，都是通过这种方式来构建的，给用户提供了极大的拓展空间。</p> 
 <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/91/6c/2qN6B0Ia_o.png"></p> 
 <p>基于注册器和构建器，我们可以轻松地进行模块拓展，而不需要在原有框架代码中进行改动，这也是我们推荐的基于 OpenMMLab 中的框架进行算法开发的流程。事实上，整个 MMDetection3D 框架就是 MMDetection 的一个拓展，并没有 fork 并修改 MMDetection 的代码，就可以直接通过注册更多模块来实现相应的功能。</p> 
 <p><strong>数据</strong></p> 
 <p>我们将数据解耦成两个主要模块，即<strong>数据集定义（Dataset）<strong>和</strong>预处理流水线（DataPipeline）</strong>。Dataset 只负责数据集本身的定义和解析，生成统一的内部格式，DataPipeline 负责数据的预处理流程，每个环节的输入输出都是字典。这样同一个数据集可以轻松配置不同的预处理，不同的数据集也可以共享同一套预处理流程，大大提升了灵活性和代码复用性。</p> 
 <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/c1/c6/dRa2KlkN_o.png"></p> 
 <h4>模型</h4> 
 <p>OpenMMLab 没有对模型的抽象和模块化做统一的规定，需要相应的开发者对具体研究领域有比较深的理解。例如在检测框架中，我们使用了 Backbone，Neck，Head 的粗粒度结构，再加上 Assigner，Sampler，Loss 等细粒度结构，而在分割框架中，我们则使用了 Backbone，DecodeHead，AuxiliaryHead的抽象。</p> 
 <p><strong>优化器</strong></p> 
 <p>在大部分项目中，大家直接针对所有模型参数构造一个优化器即可。但是在部分模型的训练中，对于优化器有一些特殊需求，比如对于不同的参数使用不同的learning rate 和 weight decay，或者使用多个优化器，每个优化器负责模型的一部分，例如生成对抗网络的训练。为了满足各种自定义的需求，我们设计了 OptimizerConstructor 来负责优化器的构造，可以通过简单的选项任意指定不同参数的学习率等参数。利用这个机制，我们便可以通过配置文件灵活地实现各种不同的优化器。</p> 
 <p><strong>训练流程</strong></p> 
 <p>在数据、模型、优化器之上，我们抽象了一套标准的训练流程，由 Runner 进行控制。Runner 分为两个主要部分：循环框架和钩子（hook）。</p> 
 <p>Runner 的主体部分是一个循环框架，进行一轮轮的模型迭代，包括取出一个 data batch，调用模型的 train_step 函数进行用户定义的 forward，循环往复。hook 是可调用的函数或类，用户可以将各种 hook 注册到 runner 里面，runner 在循环的不同位置例如 iteration 开始前和结束后，epoch 开始前和结束后等等设定了一些触发点，每当循环进行到这些触发点，就会执行用户注册进来的 hook，从而实现训练流程的自定义。</p> 
 <p>通过这样的机制，我们将一些辅助操作比如训练日志的记录、学习率的调整、模型的保存等等从训练流程的主体框架中剥离出来，通过 hook 的配置来完成。由于 MMCV 中提供了丰富的内置 hook，用户通常只需要通过配置文件做简单的配置即可。</p> 
 <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/61/03/O6AprDBE_o.jpg"></p> 
 <h3>近期展望</h3> 
 <p>这次的发布对于 OpenMMLab 来说只是一个新的起点，还有很多坑等着我们来填，很多立下的 flag 需要我们去完成。</p> 
 <p>对于算法框架，我们将持续推动框架迭代，增强不同框架间的互联互通，同时打造上下游支持工具集。</p> 
 <p>对于社区，我们将进行规范化运营。OpenMMLab 的核心在于 Open，希望能有更多的研究者和开发者能与我们一起合作，同时也希望从社区吸纳更多优秀的新框架加入 OpenMMLab，让生态更加繁荣。</p> 
 <p>http://openmmlab.org/</p> 
 <p> </p> 
 <p><strong>推荐阅读</strong>（点击标题可跳转阅读）</p> 
 <ul><li> <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMjg1Njc3Mw%3D%3D&amp;chksm=97bd17fca0ca9eeaef85dc36bdfb6a28c1d74c88a4d021df415fdde439e2efc018ebe7a21b82&amp;idx=1&amp;mid=2247494998&amp;scene=21&amp;sn=bcdf2fd8a5ce7013704e1ee7fe161e5d#wechat_redirect" rel="nofollow">为什么你的模型效果这么差，深度学习调参有哪些技巧？</a></p> </li><li> <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMjg1Njc3Mw%3D%3D&amp;chksm=97bd13d3a0ca9ac5ad115ea3985ecd0458fb38f5562f56db0b5ab8ec45562cceed1d92653d2f&amp;idx=1&amp;mid=2247494009&amp;scene=21&amp;sn=e1a79285873c6dd9fe35fe36771b9a4e#wechat_redirect" rel="nofollow">CVPR 2020：华为GhostNet，超越谷歌MobileNet，已开源</a></p> </li><li> <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMjg1Njc3Mw%3D%3D&amp;chksm=97bd1c9da0ca958b6400da161646d62bde099da1555be7bdd9469ce142c0e9f9b1ec2fce0f10&amp;idx=1&amp;mid=2247493175&amp;scene=21&amp;sn=fb93597d1ff68af040c798f02a79fa43#wechat_redirect" rel="nofollow">YOLOv5是真的吗？并不比YOLOv4强，不配这个名字</a></p> </li><li> <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMjg1Njc3Mw%3D%3D&amp;chksm=97bd1b67a0ca92717cb3e4cc97006533072f8129dac477ffe1f5fde92fb0212f9dde56662a17&amp;idx=2&amp;mid=2247492045&amp;scene=21&amp;sn=4a3124dce19c5da0260c625af39ca9a7#wechat_redirect" rel="nofollow">这个比肩ImageNet的数据集遭MIT紧急下架，原因令人愤怒</a></p> </li></ul> 
 <p><img alt="" height="733" src="https://images2.imgbox.com/89/cf/CTxUU8wS_o.png" width="1187"></p> 
 <div style="text-align:center;"> 
  <figure class="image"> 
   <img alt="" height="239" src="https://images2.imgbox.com/f7/86/AqAF6OAZ_o.png" width="239"> 
   <figcaption>
     微信扫一扫， 
    <strong>关注我</strong> 
   </figcaption> 
  </figure> 
 </div> 
 <p> </p> 
 <p>MaiweiAI-com | WeChat ID:Yida_Zhang2</p> 
 <p>机器学习+计算机视觉</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f01dbf720a60f06e9de001cb1e472e3d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Apollo启动报Config service failed to start in 120 seconds! Please check ./service/apollo-ser</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4cdf5c086a5e6e5ac274b4116509522a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">简单聊聊Betaflight的三种飞行模式</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>