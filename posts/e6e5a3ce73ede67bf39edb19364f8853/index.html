<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Live555 直播源  以及MediaSubsession - 编程鬼谷子的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Live555 直播源  以及MediaSubsession" />
<meta property="og:description" content="/* * H264DeviceSource.hh * * Created on: 2014年7月19日 * Author: zjzhang */ #ifndef H264DEVICESOURCE_HH_ #define H264DEVICESOURCE_HH_ #include&lt;DeviceSource.hh&gt; class H264DeviceSource: public DeviceSource { public: static DeviceSource* createNew(UsageEnvironment&amp; env,u_int8_t index=1,u_int width=352,u_int height=288,u_int fps=15,u_int kbps=100); protected: H264DeviceSource(UsageEnvironment&amp; env,u_int8_t index,u_int width,u_int height,u_int fps,u_int kbps); virtual ~H264DeviceSource(); private: virtual void doGetNextFrame(); virtual unsigned maxFrameSize() const; int fHeight; int fWidth; void *fH264Encoder; u_int8_t * fBuffer; u_int fBufferSize; }; #endif /* H264DEVICESOURCE_HH_ */ /* * H264DeviceSource.cpp * * Created on: 2014年7月19日 * Author: zjzhang */ #include &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcguiguzi.github.io/posts/e6e5a3ce73ede67bf39edb19364f8853/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2014-07-30T17:28:00+08:00" />
<meta property="article:modified_time" content="2014-07-30T17:28:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程鬼谷子的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程鬼谷子的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Live555 直播源  以及MediaSubsession</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="cnblogs_post_body" class="blogpost-body"> 
 <p></p> 
 <pre><code class="language-cpp">/*
 * H264DeviceSource.hh
 *
 *  Created on: 2014年7月19日
 *      Author: zjzhang
 */

#ifndef H264DEVICESOURCE_HH_
#define H264DEVICESOURCE_HH_
#include&lt;DeviceSource.hh&gt;

class H264DeviceSource: public DeviceSource {
public:
	static DeviceSource* createNew(UsageEnvironment&amp; env,u_int8_t index=1,u_int width=352,u_int height=288,u_int fps=15,u_int kbps=100);
protected:
	H264DeviceSource(UsageEnvironment&amp; env,u_int8_t index,u_int width,u_int height,u_int fps,u_int kbps);
	virtual ~H264DeviceSource();
private:
	virtual void doGetNextFrame();
	virtual unsigned maxFrameSize() const;
	int fHeight;
	int fWidth;
	void *fH264Encoder;
	u_int8_t * fBuffer;
	u_int fBufferSize;
};

#endif /* H264DEVICESOURCE_HH_ */
</code></pre> 
 <br> 
 <pre><code class="language-cpp">/*
 * H264DeviceSource.cpp
 *
 *  Created on: 2014年7月19日
 *      Author: zjzhang
 */

#include "H264DeviceSource.hh"
#ifdef __cplusplus
extern "C" {
#endif
#include "H264Stream.h"
#ifdef __cplusplus
}
#endif
DeviceSource*
H264DeviceSource::createNew(UsageEnvironment&amp; env, u_int8_t index, u_int width,
		u_int height, u_int fps, u_int kbps) {
	return new H264DeviceSource(env, index, width, height, fps, kbps);
}

H264DeviceSource::H264DeviceSource(UsageEnvironment&amp; env, u_int8_t index,
		u_int width, u_int height, u_int fps, u_int kbps) :
		DeviceSource(env, DeviceParameters()) {
	openCamera(1);
	getFrame(1);
	fHeight = getHeight(1);
	fWidth = getWidth(1);
	openH264Encoder(fWidth, fHeight, fps, kbps, &amp;fH264Encoder);
	fBufferSize = fHeight * fWidth * 3 / 2;
	fBuffer = new uint8_t[fBufferSize];

}

H264DeviceSource::~H264DeviceSource() {
	// TODO Auto-generated destructor stub

	delete[] fBuffer;
	closeH264Encoder(fH264Encoder);
	closeCamera(1);
}
unsigned H264DeviceSource::maxFrameSize() const {
  // By default, this source has no maximum frame size.
  return 4096;
}
void H264DeviceSource::doGetNextFrame() {
	if (!isCurrentlyAwaitingData())
		return; // we're not ready for the data yet

	unsigned char * rgbBuffer = getFrame(1);
	ConvertRGB2YUV(fWidth, fHeight, rgbBuffer, fBuffer);
	int newFrameSize = encodeFrame(fH264Encoder, fBuffer, fBufferSize);

	// Deliver the data here:
	if (newFrameSize &lt; 0) {
		handleClosure();
		return;
	}
	if (newFrameSize &gt; fMaxSize) {
		fFrameSize = fMaxSize;
		fNumTruncatedBytes = newFrameSize - fMaxSize;
	} else {
		fFrameSize = newFrameSize;
	}
	if (fFrameSize &gt; 0) {
		int result = 0;
		int p = 0;
		do {
			unsigned long len = 0;
			result = getNextPacket(fH264Encoder, fBuffer + p, &amp;len);
			p += len;
		} while (result &gt; 0);
	}

	gettimeofday(&amp;fPresentationTime, NULL); // If you have a more accurate time - e.g., from an encoder - then use that instead.
	// If the device is *not* a 'live source' (e.g., it comes instead from a file or buffer), then set "fDurationInMicroseconds" here.
	memmove(fTo, fBuffer, fFrameSize);

	FramedSource::afterGetting(this);
}
</code></pre> 
 <br> 
 <br> 
 <p><br></p> 
 <p></p> 
 <pre><code class="language-cpp">#ifndef _DEVIC_SERVER_MEDIA_SUBSESSION_HH
#define _DEVICE_SERVER_MEDIA_SUBSESSION_HH

#ifndef _ON_DEMAND_SERVER_MEDIA_SUBSESSION_HH
#include "OnDemandServerMediaSubsession.hh"
#endif
class  DeviceSource;
class DeviceServerMediaSubsession: public OnDemandServerMediaSubsession {
public:
  static DeviceServerMediaSubsession*
  createNew(UsageEnvironment&amp; env,
		    Boolean reuseFirstSource);

  // Used to implement "getAuxSDPLine()":
  void checkForAuxSDPLine1();
  void afterPlayingDummy1();
protected: // we're a virtual base class
  DeviceServerMediaSubsession(UsageEnvironment&amp; env,
			    Boolean reuseFirstSource);
  virtual ~DeviceServerMediaSubsession();

  void setDoneFlag() { fDoneFlag = ~0; }

protected: // redefined virtual functions
  virtual char const* getAuxSDPLine(RTPSink* rtpSink,
				    FramedSource* inputSource);
  virtual FramedSource* createNewStreamSource(unsigned clientSessionId,
					      unsigned&amp; estBitrate);
  virtual RTPSink* createNewRTPSink(Groupsock* rtpGroupsock,
                                    unsigned char rtpPayloadTypeIfDynamic,
				    FramedSource* inputSource);

private:
  char* fAuxSDPLine;
  char fDoneFlag; // used when setting up "fAuxSDPLine"
  RTPSink* fDummyRTPSink; // ditto
};

#endif
</code></pre> 
 <br> 
 <pre><code class="language-cpp">#include "DeviceServerMediaSubsession.hh"
#include "H264VideoRTPSink.hh"
#include "DeviceSource.hh"
#include "H264VideoStreamFramer.hh"
#include "H264DeviceSource.hh"

DeviceServerMediaSubsession*
DeviceServerMediaSubsession::createNew(UsageEnvironment&amp; env,
		Boolean reuseFirstSource) {
	return new DeviceServerMediaSubsession(env, reuseFirstSource);
}
DeviceServerMediaSubsession::DeviceServerMediaSubsession(UsageEnvironment&amp; env,
		Boolean reuseFirstSource) :
		OnDemandServerMediaSubsession(env, reuseFirstSource) {
}

DeviceServerMediaSubsession::~DeviceServerMediaSubsession() {
}

FramedSource* DeviceServerMediaSubsession::createNewStreamSource(
		unsigned /*clientSessionId*/, unsigned&amp; estBitrate) {
	DeviceSource* source = H264DeviceSource::createNew(envir());
	return H264VideoStreamFramer::createNew(envir(), source);
}

RTPSink* DeviceServerMediaSubsession::createNewRTPSink(Groupsock* rtpGroupsock,
		unsigned char rtpPayloadTypeIfDynamic, FramedSource* /*inputSource*/) {
	return H264VideoRTPSink::createNew(envir(), rtpGroupsock,
			rtpPayloadTypeIfDynamic);
}

static void afterPlayingDummy(void* clientData) {
	DeviceServerMediaSubsession* subsess =
			(DeviceServerMediaSubsession*) clientData;
	subsess-&gt;afterPlayingDummy1();
}

void DeviceServerMediaSubsession::afterPlayingDummy1() {
	// Unschedule any pending 'checking' task:
	envir().taskScheduler().unscheduleDelayedTask(nextTask());
	// Signal the event loop that we're done:
	setDoneFlag();
}

static void checkForAuxSDPLine(void* clientData) {
	DeviceServerMediaSubsession* subsess =
			(DeviceServerMediaSubsession*) clientData;
	subsess-&gt;checkForAuxSDPLine1();
}

void DeviceServerMediaSubsession::checkForAuxSDPLine1() {
	char const* dasl;

	if (fAuxSDPLine != NULL) {
		// Signal the event loop that we're done:
		setDoneFlag();
	} else if (fDummyRTPSink != NULL
			&amp;&amp; (dasl = fDummyRTPSink-&gt;auxSDPLine()) != NULL) {
		fAuxSDPLine = strDup(dasl);
		fDummyRTPSink = NULL;

		// Signal the event loop that we're done:
		setDoneFlag();
	} else if (!fDoneFlag) {
		// try again after a brief delay:
		int uSecsToDelay = 100000; // 100 ms
		nextTask() = envir().taskScheduler().scheduleDelayedTask(uSecsToDelay,
				(TaskFunc*) checkForAuxSDPLine, this);
	}
}
char const* DeviceServerMediaSubsession::getAuxSDPLine(RTPSink* rtpSink,
		FramedSource* inputSource) {

	if (fAuxSDPLine != NULL)
		return fAuxSDPLine; // it's already been set up (for a previous client)

	if (fDummyRTPSink == NULL) { // we're not already setting it up for another, concurrent stream
		// Note: For H264 video files, the 'config' information ("profile-level-id" and "sprop-parameter-sets") isn't known
		// until we start reading the file.  This means that "rtpSink"s "auxSDPLine()" will be NULL initially,
		// and we need to start reading data from our file until this changes.
		fDummyRTPSink = rtpSink;

		// Start reading the file:
		fDummyRTPSink-&gt;startPlaying(*inputSource, afterPlayingDummy, this);

		// Check whether the sink's 'auxSDPLine()' is ready:
		checkForAuxSDPLine(this);
	}

	envir().taskScheduler().doEventLoop(&amp;fDoneFlag);

	return fAuxSDPLine;
}
</code></pre> 
 <br> 
 <br> 
 <p><br></p> 
 <p><br></p> 
</div> 
<p>转载于:https://www.cnblogs.com/cl1024cl/p/6204790.html</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e2e895c163170717d9e8ea3a8b4e3a90/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">亚马逊 kindle 刷机  过程记录</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4732e9988b417f2fb40a06f91988c98e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Unity3D中目标相对自身的前后左右方位判断</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程鬼谷子的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>